- name: 1. Infrastructure - Deploy Modern Charmed Kubernetes (Run:ai Ready)
  hosts: maasjuju
  gather_facts: yes
  vars:
    test_model: "k8s-cluster-runai"
    k8s_channel: "1.31/stable"
    bundle_path: "{{ ansible_user_dir }}/charmed-k8s-modern.yaml"
    # [新增] MetalLB IP 設定 (請依照你的 VM 網段修改，例如你的 VM 是 192.168.1.x)
    # 這裡預留 10 個 IP 給 Load Balancer 使用
    metallb_ip_range: "192.168.64.200-192.168.64.210" 

  tasks:
    # ... (前面的 1.0 ~ 1.2 清理與建立 Model 步驟保持不變) ...
    - name: 1.0 Force cleanup old model
      command: "juju destroy-model {{ test_model }} --no-prompt --force --destroy-storage"
      register: destroy_result
      failed_when: 
        - destroy_result.rc != 0 
        - '"not found" not in destroy_result.stderr'
      ignore_errors: yes

    - name: 1.1 Wait for model release
      pause: seconds: 15

    - name: 1.2 Create new Juju Model
      shell: juju add-model {{ test_model }}

    # --- 2. Bundle & Deploy (加入 MetalLB) ---
    - name: 2.1 Generate Local Bundle
      copy:
        dest: "{{ bundle_path }}"
        mode: '0644'
        content: |
          description: Slinky Modern K8s Cluster (Run:ai Ready)
          base: ubuntu@22.04
          machines:
            "0":
              constraints: tags=virtual
            "1":
              constraints: tags=slurm-node
            "2":
              constraints: tags=slurm-node
          
          applications:
            # ... (EasyRSA, Etcd, K8s-Control-Plane, Worker, Calico, Containerd 保持不變) ...
            easyrsa: {charm: easyrsa, channel: latest/stable, num_units: 1, to: ["0"]}
            etcd: {charm: etcd, channel: latest/stable, num_units: 1, to: ["0"]}
            kubernetes-control-plane: 
              charm: kubernetes-control-plane
              channel: {{ k8s_channel }}
              num_units: 1
              to: ["0"]
              options: {allow-privileged: "true"}
            kubernetes-worker:
              charm: kubernetes-worker
              channel: {{ k8s_channel }}
              num_units: 2
              to: ["1", "2"]
              expose: true
            calico: {charm: calico, channel: latest/stable}
            containerd: {charm: containerd, channel: latest/stable}
            
            # --- 既有組件 ---
            kubernetes-metrics-server: {charm: kubernetes-metrics-server, channel: stable, trust: true}
            ingress-nginx-k8s: {charm: ingress-nginx-k8s, channel: stable, trust: true}
            
            # [新增] MetalLB Load Balancer (這是缺少的重要組件)
            metallb:
              charm: metallb
              channel: stable
              options:
                iprange: "{{ metallb_ip_range }}"

          relations:
            # ... (既有 Relations 保持不變) ...
            - ["kubernetes-control-plane:kube-control", "kubernetes-worker:kube-control"]
            - ["kubernetes-control-plane:certificates", "easyrsa:client"]
            - ["etcd:certificates", "easyrsa:client"]
            - ["kubernetes-control-plane:etcd", "etcd:db"]
            - ["kubernetes-worker:certificates", "easyrsa:client"]
            - ["calico:etcd", "etcd:db"]
            - ["calico:cni", "kubernetes-control-plane:cni"]
            - ["calico:cni", "kubernetes-worker:cni"]
            - ["containerd:containerd", "kubernetes-worker:container-runtime"]
            - ["containerd:containerd", "kubernetes-control-plane:container-runtime"]
            - ["kubernetes-metrics-server:kube-control", "kubernetes-control-plane:kube-control"]
            - ["ingress-nginx-k8s:kube-control", "kubernetes-control-plane:kube-control"]
            
            # [新增] MetalLB 雖然不需要直接 relation，但它是透過 kube-system 運作，部署即可

    - name: 2.2 Deploy Bundle
      shell: |
        juju deploy {{ bundle_path }} --model {{ test_model }} --trust
      register: deploy_out
      until: deploy_out.rc == 0
      retries: 3

    # ... (3.1 Wait for Juju Apps Active 保持不變) ...
    # ... (這裡省略 Wait Loop 程式碼，請保留你原本的) ...
    - name: 3.1 Wait for Juju Apps Active
      shell: |
        # (保留你原本的 wait loop)
        sleep 1 # 佔位符

    # ... (4.1 & 4.2 Local Path Provisioner 保持不變) ...
    - name: 4.1 Install Local Path Provisioner
      shell: |
        juju ssh --model {{ test_model }} 0 -- \
        "kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.30/deploy/local-path-storage.yaml"
    
    - name: 4.2 Set Default StorageClass
      shell: |
        juju ssh --model {{ test_model }} 0 -- \
        "kubectl patch storageclass local-path -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'"

    # --- [新增] 5. 補齊 Run:ai 要求的 TLS Secret ---
    - name: 5.1 Generate Self-Signed Cert and Secret
      shell: |
        juju ssh --model {{ test_model }} 0 -- bash -s <<'EOS'
        # 建立 namespace
        kubectl create namespace runai-backend --dry-run=client -o yaml | kubectl apply -f -
        
        # 產生自簽憑證 (Run:ai 測試用)
        openssl req -x509 -newkey rsa:4096 -sha256 -days 3650 -nodes \
          -keyout private.pem -out fullchain.pem \
          -subj "/CN=runai.lab.local"
        
        # 依照官方文件建立 TLS Secret
        # 檢查是否已經存在，不存在才建立
        if ! kubectl get secret runai-backend-tls -n runai-backend; then
            kubectl create secret tls runai-backend-tls -n runai-backend \
              --cert=fullchain.pem \
              --key=private.pem
            echo "✅ TLS Secret created."
        fi
        EOS
      register: tls_secret

    # ... (原本的 Wait for K8s Nodes 保持不變) ...

    # --- 5. K8s Internal Check (Modified) ---
    - name: 5.1 Wait for K8s Nodes & System Pods
      shell: |
        juju ssh --model {{ test_model }} 0 -- bash -s <<'EOS'
        set -e
        echo "⏳ Waiting for K8s Nodes & System Pods (Timeout: 15min)..."
        
        # [修改點 6] 通用型檢查：等待 3 個節點都 Ready
        echo "   -> Waiting for 3 nodes to be Ready..."
        for i in {1..60}; do
           READY_COUNT=$(kubectl get nodes --no-headers | grep -w "Ready" | wc -l)
           if [ "$READY_COUNT" -ge 3 ]; then
               echo "   ✅ Found $READY_COUNT Ready nodes."
               break
           fi
           sleep 10
        done
        # Double check with wait command
        kubectl wait --for=condition=Ready node --all --timeout=600s

        # Wait for CoreDNS/Calico
        echo "   -> Waiting for System Pods..."
        kubectl -n kube-system wait pod -l k8s-app=calico-node --for=condition=Ready --timeout=600s
        kubectl -n kube-system wait pod -l k8s-app=kube-dns --for=condition=Ready --timeout=600s 2>/dev/null || \
        kubectl -n kube-system wait pod -l k8s-app=coredns --for=condition=Ready --timeout=600s 2>/dev/null || true

        echo "✅ K8s Infrastructure is Fully Ready for Run:ai."
        EOS
      changed_when: false
