# ========================================================
# K8S CLuster - 3 Nodes (1 Master, 2 Workers) for Run:ai
# ========================================================

# ========================================================
# PLAY 1: Infrastructure Layer
# ========================================================
- name: 1. Infrastructure - Deploy Modern Charmed Kubernetes
  hosts: maasjuju
  gather_facts: yes
  vars:
    test_model: "k8s-cluster-runai"
    k8s_channel: "1.31/stable"
    bundle_path: "{{ ansible_user_dir }}/charmed-k8s-modern.yaml"

  tasks:
    # --- 1. Environment Cleanup ---
    - name: 1.0 Force cleanup old model
      command: "juju destroy-model {{ test_model }} --no-prompt --force --destroy-storage"
      register: destroy_result
      failed_when: 
        - destroy_result.rc != 0 
        - '"not found" not in destroy_result.stderr'
      ignore_errors: yes

    - name: 1.1 Wait for model release
      pause:
        seconds: 15

    - name: 1.2 Create new Juju Model
      shell: juju add-model {{ test_model }}

    # --- 2. Bundle & Deploy (Critical Changes Here) ---
    - name: 2.1 Generate Local Bundle
      copy:
        dest: "{{ bundle_path }}"
        mode: '0644'
        content: |
          description: Slinky Modern K8s Cluster (Run:ai Ready)
          base: ubuntu@22.04
          
          # [修改點 1] 定義三台機器: 0(Master), 1(Worker/GPU), 2(Worker/GPU)
          machines:
            "0":
              constraints: tags=virtual
            "1":
              constraints: tags=slurm-node  # 建議改為 tags=gpu-node 若實際是 GPU 機
            "2":
              constraints: tags=slurm-node  # 第三台節點
          
          applications:
            easyrsa:
              charm: easyrsa
              channel: latest/stable
              num_units: 1
              to: ["0"]
            etcd:
              charm: etcd
              channel: latest/stable
              num_units: 1
              to: ["0"]
            
            kubernetes-control-plane:
              charm: kubernetes-control-plane
              channel: {{ k8s_channel }}
              num_units: 1
              to: ["0"]
              options:
                allow-privileged: "true"
                # Run:ai 有時需要 API server 允許特定參數，預設通常夠用
            
            # [修改點 2] Worker 擴增為 2 個單位
            kubernetes-worker:
              charm: kubernetes-worker
              channel: {{ k8s_channel }}
              num_units: 2
              to: ["1", "2"]
              expose: true
            
            calico:
              charm: calico
              channel: latest/stable
            containerd:
              charm: containerd
              channel: latest/stable
            
            # [修改點 3] 加入 Metrics Server (Run:ai 排程需要資源數據)
            kubernetes-metrics-server:
              charm: kubernetes-metrics-server
              channel: stable
              trust: true
            
            # [修改點 4] 加入 Ingress (Run:ai Dashboard 存取需求)
            ingress-nginx-k8s:
              charm: ingress-nginx-k8s
              channel: stable
              trust: true

          # [修改點 5] Relations 必須是 Root Key
          relations:
            - ["kubernetes-control-plane:kube-control", "kubernetes-worker:kube-control"]
            - ["kubernetes-control-plane:certificates", "easyrsa:client"]
            - ["etcd:certificates", "easyrsa:client"]
            - ["kubernetes-control-plane:etcd", "etcd:db"]
            - ["kubernetes-worker:certificates", "easyrsa:client"]
            - ["calico:etcd", "etcd:db"]
            - ["calico:cni", "kubernetes-control-plane:cni"]
            - ["calico:cni", "kubernetes-worker:cni"]
            - ["containerd:containerd", "kubernetes-worker:container-runtime"]
            - ["containerd:containerd", "kubernetes-control-plane:container-runtime"]
            # Metrics Server & Ingress Relations
            - ["kubernetes-metrics-server:kube-control", "kubernetes-control-plane:kube-control"]
            - ["ingress-nginx-k8s:kube-control", "kubernetes-control-plane:kube-control"]

    - name: 2.2 Deploy Bundle
      shell: |
        juju deploy {{ bundle_path }} --model {{ test_model }} --trust
      register: deploy_out
      until: deploy_out.rc == 0
      retries: 3
      delay: 10

    # --- 3. Infra Monitoring (Wait Loop) ---
    - name: 3.1 Wait for Juju Apps Active
      shell: |
        echo "⏳ Monitoring Juju status (Timeout: 45min)..."
        for i in {1..540}; do
          JSON=$(juju status --model {{ test_model }} --format json)
          # 檢查主要組件
          CP=$(echo "$JSON" | jq -r '.applications["kubernetes-control-plane"]["application-status"].current // "unknown"')
          WK=$(echo "$JSON" | jq -r '.applications["kubernetes-worker"]["application-status"].current // "unknown"')
          
          if [[ "$CP" == "active" && "$WK" == "active" ]]; then
            echo "✅ Core Juju Apps are Active!"
            exit 0
          fi
          if (( i % 12 == 0 )); then echo "   [$i/540] Waiting... (CP=$CP, WK=$WK)"; fi
          sleep 5
        done
        echo "❌ Timeout waiting for Juju apps."
        exit 1
      args:
        executable: /bin/bash
      register: juju_wait
      changed_when: false

    # --- 4. Local Path Provisioner (Storage) ---
    - name: 4.1 Install Local Path Provisioner
      shell: |
        juju ssh --model {{ test_model }} 0 -- \
        "kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.30/deploy/local-path-storage.yaml"
      register: install_sc
      changed_when: "'created' in install_sc.stdout or 'configured' in install_sc.stdout"

    - name: 4.2 Set Default StorageClass
      shell: |
        juju ssh --model {{ test_model }} 0 -- \
        "kubectl patch storageclass local-path -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'"
      register: patch_sc
      changed_when: "'patched' in patch_sc.stdout"

    # --- 5. K8s Internal Check (Modified) ---
    - name: 5.1 Wait for K8s Nodes & System Pods
      shell: |
        juju ssh --model {{ test_model }} 0 -- bash -s <<'EOS'
        set -e
        echo "⏳ Waiting for K8s Nodes & System Pods (Timeout: 15min)..."
        
        # [修改點 6] 通用型檢查：等待 3 個節點都 Ready
        echo "   -> Waiting for 3 nodes to be Ready..."
        for i in {1..60}; do
           READY_COUNT=$(kubectl get nodes --no-headers | grep -w "Ready" | wc -l)
           if [ "$READY_COUNT" -ge 3 ]; then
               echo "   ✅ Found $READY_COUNT Ready nodes."
               break
           fi
           sleep 10
        done
        # Double check with wait command
        kubectl wait --for=condition=Ready node --all --timeout=600s

        # Wait for CoreDNS/Calico
        echo "   -> Waiting for System Pods..."
        kubectl -n kube-system wait pod -l k8s-app=calico-node --for=condition=Ready --timeout=600s
        kubectl -n kube-system wait pod -l k8s-app=kube-dns --for=condition=Ready --timeout=600s 2>/dev/null || \
        kubectl -n kube-system wait pod -l k8s-app=coredns --for=condition=Ready --timeout=600s 2>/dev/null || true

        echo "✅ K8s Infrastructure is Fully Ready for Run:ai."
        EOS
      changed_when: false
