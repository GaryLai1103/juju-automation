# ==========================================
# SKU: HPC-Full-Stack-Reconstruction-v3-Parallel
# ==========================================
- name: 1. 控制端佈署與運算端硬體準備
  hosts: all
  gather_facts: no
  vars:
    slurm_model: "hpc-lab"
    target_os: "ubuntu@24.04"

  tasks:
    - name: 1.1 建立全新 Model
      command: "juju add-model {{ slurm_model }}"
      ignore_errors: yes

    # [優化]：先讓大腦上線
    - name: 1.2 佈署控制端 (slurmctld)
      # [保持不變]
      command: "juju deploy slurmctld -m {{ slurm_model }} --constraints tags=virtual --base {{ target_os }} --channel=23.11/stable"
      ignore_errors: yes

    - name: 1.3 向 MAAS 請求 GPU 節點 (強制指定 OS 版本)
      # [修正重點]：加入 --base 參數，確保 MAAS 不會亂選版本
      command: "juju add-machine -m {{ slurm_model }} --constraints tags=slurm-node --base {{ target_os }}"
      register: add_machine_result

    - name: 1.4 獲取運算節點 Machine ID
      # [保持不變]
      shell: "juju machines -m {{ slurm_model }} --format json | jq -r '.machines | keys | last'"
      register: target_machine_id

    - name: 1.5 等待運算節點實體啟動 (對齊表格 State)
      # 查詢表格中顯示的 "started" 狀態
      command: "juju wait-for machine {{ target_machine_id.stdout }} -m {{ slurm_model }} --query='machine-status=="running" && juju-status=="started" && modification-status=="idle"' --timeout 15m"

    - name: 1.6 安裝 NVIDIA 驅動與 DCGM (在佈署 slurmd 前)
      shell: |
        juju ssh -m {{ slurm_model }} {{ target_machine_id.stdout }} "
        sudo apt-get update && \
        sudo apt-get install -y nvidia-driver-580 nvidia-utils-580 datacenter-gpu-manager && \
        sudo systemctl enable --now nvidia-dcgm && \
        sudo nvidia-smi -pm 1
        "

# ==========================================
# 第二階段：佈署運算端並建立連動
# ==========================================
- name: 2. 佈署 Slurm 運算端
  hosts: all
  vars:
    slurm_model: "hpc-lab"
  tasks:
    - name: 2.1 佈署運算端至指定已就緒機器
      # 使用 1.4 抓到的動態 ID
      command: "juju deploy slurmd -m {{ slurm_model }} --to {{ target_machine_id.stdout }} --channel=23.11/stable"
      ignore_errors: yes

    - name: 2.2 建立整合關係 (大腦與手腳連線)
      command: "juju integrate slurmctld slurmd -m {{ slurm_model }}"
      ignore_errors: yes

# ==========================================
# 第三階段：深度修正 (插件、網路、DCGM Exporter)
# ==========================================
- name: 3. 執行自動化環境修正
  hosts: all
  tasks:
    - name: 3.1 修正控制端 (slurmctld)
      shell: |
        juju ssh -m hpc-lab slurmctld/0 "
        REAL_HOSTNAME=\$(hostname)
        sudo sed -i \"/127.0.0.1/s/$/ \$REAL_HOSTNAME slurm-master/\" /etc/hosts
        sudo sed -i 's/SlurmctldHost=.*/SlurmctldHost='\$REAL_HOSTNAME'/g' /etc/slurm/slurm.conf
        sudo sed -i 's/select\/cons_res/select\/cons_tres/g' /etc/slurm/slurm.conf
        sudo sed -i '/MungeSocketDir/d' /etc/slurm/slurm.conf
        sudo sed -i '1i SlurmdParameters=config_overrides' /etc/slurm/slurm.conf
        sudo mkdir -p /var/spool/slurmctld && sudo chown slurm:slurm /var/spool/slurmctld
        sudo systemctl restart munge && sudo systemctl restart slurmctld
        "

    - name: 3.2 修正運算端與安裝 DCGM Exporter
      shell: |
        juju ssh -m hpc-lab slurmd/0 "
        # 1. 網路與 Slurm 修正
        CTLD_IP=\$(juju ssh -m hpc-lab slurmctld/0 'hostname -I' | awk '{print \$1}')
        sudo sed -i '/slurm-master/d' /etc/hosts
        echo \"\$CTLD_IP slurm-master\" | sudo tee -a /etc/hosts
        sudo sed -i 's/select\/cons_res/select\/cons_tres/g' /etc/slurm/slurm.conf
        sudo sed -i '/MungeSocketDir/d' /etc/slurm/slurm.conf
        echo 'Name=gpu Type=rtx4090 File=/dev/nvidia0' | sudo tee /etc/slurm/gres.conf
        sudo sed -i '/NodeName=gpu-node01/c\NodeName=gpu-node01 CPUs=18 Sockets=1 CoresPerSocket=18 ThreadsPerCore=1 RealMemory=128646 Gres=gpu:rtx4090:1 State=UNKNOWN' /etc/slurm/slurm.conf
        
        # 2. DCGM Exporter 安裝
        if [ ! -f /usr/bin/dcgm-exporter ]; then
          wget https://github.com/NVIDIA/dcgm-exporter/releases/download/v3.3.5-3.4.1/dcgm-exporter_3.3.5-3.4.1_amd64.deb
          sudo dpkg -i dcgm-exporter_3.3.5-3.4.1_amd64.deb
          sudo systemctl enable --now dcgm-exporter
        fi

        sudo mkdir -p /run/slurm && sudo chown slurm:slurm /run/slurm
        sudo systemctl restart munge && sudo systemctl restart slurmd
        "

    - name: 3.3 最終節點喚醒
      shell: |
        juju ssh -m hpc-lab slurmctld/0 "sudo scontrol update nodename=gpu-node01 state=resume"
