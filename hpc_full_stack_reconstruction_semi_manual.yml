# =================================================================
# SKU: HPC-Native-v2.0 (Pure Ansible Stack)
# Infrastructure: Juju (OS Provisioning Only)
# Software: K3s + Slurm + NVIDIA Drivers (All Ansible)
# =================================================================

# --- 階段 1: 基礎建設 (向 MAAS 要乾淨的機器) ---
- name: 1. Juju 機器調度 (純 OS)
  hosts: maasjuju
  gather_facts: no
  vars:
    model_name: "hpc-lab"
    # [請確認] MAAS Tags
    master_constraints: "tags=virtual" 
    worker_constraints: "tags=slurm-node" 
    target_os: "ubuntu@24.04"

  tasks:
    - name: 1.1 建立 Juju Model
      command: "juju add-model {{ model_name }}"
      ignore_errors: yes

    - name: 1.2 請求 Machine 0 (未來的主控節點)
      # 只執行 add-machine，不 deploy 任何 charm
      command: "juju add-machine -m {{ model_name }} --constraints {{ master_constraints }} --base {{ target_os }}"
      register: machine0_req

    - name: 1.3 請求 Machine 1 (未來的運算節點)
      command: "juju add-machine -m {{ model_name }} --constraints {{ worker_constraints }} --base {{ target_os }}"
      register: machine1_req

    - name: 1.4 等待機器分配 IP (強制抓取 IPv4)
      shell: |
        # 迴圈檢查直到兩台機器都有 IPv4
        for i in {1..60}; do
          STATUS=$(juju status -m {{ model_name }} --format=json)
          
          # [修改點] 使用 select 和 test 正則表達式來過濾出 IPv4 (數字.數字...)
          # Machine 0 IP
          M0_IP=$(echo "$STATUS" | jq -r '.machines["0"]["ip-addresses"][] | select(test("^[0-9]+\\."))' | head -n 1)
          
          # Machine 1 IP
          M1_IP=$(echo "$STATUS" | jq -r '.machines["1"]["ip-addresses"][] | select(test("^[0-9]+\\."))' | head -n 1)
          
          # 確保抓到的不是空值
          if [[ -n "$M0_IP" && -n "$M1_IP" ]]; then
            echo "$M0_IP|$M1_IP"
            exit 0
          fi
          sleep 10
        done
        exit 1
      register: node_ips
      changed_when: false
      args:
        executable: /bin/bash

    - name: 1.5 解析 IP 並加入 Inventory
      set_fact:
        master_ip: "{{ node_ips.stdout.split('|')[0] }}"
        worker_ip: "{{ node_ips.stdout.split('|')[1] }}"

    - name: 1.6 註冊主機
      add_host:
        name: "{{ master_ip }}"
        groups: slurm_masters
        ansible_user: ubuntu
        # 若需要 SSH 跳板設定
        # ansible_ssh_common_args: '-o ProxyCommand="ssh -W %h:%p -q ubuntu@maasjuju"'

    - name: 1.7 註冊主機
      add_host:
        name: "{{ worker_ip }}"
        groups: slurm_workers
        ansible_user: ubuntu

    - name: 1.8 等待 SSH 通暢
      wait_for:
        host: "{{ item }}"
        port: 22
        timeout: 600
        search_regex: OpenSSH
      loop:
        - "{{ master_ip }}"
        - "{{ worker_ip }}"
      delegate_to: maasjuju

# --- 階段 2: 基礎環境 (Munge & User) ---
- name: 2. 基礎 Slurm 環境設置
  hosts: slurm_masters, slurm_workers
  become: yes
  vars:
    master_addr: "{{ groups['slurm_masters'][0] }}"
    worker_addr: "{{ groups['slurm_workers'][0] }}"
  
  tasks:
    - name: 2.1 設定 Hosts 解析
      blockinfile:
        path: /etc/hosts
        block: |
          {{ master_addr }} slurm-master
          {{ worker_addr }} gpu-node01

    - name: 2.2 建立 Slurm 帳號 (UID 同步)
      group: { name: slurm, gid: 64030, state: present }
    - name: 2.3 建立 Slurm 使用者
      user: { name: slurm, uid: 64030, group: slurm, shell: /bin/bash, state: present }

    - name: 2.4 安裝 Munge
      apt: { name: [munge, libmunge-dev], state: present, update_cache: yes }

# --- 階段 3: Munge Key 同步 ---
- name: 3. 同步 Munge Key
  hosts: slurm_masters
  become: yes
  tasks:
    - name: 3.1 生成 Key
      command: /usr/sbin/create-munge-key -f
      args: { creates: /etc/munge/munge.key }
    - name: 3.2 下載 Key
      fetch: { src: /etc/munge/munge.key, dest: /tmp/munge.key, flat: yes }

- name: 3.5 上傳 Key 到 Worker
  hosts: slurm_workers
  become: yes
  tasks:
    - name: 3.6 上傳 Key
      copy: { src: /tmp/munge.key, dest: /etc/munge/munge.key, owner: munge, group: munge, mode: '0400' }
    - name: 3.7 重啟 Munge
      systemd: { name: munge, state: restarted }

# --- 階段 4: Master 建置 (K3s + Slurmctld) ---
- name: 4. Master 軟體堆疊 (K3s & Slurm)
  hosts: slurm_masters
  become: yes
  vars:
    # RTX 4090 規格
    node_cpus: 18
    node_mem: 128646
  tasks:
    # --- 4A. K3s 安裝 ---
    - name: 4.1 安裝 K3s (Master Mode)
      shell: |
        curl -sfL https://get.k3s.io | sh -s - server \
          --write-kubeconfig-mode 644 \
          --disable traefik \
          --disable servicelb
      args:
        creates: /usr/local/bin/k3s

    - name: 4.2 等待 K3s 就緒
      shell: k3s kubectl get nodes | grep -q "Ready"
      register: k3s_ready
      until: k3s_ready.rc == 0
      retries: 20
      delay: 5

    # --- 4B. Slurmctld 安裝 ---
    - name: 4.3 安裝 Slurm Master
      apt: { name: [slurmctld, slurm-client], state: present }

    - name: 4.4 寫入 slurm.conf
      copy:
        dest: /etc/slurm/slurm.conf
        content: |
          ClusterName=hpc-lab
          SlurmctldHost=slurm-master
          MungeSocketPath=/var/run/munge/munge.socket.2
          ProctrackType=proctrack/cgroup
          ReturnToService=1
          SlurmUser=slurm
          StateSaveLocation=/var/spool/slurmctld
          SelectType=select/cons_tres
          SelectTypeParameters=CR_Core_Memory
          # 節點定義
          NodeName=gpu-node01 CPUs={{ node_cpus }} Sockets=1 CoresPerSocket={{ node_cpus }} ThreadsPerCore=1 RealMemory={{ node_mem }} Gres=gpu:rtx4090:1 State=UNKNOWN
          # 分區定義
          PartitionName=debug Nodes=gpu-node01 Default=YES MaxTime=INFINITE State=UP

    - name: 4.5 建立 cgroup.conf
      copy:
        dest: /etc/slurm/cgroup.conf
        content: "CgroupAutomount=yes\nConstrainCores=yes\nConstrainDevices=yes\nConstrainRAM=yes"

    - name: 4.6 啟動 Slurmctld
      systemd: { name: slurmctld, state: restarted, enabled: yes }

# --- 階段 5: Worker 建置 (GPU + Slurmd) ---
- name: 5. Worker 軟體堆疊
  hosts: slurm_workers
  become: yes
  tasks:
    - name: 5.1 安裝 Slurmd
      apt: { name: [slurmd, slurm-client, ubuntu-drivers-common], state: present }

    - name: 5.2 安裝 GPU 驅動
      shell: |
        if ! nvidia-smi; then
          ubuntu-drivers autoinstall
          echo "reboot_needed"
        fi
      register: driver_install
      ignore_errors: yes

    - name: 5.3 重開機生效
      reboot:
      when: driver_install.stdout is search("reboot_needed")

    - name: 5.4 寫入 gres.conf
      copy:
        dest: /etc/slurm/gres.conf
        content: "Name=gpu Type=rtx4090 File=/dev/nvidia0"

    - name: 5.5 同步 slurm.conf (與 Master 一致)
      copy:
        dest: /etc/slurm/slurm.conf
        content: |
          ClusterName=hpc-lab
          SlurmctldHost=slurm-master
          MungeSocketPath=/var/run/munge/munge.socket.2
          ProctrackType=proctrack/cgroup
          ReturnToService=1
          SlurmUser=slurm
          StateSaveLocation=/var/spool/slurmctld
          SelectType=select/cons_tres
          SelectTypeParameters=CR_Core_Memory
          NodeName=gpu-node01 CPUs=18 Sockets=1 CoresPerSocket=18 ThreadsPerCore=1 RealMemory=128646 Gres=gpu:rtx4090:1 State=UNKNOWN
          PartitionName=debug Nodes=gpu-node01 Default=YES MaxTime=INFINITE State=UP

    - name: 5.6 啟動 Slurmd
      systemd: { name: slurmd, state: restarted, enabled: yes }

# --- 階段 6: 收尾 (KubeSphere 準備) ---
- name: 6. 輸出 Kubeconfig
  hosts: slurm_masters
  become: yes
  tasks:
    - name: 6.1 抓取並修正 Kubeconfig (將 127.0.0.1 換成外部 IP)
      shell: |
        cat /etc/rancher/k3s/k3s.yaml | sed "s/127.0.0.1/{{ ansible_host }}/g"
      register: k3s_config

    - name: 6.2 顯示連線資訊
      debug:
        msg: 
          - "========================================================"
          - "K3s 安裝完成！(Juju 僅提供 OS)"
          - "請複製下方的設定檔內容，在 KubeSphere 進行 'Add Cluster' (Direct Connection)"
          - "--------------------------------------------------------"
          - "{{ k3s_config.stdout }}"
          - "========================================================"
