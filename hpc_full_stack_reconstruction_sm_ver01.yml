# ==========================================
# SKU: HPC-Hybrid-v7.0 (Native OS + Ansible App)
# 邏輯：Juju 佈署 OS，Ansible 手動安裝 K3s/Slurm/NVIDIA
# ==========================================
- name: 1. 基礎建設 (Juju 供應 OS)
  hosts: maasjuju
  gather_facts: no
  vars:
    slurm_model: "hpc-lab"
    target_os: "ubuntu@24.04"
    # 硬體標籤定義
    master_tags: "tags=virtual"
    worker_tags: "tags=slurm-node"

  tasks:
    - name: 1.1 初始化環境
      shell: |
        juju add-model {{ slurm_model }} || true
        # 請求 Machine 0 (Master)
        juju add-machine -m {{ slurm_model }} --constraints {{ master_tags }} --base {{ target_os }}
        # 請求 Machine 1 (Worker)
        juju add-machine -m {{ slurm_model }} --constraints {{ worker_tags }} --base {{ target_os }}

    - name: 1.2 等待兩台機器 SSH 啟動
      shell: |
        echo "Waiting for Machine 0 & 1..."
        while ! juju ssh -m {{ slurm_model }} 0 -- 'echo up' >/dev/null 2>&1; do sleep 5; done
        while ! juju ssh -m {{ slurm_model }} 1 -- 'echo up' >/dev/null 2>&1; do sleep 5; done
      changed_when: false

    - name: 1.2.1 根據 MAAS Tag 動態偵測 Machine ID
      shell: |
        STATUS=$(juju machines -m {{ slurm_model }} --format json)
        # 尋找帶有 virtual tag 的機器 ID (Master)
        M0=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ master_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        # 尋找帶有 slurm-node tag 的機器 ID (Worker)
        M1=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ worker_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        echo "$M0|$M1"
      register: detected_ids
      changed_when: false

    - name: 1.2.2 儲存 ID 變數
      set_fact:
        master_id: "{{ detected_ids.stdout.split('|')[0] }}"
        worker_id: "{{ detected_ids.stdout.split('|')[1] }}"

    - name: 1.3 Machine 0：安裝 K3s (手動模式)
      shell: |
        juju ssh -m {{ slurm_model }} {{ master_id }} '
          curl -sfL https://get.k3s.io | sudo sh -s - server \
            --write-kubeconfig-mode 644 \
            --disable traefik \
            --disable servicelb
          sudo kubectl get nodes
        '
      changed_when: false

    - name: 1.4 Machine 1：安裝 NVIDIA Driver (手動模式)
      shell: |
        juju ssh -m {{ slurm_model }} {{ worker_id }} '
          sudo apt-get update && sudo apt-get install -y ubuntu-drivers-common
          if ! nvidia-smi; then
            sudo ubuntu-drivers autoinstall
            sudo reboot
          fi
        '
      ignore_errors: yes

    - name: 1.5 等待 Worker 重啟完成
      shell: |
        echo "Waiting for Worker back..."
        sleep 30
        while ! juju ssh -m {{ slurm_model }} {{ worker_id }} -- 'nvidia-smi' >/dev/null 2>&1; do sleep 10; done
      changed_when: false

# ==========================================
# 第二階段：手動佈署 Slurm (去 Charm 化)
# ==========================================
- name: 2. Slurm 原生安裝與配置
  hosts: maasjuju
  vars:
    slurm_model: "hpc-lab"
    # 黃金規格數值
    node_cpus: 18
    node_mem: 128646
  tasks:
    - name: 2.0.1 根據 MAAS Tag 動態偵測 Machine ID
      shell: |
        STATUS=$(juju machines -m {{ slurm_model }} --format json)
        # 尋找帶有 virtual tag 的機器 ID (Master)
        M0=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ master_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        # 尋找帶有 slurm-node tag 的機器 ID (Worker)
        M1=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ worker_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        echo "$M0|$M1"
      register: detected_ids
      changed_when: false

    - name: 2.0.2 儲存 ID 變數
      set_fact:
        master_id: "{{ detected_ids.stdout.split('|')[0] }}"
        worker_id: "{{ detected_ids.stdout.split('|')[1] }}"
  
    - name: 2.1 抓取 Master IP (用於 hosts 解析)
      shell: "juju ssh -m {{ slurm_model }} {{ master_id }} 'hostname -I' | awk '{print $1}'"
      register: master_ip

    - name: 2.2 控制端安裝 Slurmctld
      shell: |
        juju ssh -m {{ slurm_model }} {{ master_id }} '
          sudo apt-get update && sudo apt-get install -y slurmctld munge
          sudo systemctl enable munge --now
        '

    - name: 2.3 運算端安裝 Slurmd
      shell: |
        MASTER_IP="{{ master_ip.stdout }}"
        juju ssh -m {{ slurm_model }} {{ worker_id }} "
          sudo apt-get update && sudo apt-get install -y slurmd munge
          echo '$MASTER_IP slurm-master' | sudo tee -a /etc/hosts
          sudo systemctl enable munge --now
        "

    - name: 2.4 同步 Munge Key (Master to Worker)
      shell: |
        # 從 Master 抓 Key 到跳板機，再傳給 Worker
        juju ssh -m {{ slurm_model }} {{ master_id }} 'sudo cat /etc/munge/munge.key' > /tmp/munge.key
        juju scp /tmp/munge.key {{ slurm_model }}:1:/tmp/munge.key
        juju ssh -m {{ slurm_model }} {{ worker_id }} 'sudo mv /tmp/munge.key /etc/munge/munge.key && sudo chown munge:munge /etc/munge/munge.key && sudo systemctl restart munge'
        rm /tmp/munge.key
    - name: 2.5 在 Worker 安裝 GPU 監控 (DCGM Exporter)
      shell: |
        juju ssh -m {{ slurm_model }} {{ worker_id }} "
          sudo snap install dcgm || true
          sudo snap start --enable dcgm.nv-hostengine
          sudo snap start --enable dcgm.dcgm-exporter
          sleep 5
          sudo snap restart dcgm.nv-hostengine
          sudo snap restart dcgm.dcgm-exporter
        "
      changed_when: true

# ==========================================
# 第三階段：設定檔深度寫入與啟動
# ==========================================
- name: 3. 最終設定與服務啟動
  hosts: maasjuju
  vars:
    slurm_model: "hpc-lab"
    node_cpus: 18
    node_mem: 128646
  tasks:
    - name: 3.0.1 根據 MAAS Tag 動態偵測 Machine ID
      shell: |
        STATUS=$(juju machines -m {{ slurm_model }} --format json)
        # 尋找帶有 virtual tag 的機器 ID (Master)
        M0=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ master_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        # 尋找帶有 slurm-node tag 的機器 ID (Worker)
        M1=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ worker_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        echo "$M0|$M1"
      register: detected_ids
      changed_when: false

    - name: 3.0.2 儲存 ID 變數
      set_fact:
        master_id: "{{ detected_ids.stdout.split('|')[0] }}"
        worker_id: "{{ detected_ids.stdout.split('|')[1] }}"
    
    - name: 3.1 寫入 Master slurm.conf (語法修正版)
      shell: |
        juju ssh -m {{ slurm_model }} {{ master_id }} "
        # 1. 建立必要目錄
        sudo mkdir -p /var/spool/slurmctld /var/run/slurm /var/log/slurm
        sudo chown -R slurm:slurm /var/spool/slurmctld /var/run/slurm /var/log/slurm
        
        # 2. 寫入設定檔 (移除不相容的 MungeSocketPath)
        sudo bash -c \"cat > /etc/slurm/slurm.conf <<'EOF'
        ClusterName=hpc-lab
        SlurmctldHost=\$(hostname)
        
        # [修正] 移除 MungeSocketPath，讓 AuthType 插件自行處理
        AuthType=auth/munge
        SlurmUser=slurm
        
        # PID 與路徑
        SlurmctldPidFile=/var/run/slurm/slurmctld.pid
        SlurmdPidFile=/var/run/slurm/slurmd.pid
        StateSaveLocation=/var/spool/slurmctld
        SlurmdSpoolDir=/var/spool/slurmd
        
        # 日誌
        SlurmctldLogFile=/var/log/slurm/slurmctld.log
        SlurmdLogFile=/var/log/slurm/slurmd.log
        
        # 插件設定
        GresTypes=gpu
        ProctrackType=proctrack/cgroup
        TaskPlugin=task/affinity,task/cgroup
        SelectType=select/cons_tres
        SelectTypeParameters=CR_Core_Memory
        
        # 節點定義
        NodeName=gpu-node01 CPUs=18 Sockets=1 CoresPerSocket=18 ThreadsPerCore=1 RealMemory=128646 Gres=gpu:rtx4090:1 State=UNKNOWN
        PartitionName=debug Nodes=gpu-node01 Default=YES MaxTime=INFINITE State=UP
        EOF\"
        
        # 3. 權限與啟動
        sudo chown slurm:slurm /etc/slurm/slurm.conf
        sudo systemctl restart munge
        sudo systemctl restart slurmctld
        "

    - name: 3.2 同步設定檔與 Munge Key (暴力同步版)
      shell: |
        # 1. 抓取 Master 的設定檔與 Key (Base64 確保安全傳輸)
        CONF_B64=$(juju ssh -m {{ slurm_model }} {{ master_id }} "sudo base64 -w0 /etc/slurm/slurm.conf")
        KEY_B64=$(juju ssh -m {{ slurm_model }} {{ master_id }} "sudo base64 -w0 /etc/munge/munge.key")
        
        # 2. 推送到 Worker 並強制配置
        juju ssh -m {{ slurm_model }} {{ worker_id }} "
          # 目錄與權限
          sudo mkdir -p /etc/slurm /var/spool/slurmd /var/run/slurm /var/log/slurm
          sudo chown -R slurm:slurm /etc/slurm /var/spool/slurmd /var/run/slurm /var/log/slurm
          
          # 寫入 Slurm 設定
          echo '$CONF_B64' | base64 -d | sudo tee /etc/slurm/slurm.conf > /dev/null
          
          # 寫入並保護 Munge Key (關鍵修正)
          echo '$KEY_B64' | base64 -d | sudo tee /etc/munge/munge.key > /dev/null
          sudo chown munge:munge /etc/munge/munge.key
          sudo chmod 400 /etc/munge/munge.key
          
          # 寫入 GPU 設定
          echo 'Name=gpu Type=rtx4090 File=/dev/nvidia0' | sudo tee /etc/slurm/gres.conf
          
          # 依序啟動服務
          sudo systemctl restart munge
          sudo systemctl restart slurmd
        "
      args:
        executable: /bin/bash

    - name: 3.3 最終喚醒節點
      shell: |
        # 先設為 Down 再 Resume，強制重置狀態
        juju ssh -m {{ slurm_model }} {{ master_id }} "sudo scontrol update nodename=gpu-node01 state=down reason='ansible_sync'"
        sleep 2
        juju ssh -m {{ slurm_model }} {{ master_id }} "sudo scontrol update nodename=gpu-node01 state=resume"
      changed_when: false
