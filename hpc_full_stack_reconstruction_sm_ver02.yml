# ==========================================
# SKU: HPC-Hybrid-v7.0 (Native OS + Ansible App)
# é‚è¼¯ï¼šJuju ä½ˆç½² OSï¼ŒAnsible æ‰‹å‹•å®‰è£ K3s/Slurm/NVIDIA
# ==========================================
- name: 1. åŸºç¤å»ºè¨­ (Juju ä¾›æ‡‰ OS)
  hosts: maasjuju
  gather_facts: no
  vars:
    slurm_model: "hpc-lab"
    target_os: "ubuntu@24.04"
    # ç¡¬é«”æ¨™ç±¤å®šç¾©
    master_tags: "tags=virtual"
    worker_tags: "tags=slurm-node"

  tasks:
    - name: 1.1 åˆå§‹åŒ–ç’°å¢ƒ
      shell: |
        juju add-model {{ slurm_model }} || true
        # è«‹æ±‚ Machine 0 (Master)
        juju add-machine -m {{ slurm_model }} --constraints {{ master_tags }} --base {{ target_os }}
        # è«‹æ±‚ Machine 1 (Worker)
        juju add-machine -m {{ slurm_model }} --constraints {{ worker_tags }} --base {{ target_os }}

    - name: 1.2 ç­‰å¾…å…©å°æ©Ÿå™¨ SSH å•Ÿå‹•
      shell: |
        echo "Waiting for Machine 0 & 1..."
        while ! juju ssh -m {{ slurm_model }} 0 -- 'echo up' >/dev/null 2>&1; do sleep 5; done
        while ! juju ssh -m {{ slurm_model }} 1 -- 'echo up' >/dev/null 2>&1; do sleep 5; done
      changed_when: false

    - name: 1.2.1 æ ¹æ“š MAAS Tag å‹•æ…‹åµæ¸¬ Machine ID
      shell: |
        STATUS=$(juju machines -m {{ slurm_model }} --format json)
        # å°‹æ‰¾å¸¶æœ‰ virtual tag çš„æ©Ÿå™¨ ID (Master)
        M0=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ master_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        # å°‹æ‰¾å¸¶æœ‰ slurm-node tag çš„æ©Ÿå™¨ ID (Worker)
        M1=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ worker_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        echo "$M0|$M1"
      register: detected_ids
      changed_when: false

    - name: 1.2.2 å„²å­˜ ID è®Šæ•¸
      set_fact:
        master_id: "{{ detected_ids.stdout.split('|')[0] }}"
        worker_id: "{{ detected_ids.stdout.split('|')[1] }}"

    - name: 1.3 Machine 0ï¼šå®‰è£ K3s (æ‰‹å‹•æ¨¡å¼)
      shell: |
        juju ssh -m {{ slurm_model }} {{ master_id }} '
          curl -sfL https://get.k3s.io | sudo sh -s - server \
            --write-kubeconfig-mode 644 \
            --disable traefik \
            --disable servicelb
          sudo kubectl get nodes
        '
      changed_when: false

    - name: 1.4 Machine 1ï¼šå®‰è£ NVIDIA Driver (æ‰‹å‹•æ¨¡å¼)
      shell: |
        juju ssh -m {{ slurm_model }} {{ worker_id }} '
          sudo apt-get update && sudo apt-get install -y ubuntu-drivers-common
          if ! nvidia-smi; then
            sudo ubuntu-drivers autoinstall
            sudo reboot
          fi
        '
      ignore_errors: yes

    - name: 1.5 ç­‰å¾… Worker é‡å•Ÿå®Œæˆ
      shell: |
        echo "Waiting for Worker back..."
        sleep 30
        while ! juju ssh -m {{ slurm_model }} {{ worker_id }} -- 'nvidia-smi' >/dev/null 2>&1; do sleep 10; done
      changed_when: false

# ==========================================
# ç¬¬äºŒéšæ®µï¼šæ‰‹å‹•ä½ˆç½² Slurm (å» Charm åŒ–)
# ==========================================
- name: 2. Slurm åŸç”Ÿå®‰è£èˆ‡é…ç½®
  hosts: maasjuju
  vars:
    slurm_model: "hpc-lab"
    # é»ƒé‡‘è¦æ ¼æ•¸å€¼
    node_cpus: 18
    node_mem: 128646
    # ç¡¬é«”æ¨™ç±¤å®šç¾©
    master_tags: "tags=virtual"
    worker_tags: "tags=slurm-node"
  tasks:
    - name: 2.0.1 æ ¹æ“š MAAS Tag å‹•æ…‹åµæ¸¬ Machine ID
      shell: |
        STATUS=$(juju machines -m {{ slurm_model }} --format json)
        # å°‹æ‰¾å¸¶æœ‰ virtual tag çš„æ©Ÿå™¨ ID (Master)
        M0=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ master_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        # å°‹æ‰¾å¸¶æœ‰ slurm-node tag çš„æ©Ÿå™¨ ID (Worker)
        M1=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ worker_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        echo "$M0|$M1"
      register: detected_ids
      changed_when: false

    - name: 2.0.2 å„²å­˜ ID è®Šæ•¸
      set_fact:
        master_id: "{{ detected_ids.stdout.split('|')[0] }}"
        worker_id: "{{ detected_ids.stdout.split('|')[1] }}"
  
    - name: 2.1 æŠ“å– Master IP (ç”¨æ–¼ hosts è§£æ)
      shell: "juju ssh -m {{ slurm_model }} {{ master_id }} 'hostname -I' | awk '{print $1}'"
      register: master_ip

    - name: 2.2 æ§åˆ¶ç«¯å®‰è£ Slurmctld
      shell: |
        juju ssh -m {{ slurm_model }} {{ master_id }} '
          sudo apt-get update && sudo apt-get install -y slurmctld munge
          sudo systemctl enable munge --now
        '

    - name: 2.3 é‹ç®—ç«¯å®‰è£ Slurmd
      shell: |
        MASTER_IP="{{ master_ip.stdout }}"
        juju ssh -m {{ slurm_model }} {{ worker_id }} "
          sudo apt-get update && sudo apt-get install -y slurmd munge
          echo '$MASTER_IP slurm-master' | sudo tee -a /etc/hosts
          sudo systemctl enable munge --now
        "

    - name: 2.4 åŒæ­¥ Munge Key (Master to Worker)
      shell: |
        # å¾ Master æŠ“ Key åˆ°è·³æ¿æ©Ÿï¼Œå†å‚³çµ¦ Worker
        juju ssh -m {{ slurm_model }} {{ master_id }} 'sudo cat /etc/munge/munge.key' > /tmp/munge.key
        juju scp /tmp/munge.key {{ slurm_model }}:{{ worker_id }}:/tmp/munge.key
        juju ssh -m {{ slurm_model }} {{ worker_id }} 'sudo mv /tmp/munge.key /etc/munge/munge.key && sudo chown munge:munge /etc/munge/munge.key && sudo systemctl restart munge'
        rm /tmp/munge.key
    - name: 2.5 åœ¨ Worker å®‰è£ GPU ç›£æ§ (DCGM Exporter)
      shell: |
        juju ssh -m {{ slurm_model }} {{ worker_id }} "
          sudo snap install dcgm || true
          sudo snap start --enable dcgm.nv-hostengine
          sleep 5
          sudo snap start --enable dcgm.dcgm-exporter
          sleep 5
          sudo snap restart dcgm.nv-hostengine
          sleep 10
          sudo snap restart dcgm.dcgm-exporter
        "
      changed_when: true

# ==========================================
# ç¬¬ä¸‰éšæ®µï¼šè¨­å®šæª”æ·±åº¦å¯«å…¥èˆ‡å•Ÿå‹•
# ==========================================
- name: 3. æœ€çµ‚è¨­å®šèˆ‡æœå‹™å•Ÿå‹•
  hosts: maasjuju
  vars:
    slurm_model: "hpc-lab"
    node_cpus: 18
    node_mem: 128646
    # ç¡¬é«”æ¨™ç±¤å®šç¾©
    master_tags: "tags=virtual"
    worker_tags: "tags=slurm-node"
  tasks:
    - name: 3.0.1 æ ¹æ“š MAAS Tag å‹•æ…‹åµæ¸¬ Machine ID
      shell: |
        STATUS=$(juju machines -m {{ slurm_model }} --format json)
        # å°‹æ‰¾å¸¶æœ‰ virtual tag çš„æ©Ÿå™¨ ID (Master)
        M0=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ master_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        # å°‹æ‰¾å¸¶æœ‰ slurm-node tag çš„æ©Ÿå™¨ ID (Worker)
        M1=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ worker_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        echo "$M0|$M1"
      register: detected_ids
      changed_when: false

    - name: 3.0.2 å„²å­˜ ID è®Šæ•¸
      set_fact:
        master_id: "{{ detected_ids.stdout.split('|')[0] }}"
        worker_id: "{{ detected_ids.stdout.split('|')[1] }}"
    
    - name: 3.1 å¯«å…¥ Master slurm.conf (èªæ³•ä¿®æ­£ç‰ˆ)
      shell: |
        juju ssh -m {{ slurm_model }} {{ master_id }} "
        # 1. å»ºç«‹å¿…è¦ç›®éŒ„
        sudo mkdir -p /var/spool/slurmctld /var/run/slurm /var/log/slurm
        sudo chown -R slurm:slurm /var/spool/slurmctld /var/run/slurm /var/log/slurm
        
        # 2. å¯«å…¥è¨­å®šæª” (ç§»é™¤ä¸ç›¸å®¹çš„ MungeSocketPath)
        sudo bash -c \"cat > /etc/slurm/slurm.conf <<'EOF'
        ClusterName=hpc-lab
        SlurmctldHost=\$(hostname)
        
        # [ä¿®æ­£] ç§»é™¤ MungeSocketPathï¼Œè®“ AuthType æ’ä»¶è‡ªè¡Œè™•ç†
        AuthType=auth/munge
        SlurmUser=slurm
        
        # PID èˆ‡è·¯å¾‘
        SlurmctldPidFile=/var/run/slurm/slurmctld.pid
        SlurmdPidFile=/var/run/slurm/slurmd.pid
        StateSaveLocation=/var/spool/slurmctld
        SlurmdSpoolDir=/var/spool/slurmd
        
        # æ—¥èªŒ
        SlurmctldLogFile=/var/log/slurm/slurmctld.log
        SlurmdLogFile=/var/log/slurm/slurmd.log
        
        # æ’ä»¶è¨­å®š
        GresTypes=gpu
        ProctrackType=proctrack/cgroup
        TaskPlugin=task/affinity,task/cgroup
        SelectType=select/cons_tres
        SelectTypeParameters=CR_Core_Memory
        
        # ç¯€é»å®šç¾©
        NodeName=gpu-node01 CPUs=18 Sockets=1 CoresPerSocket=18 ThreadsPerCore=1 RealMemory=128646 Gres=gpu:rtx4090:1 State=UNKNOWN
        PartitionName=debug Nodes=gpu-node01 Default=YES MaxTime=INFINITE State=UP
        EOF\"
        
        # 3. æ¬Šé™èˆ‡å•Ÿå‹•
        sudo chown slurm:slurm /etc/slurm/slurm.conf
        sudo systemctl restart munge
        sudo systemctl restart slurmctld
        "

    - name: 3.2 åŒæ­¥è¨­å®šæª”èˆ‡ Munge Key (æš´åŠ›åŒæ­¥ç‰ˆ)
      shell: |
        # 1. æŠ“å– Master çš„è¨­å®šæª”èˆ‡ Key (Base64 ç¢ºä¿å®‰å…¨å‚³è¼¸)
        CONF_B64=$(juju ssh -m {{ slurm_model }} {{ master_id }} "sudo base64 -w0 /etc/slurm/slurm.conf")
        KEY_B64=$(juju ssh -m {{ slurm_model }} {{ master_id }} "sudo base64 -w0 /etc/munge/munge.key")
        
        # 2. æ¨é€åˆ° Worker ä¸¦å¼·åˆ¶é…ç½®
        juju ssh -m {{ slurm_model }} {{ worker_id }} "
          # ç›®éŒ„èˆ‡æ¬Šé™
          sudo mkdir -p /etc/slurm /var/spool/slurmd /var/run/slurm /var/log/slurm
          sudo chown -R slurm:slurm /etc/slurm /var/spool/slurmd /var/run/slurm /var/log/slurm
          
          # å¯«å…¥ Slurm è¨­å®š
          echo '$CONF_B64' | base64 -d | sudo tee /etc/slurm/slurm.conf > /dev/null
          
          # å¯«å…¥ä¸¦ä¿è­· Munge Key (é—œéµä¿®æ­£)
          echo '$KEY_B64' | base64 -d | sudo tee /etc/munge/munge.key > /dev/null
          sudo chown munge:munge /etc/munge/munge.key
          sudo chmod 400 /etc/munge/munge.key
          
          # å¯«å…¥ GPU è¨­å®š
          echo 'Name=gpu Type=rtx4090 File=/dev/nvidia0' | sudo tee /etc/slurm/gres.conf
          
          # ä¾åºå•Ÿå‹•æœå‹™
          sudo systemctl restart munge
          sudo systemctl restart slurmd
        "
      args:
        executable: /bin/bash

    - name: 3.3 æœ€çµ‚å–šé†’ç¯€é»
      shell: |
        # å…ˆè¨­ç‚º Down å† Resumeï¼Œå¼·åˆ¶é‡ç½®ç‹€æ…‹
        juju ssh -m {{ slurm_model }} {{ master_id }} "sudo scontrol update nodename=gpu-node01 state=down reason='ansible_sync'"
        sleep 2
        juju ssh -m {{ slurm_model }} {{ master_id }} "sudo scontrol update nodename=gpu-node01 state=resume"
      changed_when: false

    - name: 3.4 é…ç½® K3s ç›£æ§å°æ¥ (GPU Metrics to K8s)
      shell: |
        # 1. åœ¨æœ¬åœ°åŸ·è¡Œ juju æŒ‡ä»¤æŠ“å– Worker IP
        W_IP=$(juju ssh -m {{ slurm_model }} {{ worker_id }} 'hostname -I' | awk '{print $1}')
        
        # 2. åœ¨ Master (Machine 0) é€éä¸€è¡ŒæŒ‡ä»¤å®Œæˆå‘½åç©ºé–“èˆ‡æ–‡ä»¶å¯«å…¥
        # æˆ‘å€‘å°‡ YAML å…§å®¹å¯«å…¥ /tmp/gpu-metrics.yaml é¿é–‹ EOF ç¸®æ’å•é¡Œ
        juju ssh -m {{ slurm_model }} {{ master_id }} "
          sudo kubectl create ns kubesphere-monitoring-system --dry-run=client -o yaml | sudo kubectl apply -f -
          
          echo \"apiVersion: v1
        kind: Endpoints
        metadata:
          name: gpu-metrics-worker
          namespace: kubesphere-monitoring-system
        subsets:
          - addresses:
              - ip: $W_IP
            ports:
              - name: metrics
                port: 9400
        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: gpu-metrics-worker
          namespace: kubesphere-monitoring-system
          labels:
            app: dcgm-exporter
        spec:
          ports:
            - name: metrics
              port: 9400
              targetPort: 9400\" > /tmp/gpu-metrics.yaml

          sudo kubectl apply -f /tmp/gpu-metrics.yaml
        "
      vars:
        slurm_model: "hpc-lab"
# ==========================================
# ç¬¬å››éšæ®µï¼šå¤šå¢é›†ç™»éŒ„èˆ‡ç›£æ§è‡ªå‹•æ¿€æ´»
# ==========================================
- name: 4. Register Cluster to KubeSphere and Enable Monitoring
  hosts: maasjuju
  vars:
    # è«‹æ ¹æ“š AWX Survey å‚³å…¥æˆ–æ‰‹å‹•å®šç¾©
    k8s_cluster: "hpc-native-cluster" 
    ks_host_ip: "192.168.100.4"
    ks_host_user: "ubuntu"
    slurm_model: "hpc-lab"

  tasks:
    # ----------------------------------------------------------------
    # 1. åœ¨ Machine 0 å–å¾— K3s Config
    # ----------------------------------------------------------------
    - name: 4.1 å–å¾— Machine 0 çš„ Kubeconfig (Base64)
      shell: |
        # æŠ“å– IP ä¸¦å°‡ 127.0.0.1 æ›æˆå¯¦é«” IPï¼Œè½‰ç‚º Base64
        NODE_IP=$(juju ssh -m {{ slurm_model }} {{ master_id }} 'hostname -I' | awk '{print $1}')
        juju ssh -m {{ slurm_model }} {{ master_id }} "sudo cat /etc/rancher/k3s/k3s.yaml | sed 's/127.0.0.1/'\$NODE_IP'/g' | base64 -w 0"
      register: k3s_kubeconfig_b64

    - name: 4.2 å°‡ KubeSphere Host åŠ å…¥è‡¨æ™‚ Inventory
      add_host:
        name: "ks_host"
        ansible_host: "{{ ks_host_ip }}"
        ansible_user: "{{ ks_host_user }}"
        cluster_name: "{{ k8s_cluster }}"
        cluster_config: "{{ k3s_kubeconfig_b64.stdout }}"

# ==========================================
# ç¬¬äº”éšæ®µï¼šåœ¨ KubeSphere Host åŸ·è¡Œè¨»å†Šèˆ‡ç›£æ§å®£å‘Š
# ==========================================
- name: 5. åŸ·è¡Œ KubeSphere è³‡æºæ³¨å…¥
  hosts: ks_host
  gather_facts: no
  tasks:
    - name: 5.1 æ³¨å…¥ Cluster CRD (ç›´æ¥é€£æ¥æ¨¡å¼)
      shell: |
        cat <<EOF | kubectl apply -f -
        apiVersion: cluster.kubesphere.io/v1alpha1
        kind: Cluster
        metadata:
          name: {{ cluster_name }}
          labels:
            kubesphere.io/managed: "true"
            cluster-role.kubesphere.io/member: ""
            cluster-role.kubesphere.io/public: ""
        spec:
          provider: k3s
          joinFederation: true
          connection:
            type: direct
            kubeconfig: {{ cluster_config }}
        EOF

    - name: 5.2 è‡ªå‹•å•Ÿç”¨ WhizardTelemetry ç›£æ§ä»£ç†
      # é€™æ˜¯å°æ‡‰æ‚¨åœ–ç‰‡ä¸­ "Agent Config" çš„è‡ªå‹•åŒ–æ­¥é©Ÿ
      shell: |
        cat <<EOF | kubectl apply -f -
        apiVersion: extensions.kubesphere.io/v1alpha1
        kind: ClusterExtension
        metadata:
          name: whizard-telemetry-monitoring
        spec:
          clusterName: {{ cluster_name }}
          enabled: true
          # é€™è£¡å¯ä»¥é å…ˆé…ç½® GPU æŒ‡æ¨™çš„æŠ“å–è¦å‰‡
          config: |
            monitoring:
              gpu:
                enabled: true
        EOF
      register: extension_result
      ignore_errors: yes

    - name: ğŸš€ è¨»å†ŠæˆåŠŸè¨Šæ¯
      debug:
        msg: "ğŸ‰ é›†ç¾¤ {{ cluster_name }} å·²æˆåŠŸè¨»å†Šã€‚è«‹å‰å¾€ KubeSphere æ“´å±•ä¸­å¿ƒæŸ¥çœ‹ Agent ç‹€æ…‹ã€‚"
