# ========================================================
# SKU: Slinky-SlurmOnK8s-Standard
# ä¾ä½ å‰›å‰›æˆåŠŸæµç¨‹æ¨™æº–åŒ–ï¼š
#   - (optional) NFS server + nfs-subdir-external-provisioner
#   - cert-manager
#   - slurm-operator-crds + slurm-operator
#   - nvidia k8s-device-plugin
#   - slurm chart (OCI) + values (fix partition / fix duplicate extraConf / keep gres autodetect)
# ========================================================

- name: Application Layer - Deploy Slinky Slurm on K8s (Standard)
  hosts: maasjuju
  gather_facts: yes

  vars:
    # -----------------------------
    # Juju / Cluster
    # -----------------------------
    test_model: "slinky-cluster"

    # ä½ ç¾åœ¨çš„æ‹“æ’²ï¼šmachine0=control-plane(slurmhn)ï¼Œmachine1=gpu worker(gpu-node01)
    # å¦‚æœä¸æ˜¯ 0/1ï¼Œæ”¹é€™è£¡å³å¯
    machine_cp_id: "0"
    machine_gpu_id: "1"

    # -----------------------------
    # Namespaces / Release Names
    # -----------------------------
    cert_ns: "cert-manager"
    operator_ns: "slinky"
    slurm_ns: "slurm"
    nfs_ns: "nfs-provisioner"
    device_plugin_ns: "nvidia-device-plugin"

    cert_release: "cert-manager"
    operator_crds_release: "slurm-operator-crds"
    operator_release: "slurm-operator"
    slurm_release: "slurm"
    nfs_release: "nfs-provisioner"
    device_plugin_release: "nvidia-device-plugin"

    # -----------------------------
    # Versions (ä½ å¯å›ºå®šä»¥åˆ©é‡ç¾)
    # -----------------------------
    cert_manager_version: "v1.19.2"
    slinky_chart_version: "1.0.1"     # slurm / slurm-operator / slurm-operator-crds chart version
    slinky_app_version: "25.11"       # åƒè€ƒç”¨ï¼ˆä¸æ˜¯ helm åƒæ•¸ï¼‰
    # Device plugin chart ç‰ˆæœ¬ä½ å¯è‡ªè¡Œå›ºå®šï¼›ä¸å¡«ä¹Ÿå¯ä»¥
    device_plugin_chart_version: ""   # e.g. "0.17.1" (ç•™ç©º=ä½¿ç”¨ repo æœ€æ–°)

    # -----------------------------
    # Node naming (ç”¨ä¾† nodeSelector / label / taint)
    # -----------------------------
    gpu_k8s_node_name: "gpu-node01"   # kubectl get nodes çš„åç¨±

    # ä½ ç›®å‰çš„ GPU taint
    gpu_taint_key: "node-role.anxpert/gpu"
    gpu_taint_value: "true"
    gpu_taint_effect: "NoSchedule"

    # æœƒè£œä¸€å€‹ labelï¼Œè®“ device-plugin å¥½é¸æ“‡ç¯€é»ï¼ˆä¸”è·Ÿ taint key åŒåæ¯”è¼ƒç›´è¦ºï¼‰
    gpu_label_key: "node-role.anxpert/gpu"
    gpu_label_value: "true"

    # -----------------------------
    # Optional: NFS (for standardization)
    # -----------------------------
    nfs_enabled: true
    # NFS export path on control-plane node
    nfs_export_dir: "/srv/nfs/k8s"
    # provisioner ä½¿ç”¨çš„å­è·¯å¾‘ï¼ˆPV æœƒåœ¨æ­¤åº•ä¸‹è‡ªå‹•å»ºç«‹å­ç›®éŒ„ï¼‰
    nfs_provisioner_path: "/srv/nfs/k8s"
    nfs_storage_class: "nfs-rwx"
    # ä½ è¦ä¸è¦è®“ slurm controller çš„ statesave PVC ä½¿ç”¨ NFS çš„ storageClass
    #ï¼ˆä½ å‰›å‰›æˆåŠŸä¸ä¸€å®šéœ€è¦ï¼Œä½†æ¨™æº–åŒ–æ™‚å¸¸æœƒæƒ³æŒ‡å®šï¼‰
    slurm_controller_storageclass: "nfs-rwx"

    # -----------------------------
    # Optional: Reset namespaces (æ¸¬è©¦é‡ä½ˆç½²å»ºè­° true)
    # -----------------------------
    reset_before_install: true

  tasks:
    # -----------------------------------------------------------
    # 1.0) Runtime Discovery - Machine IPs
    # -----------------------------------------------------------
    - name: 1.1 Get Juju status JSON
      shell: "juju status --model {{ test_model }} --format json"
      register: juju_status_raw
      changed_when: false

    - name: 1.2 Extract machine IPs
      set_fact:
        machine0_ip: "{{ (juju_status_raw.stdout | from_json).machines[machine_cp_id]['ip-addresses'] | select('match','^[0-9.]+$') | first }}"
        machine1_ip: "{{ (juju_status_raw.stdout | from_json).machines[machine_gpu_id]['ip-addresses'] | select('match','^[0-9.]+$') | first }}"

    - name: 1.3 Show detected IPs
      debug:
        msg:
          - "Control-plane machine {{ machine_cp_id }} IP: {{ machine0_ip }}"
          - "GPU worker   machine {{ machine_gpu_id }} IP: {{ machine1_ip }}"

    # -----------------------------------------------------------
    # 2.0) Install Helm (machine0)
    # -----------------------------------------------------------
    - name: 2.1 Ensure helm installed on machine0
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          if ! command -v helm >/dev/null 2>&1; then
            echo "[INFO] Installing Helm..."
            curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          else
            echo "[INFO] Helm already installed: $(helm version --short || true)"
          fi
        '
      changed_when: false

    # -----------------------------------------------------------
    # 3.0) Optional Reset (namespaces)
    # -----------------------------------------------------------
    - name: 3.1 Reset namespaces (slurm/slinky/cert-manager/device-plugin/nfs)
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          for ns in {{ slurm_ns }} {{ operator_ns }} {{ cert_ns }} {{ device_plugin_ns }} {{ nfs_ns }}; do
            kubectl delete ns "$ns" --ignore-not-found=true --wait=true || true
          done
        '
      when: reset_before_install | bool
      changed_when: true

    # -----------------------------------------------------------
    # 4.0) Optional NFS: server + provisioner (machine0)
    # -----------------------------------------------------------
    - name: 4.1 (NFS) Install nfs-kernel-server on machine0
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          sudo apt-get update -y
          sudo apt-get install -y nfs-kernel-server
        '
      when: nfs_enabled | bool
      changed_when: true

    - name: 4.2 (NFS) Create export dir + export config
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          sudo mkdir -p "{{ nfs_export_dir }}"
          sudo chown -R nobody:nogroup "{{ nfs_export_dir }}"
          sudo chmod 0777 "{{ nfs_export_dir }}"

          # å…è¨±ä½ çš„ cluster ç¶²æ®µå­˜å–ï¼ˆä½ å¯è‡ªè¡Œæ”¶æ–‚ï¼‰
          EXPORT_LINE="{{ nfs_export_dir }} *(rw,sync,no_subtree_check,no_root_squash)"
          if ! sudo grep -qF "$EXPORT_LINE" /etc/exports; then
            echo "$EXPORT_LINE" | sudo tee -a /etc/exports >/dev/null
          fi

          sudo exportfs -ra
          sudo systemctl restart nfs-kernel-server
          sudo exportfs -v
        '
      when: nfs_enabled | bool
      changed_when: true

    - name: 4.3 (NFS) Install nfs-subdir-external-provisioner (creates StorageClass)
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          kubectl get ns {{ nfs_ns }} >/dev/null 2>&1 || kubectl create ns {{ nfs_ns }}

          helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ || true
          helm repo update

          # å®‰è£ provisioner
          helm upgrade --install {{ nfs_release }} nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
            -n {{ nfs_ns }} \
            --set nfs.server={{ machine0_ip }} \
            --set nfs.path={{ nfs_provisioner_path }} \
            --set storageClass.name={{ nfs_storage_class }} \
            --set storageClass.defaultClass=false \
            --set storageClass.archiveOnDelete=false

          # ç­‰ DS ready
          kubectl -n {{ nfs_ns }} rollout status deploy/{{ nfs_release }} --timeout=180s || true
          kubectl get sc | egrep -n "{{ nfs_storage_class }}|NAME" || true
        '
      when: nfs_enabled | bool
      changed_when: true

    # -----------------------------------------------------------
    # 5.0) Prepare GPU node label/taint consistency (machine0)
    # -----------------------------------------------------------
    - name: 5.1 Ensure GPU node label exists (for nodeSelector)
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          kubectl label node {{ gpu_k8s_node_name }} {{ gpu_label_key }}={{ gpu_label_value }} --overwrite
          kubectl get node {{ gpu_k8s_node_name }} --show-labels | tr "," "\n" | egrep -n "{{ gpu_label_key }}|kubernetes.io/hostname"
        '
      changed_when: true

    - name: 5.2 Ensure GPU node taint exists (NoSchedule)
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          # è‹¥å·²å­˜åœ¨ä¸æœƒå ±éŒ¯ï¼ˆæˆ–ä½ å¯æ”¹æˆå…ˆæª¢æŸ¥ï¼‰
          kubectl taint node {{ gpu_k8s_node_name }} {{ gpu_taint_key }}={{ gpu_taint_value }}:{{ gpu_taint_effect }} --overwrite || true
          kubectl describe node {{ gpu_k8s_node_name }} | egrep -i "Taints|{{ gpu_taint_key }}" || true
        '
      changed_when: true

    # -----------------------------------------------------------
    # 6.0) Install NVIDIA k8s-device-plugin (machine0)
    # -----------------------------------------------------------
    - name: 6.1 Install NVIDIA device plugin via Helm (tolerate your GPU taint)
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          kubectl get ns {{ device_plugin_ns }} >/dev/null 2>&1 || kubectl create ns {{ device_plugin_ns }}

          helm repo add nvdp https://nvidia.github.io/k8s-device-plugin || true
          helm repo update

          # çµ„å‡º version åƒæ•¸ï¼ˆè‹¥ device_plugin_chart_version ç•™ç©ºå‰‡ä¸å¸¶ --versionï¼‰
          VER_ARG=""
          if [ -n "{{ device_plugin_chart_version }}" ]; then
            VER_ARG="--version {{ device_plugin_chart_version }}"
          fi

          # å®‰è£ / å‡ç´š
          # åªè·‘åœ¨ä½ æ¨™è¨˜çš„ GPU nodesï¼Œä¸¦å®¹å¿ NoSchedule taint
          helm upgrade --install {{ device_plugin_release }} nvdp/nvidia-device-plugin \
            -n {{ device_plugin_ns }} \
            $VER_ARG \
            --set nodeSelector."{{ gpu_label_key }}"="{{ gpu_label_value }}" \
            --set tolerations[0].key="{{ gpu_taint_key }}" \
            --set tolerations[0].operator="Equal" \
            --set tolerations[0].value="{{ gpu_taint_value }}" \
            --set tolerations[0].effect="{{ gpu_taint_effect }}"

          # ç­‰ daemonset readyï¼ˆchart æœ‰æ™‚æ˜¯ daemonsetï¼‰
          kubectl -n {{ device_plugin_ns }} get ds -o wide || true
          DS=$(kubectl -n {{ device_plugin_ns }} get ds -o jsonpath="{.items[0].metadata.name}" 2>/dev/null || true)
          if [ -n "$DS" ]; then
            kubectl -n {{ device_plugin_ns }} rollout status ds/$DS --timeout=180s || true
          fi

          # é©—è­‰ GPU resource æœ‰å‡ºç¾
          kubectl get node {{ gpu_k8s_node_name }} -o jsonpath="{.status.capacity.nvidia\.com/gpu}{\"\n\"}" || true
        '
      changed_when: true

    # -----------------------------------------------------------
    # 7.0) Install cert-manager (machine0)
    # -----------------------------------------------------------
    - name: 7.1 Install cert-manager (required by webhook)
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          kubectl get ns {{ cert_ns }} >/dev/null 2>&1 || kubectl create ns {{ cert_ns }}

          helm repo add jetstack https://charts.jetstack.io || true
          helm repo update

          helm upgrade --install {{ cert_release }} jetstack/cert-manager \
            -n {{ cert_ns }} \
            --version {{ cert_manager_version }} \
            --set crds.enabled=true

          kubectl -n {{ cert_ns }} rollout status deploy/{{ cert_release }} --timeout=240s || true
          kubectl -n {{ cert_ns }} get pods -o wide
        '
      changed_when: true

    # -----------------------------------------------------------
    # 8.0) Install slurm-operator CRDs + operator (machine0)
    # -----------------------------------------------------------
    - name: 8.1 Install slurm-operator-crds (OCI)
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          helm upgrade --install {{ operator_crds_release }} oci://ghcr.io/slinkyproject/charts/slurm-operator-crds \
            --version {{ slinky_chart_version }}

          # CRDs å‡ºç¾å³å¯
          kubectl get crd | egrep "slinky\.slurm\.net" || true
        '
      changed_when: true

    - name: 8.2 Install slurm-operator (OCI) in operator_ns
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          kubectl get ns {{ operator_ns }} >/dev/null 2>&1 || kubectl create ns {{ operator_ns }}

          helm upgrade --install {{ operator_release }} oci://ghcr.io/slinkyproject/charts/slurm-operator \
            -n {{ operator_ns }} \
            --version {{ slinky_chart_version }}

          kubectl -n {{ operator_ns }} get pods -o wide
        '
      changed_when: true

    # -----------------------------------------------------------
    # 9.0) Deploy Slurm (OCI) with the "stable values" you ended up using
    # -----------------------------------------------------------
    - name: 9.1 Write slurm values (stable) to machine0
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          cat > /home/ubuntu/slinky-slurm-values.yaml <<EOF
controller:
  # æ¸…ç©ºé¿å…è·Ÿ chart è‡ªå·±ç”¢ç”Ÿå…§å®¹é‡è¤‡/æ‰“æ¶ï¼ˆä½ ä¹‹å‰é‡åˆ° duplicate partition å°±æ˜¯é€™é¡å‹ï¼‰
  extraConf: ""
  extraConfMap: {}
  persistence:
    enabled: true
    {{- " "}}
    {{- ""}}

nodesets:
  slinky:
    enabled: true
    # é—œæ‰ nodeset è‡ªå‹•ç”Ÿ partitionï¼Œé¿å…ä¾è³´ Feature/NodeSet æ³¢å‹•é€ æˆ partition 0 n/a
    partition:
      enabled: false
    useResourceLimits: true
    podSpec:
      nodeSelector:
        kubernetes.io/hostname: {{ gpu_k8s_node_name }}
        kubernetes.io/os: linux
      tolerations:
      - key: "{{ gpu_taint_key }}"
        operator: "Equal"
        value: "{{ gpu_taint_value }}"
        effect: "{{ gpu_taint_effect }}"
    slurmd:
      resources:
        limits:
          nvidia.com/gpu: 1

partitions:
  all:
    enabled: true
    nodesets: [ALL]
    configMap:
      Default: "YES"
      MaxTime: UNLIMITED
      State: UP

  slinky:
    enabled: true
    nodesets: [ALL]
    configMap:
      Default: "NO"
      MaxTime: UNLIMITED
      State: UP

configFiles:
  gres.conf: |
    AutoDetect=nvidia
EOF

          # è‹¥ä½ æƒ³å¼·åˆ¶ controller statesave ä¹Ÿèµ° NFS SCï¼Œå¯æ‰“é–‹ä¸‹é¢å…©è¡Œï¼ˆå‰æ nfs_enabled=true ä¸” SC å­˜åœ¨ï¼‰
          if [ "{{ nfs_enabled | bool | lower }}" = "true" ] && kubectl get sc {{ slurm_controller_storageclass }} >/dev/null 2>&1; then
            echo "[INFO] Patch controller.persistence.storageClassName={{ slurm_controller_storageclass }}"
            yq -i ".controller.persistence.storageClassName = \"{{ slurm_controller_storageclass }}\"" /home/ubuntu/slinky-slurm-values.yaml || true
          fi

          echo "=== /home/ubuntu/slinky-slurm-values.yaml ==="
          cat /home/ubuntu/slinky-slurm-values.yaml
        '
      changed_when: true

    - name: 9.2 Install/Upgrade slurm chart (OCI) in slurm_ns
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          kubectl get ns {{ slurm_ns }} >/dev/null 2>&1 || kubectl create ns {{ slurm_ns }}

          helm upgrade --install {{ slurm_release }} oci://ghcr.io/slinkyproject/charts/slurm \
            -n {{ slurm_ns }} \
            --version {{ slinky_chart_version }} \
            -f /home/ubuntu/slinky-slurm-values.yaml

          kubectl -n {{ slurm_ns }} get pods -o wide
        '
      changed_when: true

    # -----------------------------------------------------------
    # 10.0) Restart controller + worker to make sure config settled
    # -----------------------------------------------------------
    - name: 10.1 Restart slurm controller + worker pods
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          # ç›´æ¥åˆª pod è®“å®ƒå€‘é‡å»ºï¼ˆä½ å¯¦æ¸¬æœ€ç›´è¦ºï¼‰
          kubectl -n {{ slurm_ns }} delete pod slurm-controller-0 slurm-worker-slinky-0 --ignore-not-found=true || true
          kubectl -n {{ slurm_ns }} get pods -w --timeout=180s || true
          kubectl -n {{ slurm_ns }} get pods -o wide
        '
      changed_when: true

    # -----------------------------------------------------------
    # 11.0) Verification (inside slurm-controller pod)
    # -----------------------------------------------------------
    - name: 11.1 Verify partitions + GPU GRES + run srun (inside slurmctld pod)
      shell: |
        juju ssh --model {{ test_model }} {{ machine_cp_id }} -- bash -lc '
          set -e
          echo "== pods =="
          kubectl -n {{ slurm_ns }} get pods -o wide

          echo
          echo "== slurm.conf partitions =="
          kubectl -n {{ slurm_ns }} exec -it slurm-controller-0 -c slurmctld -- bash -lc \
            "grep -n \"^PartitionName=\" /etc/slurm/slurm.conf || true"

          echo
          echo "== sinfo =="
          kubectl -n {{ slurm_ns }} exec -it slurm-controller-0 -c slurmctld -- bash -lc \
            "sinfo -o \"%P %a %l %D %t %N\""

          echo
          echo "== node gres =="
          kubectl -n {{ slurm_ns }} exec -it slurm-controller-0 -c slurmctld -- bash -lc \
            "scontrol show node slinky-0 | egrep -i \"Gres=|CfgTRES=|State=|Reason=\" || true"

          echo
          echo "== srun GPU test (default/all) =="
          kubectl -n {{ slurm_ns }} exec -it slurm-controller-0 -c slurmctld -- bash -lc \
            "srun -N1 -n1 --gres=gpu:1 bash -lc \"ls -l /dev/nvidia0 && echo GPU_OK\""

          echo
          echo "== srun GPU test (partition slinky) =="
          kubectl -n {{ slurm_ns }} exec -it slurm-controller-0 -c slurmctld -- bash -lc \
            "srun -p slinky -N1 -n1 --gres=gpu:1 bash -lc \"ls -l /dev/nvidia0 && echo GPU_OK\""
        '
      changed_when: false

    - name: 12.0 Done
      debug:
        msg:
          - "ğŸ‰ Slinky Slurm on K8s å·²å®Œæˆæ¨™æº–åŒ–éƒ¨ç½²"
          - "åŒ…å«ï¼šNFS(å¯é¸) + NVIDIA device-plugin + cert-manager + slurm-operator + slurm chart"
          - "é©—è­‰è«‹çœ‹æœ€å¾Œä¸€æ­¥ï¼šsinfo / scontrol / srun GPU_OK"
