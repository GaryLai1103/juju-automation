---
# ========================================================
# SKU: Slinky-SlurmOnK8s-Standardized (Merged, Repeatable)
#
# Two-mode:
#   reset_before_install=true  : clean rebuild (testing)
#   reset_before_install=false : normal upgrade (daily)
#
# Includes:
#   1) NFS server (machine0) + nfs-subdir-external-provisioner + StorageClass
#   2) NVIDIA k8s-device-plugin (Helm) with tolerations + nodeSelector + affinity={}
#   3) cert-manager + slurm-operator-crds + slurm-operator + slurm (OCI chart)
#
# Pitfalls avoided:
#   - NEVER kubectl patch Controller.extraConf (Helm conflict)
#   - Partitions do NOT depend on Feature
#   - Device-plugin MUST toleration + nodeSelector; also clear affinity to avoid NFD dependency
# ========================================================

- name: Application Layer - Deploy Slinky Slurm on K8s (Standardized Merged)
  hosts: maasjuju
  gather_facts: yes

  vars:
    # ---- Mode ----
    reset_before_install: true

    # ---- Juju / machines ----
    test_model: "slinky-cluster"
    machine0: "0"                 # control-plane node (slurmhn)
    machine1: "1"                 # gpu worker node (gpu-node01)

    # ---- K8s nodes ----
    controlplane_hostname: "slurmhn"
    gpu_node_hostname: "gpu-node01"

    # ---- Namespaces ----
    ns_nfs: "nfs-provisioner"
    ns_nvidia: "nvidia-device-plugin"
    ns_certmgr: "cert-manager"
    ns_slurm_operator: "slinky"
    ns_slurm: "slurm"

    # ---- NFS (server on machine0) ----
    nfs_server_setup: true
    nfs_server: ""                # leave empty => use machine0_ip
    nfs_export_path: "/srv/nfs/k8s"
    nfs_storageclass: "nfs-rwx"
    nfs_make_default_sc: false     # if you want nfs-rwx as default SC => true

    # ---- NVIDIA device plugin ----
    gpu_taint_key: "node-role.anxpert/gpu"
    gpu_taint_value: "true"
    slurm_gpu_count: 1

    # ---- Versions ----
    cert_manager_chart_version: "v1.19.2"
    slurm_operator_chart_ver: "1.0.1"
    slurm_chart_ver: "1.0.1"

  tasks:
    # -----------------------------------------------------------
    # 1) Discover machine IPs from Juju
    # -----------------------------------------------------------
    - name: 1.1 Get Juju model status (json)
      shell: "juju status --model {{ test_model }} --format json"
      register: juju_status_raw
      changed_when: false

    - name: 1.2 Parse machine0/machine1 IPv4
      set_fact:
        machine0_ip: "{{ (juju_status_raw.stdout | from_json).machines[machine0]['ip-addresses'] | select('match','^[0-9.]+$') | first }}"
        machine1_ip: "{{ (juju_status_raw.stdout | from_json).machines[machine1]['ip-addresses'] | select('match','^[0-9.]+$') | first }}"

    - name: 1.3 Show discovered IPs
      debug:
        msg:
          - "Machine0 (control-plane) IP: {{ machine0_ip }}"
          - "Machine1 (gpu worker)    IP: {{ machine1_ip }}"

    - name: 1.4 Decide NFS server IP
      set_fact:
        nfs_server_ip: "{{ (nfs_server | trim) if (nfs_server | trim != '') else machine0_ip }}"

    # -----------------------------------------------------------
    # 2) Ensure Helm exists on machine0
    # -----------------------------------------------------------
    - name: 2.1 Install Helm on machine0 if missing
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          if ! command -v helm >/dev/null 2>&1; then
            echo "[INFO] Installing Helm..."
            curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          else
            echo "[INFO] Helm already installed"
          fi
        '
      changed_when: false

    # -----------------------------------------------------------
    # 0) Reset (two-mode)
    # -----------------------------------------------------------
    - name: 0.1 Reset (helm uninstall releases)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set +e
          echo "[RESET] uninstall releases..."
          helm -n {{ ns_slurm }} uninstall slurm 2>/dev/null || true
          helm -n {{ ns_slurm_operator }} uninstall slurm-operator 2>/dev/null || true
          helm -n default uninstall slurm-operator-crds 2>/dev/null || true
          helm -n {{ ns_certmgr }} uninstall cert-manager 2>/dev/null || true
          helm -n {{ ns_nvidia }} uninstall nvidia-device-plugin 2>/dev/null || true
          helm -n {{ ns_nfs }} uninstall nfs-provisioner 2>/dev/null || true
        '
      when: reset_before_install | bool
      changed_when: true

    - name: 0.2 Reset (delete namespaces)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set +e
          echo "[RESET] delete namespaces..."
          kubectl delete ns {{ ns_slurm }} --ignore-not-found=true --wait=true || true
          kubectl delete ns {{ ns_slurm_operator }} --ignore-not-found=true --wait=true || true
          kubectl delete ns {{ ns_certmgr }} --ignore-not-found=true --wait=true || true
          kubectl delete ns {{ ns_nvidia }} --ignore-not-found=true --wait=true || true
          kubectl delete ns {{ ns_nfs }} --ignore-not-found=true --wait=true || true
        '
      when: reset_before_install | bool
      changed_when: true

    # -----------------------------------------------------------
    # 3.0 Install NFS client utils on ALL nodes (IMPORTANT)
    # - Fix: mount: bad option ... need /sbin/mount.nfs
    # -----------------------------------------------------------
    - name: 3.0 Install nfs-common on machine0+machine1
      shell: |
        for m in {{ machine0 }} {{ machine1 }}; do
          juju ssh --model {{ test_model }} $m -- bash -lc '
            set -e
            sudo apt-get update -y
            sudo apt-get install -y nfs-common
            command -v mount.nfs >/dev/null 2>&1 && echo "[OK] mount.nfs exists" || (echo "[FAIL] mount.nfs missing" && exit 1)
          '
        done
      changed_when: true
    
    - name: 3.1 Setup NFS server export on machine0 (optional)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          sudo apt-get update -y
          sudo apt-get install -y nfs-kernel-server
          sudo mkdir -p {{ nfs_export_path }}
          sudo chmod 777 {{ nfs_export_path }}

          # âœ… ensure exports.d exists
          sudo mkdir -p /etc/exports.d

          EXPORT_LINE="{{ nfs_export_path }} *(rw,sync,no_subtree_check,no_root_squash)"
          echo "$EXPORT_LINE" | sudo tee /etc/exports.d/k8s.exports >/dev/null

          sudo exportfs -ra
          sudo systemctl enable --now nfs-server

          echo "[INFO] exportfs:"
          sudo exportfs -v | sed -n "1,120p"
        '
      when: nfs_server_setup | bool
      changed_when: true

    - name: 3.9 Install nfs-common on k8s nodes (required for NFS mounts)
      shell: |
        juju ssh --model {{ test_model }} {{ item }} -- bash -lc '
          set -euo pipefail
          sudo -n true
          export DEBIAN_FRONTEND=noninteractive NEEDRESTART_MODE=a
          timeout 600 sudo apt-get -o Acquire::ForceIPv4=true update -y
          timeout 600 sudo apt-get -o Acquire::ForceIPv4=true install -y nfs-common
          command -v mount.nfs || ls -l /sbin/mount.nfs* || true
        '
      loop:
        - "{{ machine0 }}"
        - "{{ machine1 }}"
      changed_when: true

    # -----------------------------------------------------------
    # 4) Install NFS provisioner (no heredoc)
    # -----------------------------------------------------------
    - name: 4.1 Create namespace for NFS provisioner
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          kubectl create ns {{ ns_nfs }} --dry-run=client -o yaml | kubectl apply -f -
        '
      changed_when: false

    - name: 4.2 Install/upgrade nfs-subdir-external-provisioner via Helm (values file via printf)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -euo pipefail
          helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ >/dev/null 2>&1 || true
          helm repo update >/dev/null 2>&1

          f=/tmp/nfs-values.yaml
          : > $f
          printf "nfs:\n" >> $f
          printf "  server: %s\n" "{{ nfs_server_ip }}" >> $f
          printf "  path: %s\n" "{{ nfs_export_path }}" >> $f
          printf "storageClass:\n" >> $f
          printf "  name: %s\n" "{{ nfs_storageclass }}" >> $f
          printf "  defaultClass: %s\n" "{{ (nfs_make_default_sc | bool) | lower }}" >> $f
          printf "  reclaimPolicy: Delete\n" >> $f
          printf "  archiveOnDelete: true\n" >> $f

          helm upgrade --install nfs-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
            -n {{ ns_nfs }} --create-namespace \
            -f $f

          echo "[INFO] storageclass:"
          kubectl get sc | sed -n "1,120p"
        '
      changed_when: true

    # -----------------------------------------------------------
    # [æ–°å¢] 4.5 å®‰è£ NVIDIA Container Toolkit (Host Level)
    # é€™æ˜¯ç‚ºäº†è®“ Containerd çŸ¥é“å¦‚ä½•æŠŠ GPU æ›é€²å®¹å™¨
    # æ³¨æ„ï¼šç›®æ¨™æ˜¯ machine1 (GPU Node)ï¼Œä¸æ˜¯ machine0
    # -----------------------------------------------------------
    - name: 4.5 Install NVIDIA Container Toolkit on GPU Node (Machine 1)
      shell: |
        juju ssh --model {{ test_model }} {{ machine1 }} -- bash -lc '
          set -e
          
          # 1. æª¢æŸ¥æ˜¯å¦å·²å®‰è£ï¼Œé¿å…é‡è¤‡åŸ·è¡Œ (å†ªç­‰æ€§æª¢æŸ¥)
          if dpkg -l | grep -q nvidia-container-toolkit; then
            echo "[INFO] Toolkit already installed."
            # æª¢æŸ¥ config æ˜¯å¦å·²ç¶“ä¿®æ”¹é
            if grep -q "nvidia-container-runtime" /etc/containerd/config.toml; then
               echo "[INFO] Containerd already configured."
               exit 0
            fi
          fi

          echo "[INFO] Installing NVIDIA Container Toolkit..."
          curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
          
          curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
            sed "s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g" | \
            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

          sudo apt-get update
          sudo apt-get install -y nvidia-container-toolkit

          echo "[INFO] Configuring containerd to use NVIDIA runtime..."
          # é€™è¡ŒæŒ‡ä»¤æœƒè‡ªå‹•ä¿®æ”¹ /etc/containerd/config.toml
          sudo nvidia-ctk runtime configure --runtime=containerd
          
          echo "[INFO] Restarting containerd..."
          sudo systemctl restart containerd
          
          echo "[INFO] Done. GPU Node is ready for passthrough."
        '
      changed_when: true

    # -----------------------------------------------------------
    # 5) NVIDIA device plugin (Aæ–¹æ¡ˆï¼šæ¸…ç©º affinityï¼Œé¿å… NFD ä¾è³´)
    # -----------------------------------------------------------
    - name: 5.0 Precheck gpu node labels/taints
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          echo "== gpu node labels =="
          kubectl get node {{ gpu_node_hostname }} --show-labels | tr "," "\n" | egrep -n "kubernetes.io/hostname|hostname" || true
          echo
          echo "== gpu node taints =="
          kubectl describe node {{ gpu_node_hostname }} | egrep -n "Taints:" -A2 || true
        '
      changed_when: false

    - name: 5.1 Create namespace for NVIDIA device plugin
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          kubectl create ns {{ ns_nvidia }} --dry-run=client -o yaml | kubectl apply -f -
        '
      changed_when: false

    - name: "5.2 Install/upgrade NVIDIA device plugin (Fix: override NFD affinity + reset-values)"
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -euo pipefail
          helm repo add nvidia https://nvidia.github.io/k8s-device-plugin >/dev/null 2>&1 || true
          helm repo update >/dev/null 2>&1
    
          # values.yamlï¼šç”¨ hostname nodeAffinity æ˜ç¢ºè¦†è“‹é è¨­çš„ NFD affinity
          # ä¸¦ä¿ç•™ nodeSelector + toleration ç¶å®š GPU node
          cat <<EOF > /tmp/nvidia-values.yaml
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: kubernetes.io/hostname
                    operator: In
                    values:
                    - {{ gpu_node_hostname }}
          nodeSelector:
            kubernetes.io/hostname: {{ gpu_node_hostname }}
          tolerations:
          - key: {{ gpu_taint_key }}
            operator: Equal
            value: "{{ gpu_taint_value }}"
            effect: NoSchedule
          EOF
    
          echo "== Generated Values File =="
          cat /tmp/nvidia-values.yaml
    
          # é—œéµï¼š--reset-values æ¸…æ‰èˆŠ release æ®˜ç•™çš„ nodeAffinityï¼ˆfeature.node.kubernetes.io/*ï¼‰
          helm upgrade --install nvidia-device-plugin nvidia/nvidia-device-plugin \
            -n {{ ns_nvidia }} --create-namespace \
            --reset-values \
            -f /tmp/nvidia-values.yaml
    
          kubectl -n {{ ns_nvidia }} rollout status ds/nvidia-device-plugin --timeout=240s
    
          echo "== Device Plugin DS status =="
          kubectl -n {{ ns_nvidia }} get ds -o wide
    
          echo "== GPU capacity/allocatable on node =="
          kubectl describe node {{ gpu_node_hostname }} | egrep -n "Capacity:|Allocatable:|nvidia.com/gpu" -A5 || true
        '
      changed_when: true


    - name: 5.3 Verify nvidia.com/gpu on gpu node
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          kubectl describe node {{ gpu_node_hostname }} | egrep -n "Taints:|Capacity:|Allocatable:|nvidia.com/gpu" -n
        '
      changed_when: false

    # -----------------------------------------------------------
    # 6) cert-manager
    # -----------------------------------------------------------
    - name: 6.1 Create namespace cert-manager
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          kubectl create ns {{ ns_certmgr }} --dry-run=client -o yaml | kubectl apply -f -
        '
      changed_when: false

    - name: 6.2 Install/upgrade cert-manager via Helm
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -euo pipefail
          helm repo add jetstack https://charts.jetstack.io >/dev/null 2>&1 || true
          helm repo update >/dev/null 2>&1

          helm upgrade --install cert-manager jetstack/cert-manager \
            -n {{ ns_certmgr }} --create-namespace \
            --version {{ cert_manager_chart_version }} \
            --set crds.enabled=true

          kubectl -n {{ ns_certmgr }} rollout status deploy/cert-manager --timeout=240s
          kubectl -n {{ ns_certmgr }} rollout status deploy/cert-manager-webhook --timeout=240s
          kubectl -n {{ ns_certmgr }} rollout status deploy/cert-manager-cainjector --timeout=240s
        '
      changed_when: true

    # -----------------------------------------------------------
    # 7) slurm-operator-crds + slurm-operator (OCI charts)
    # -----------------------------------------------------------
    - name: 7.1 Create namespace for slurm operator
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          kubectl create ns {{ ns_slurm_operator }} --dry-run=client -o yaml | kubectl apply -f -
        '
      changed_when: false

    - name: 7.2 Install/upgrade slurm-operator-crds (OCI)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -euo pipefail
          helm upgrade --install slurm-operator-crds oci://ghcr.io/slinkyproject/charts/slurm-operator-crds \
            -n default --create-namespace \
            --version {{ slurm_operator_chart_ver }}
        '
      changed_when: true

    - name: 7.3 Install/upgrade slurm-operator (OCI)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -euo pipefail
          helm upgrade --install slurm-operator oci://ghcr.io/slinkyproject/charts/slurm-operator \
            -n {{ ns_slurm_operator }} --create-namespace \
            --version {{ slurm_operator_chart_ver }}

          kubectl -n {{ ns_slurm_operator }} rollout status deploy/slurm-operator --timeout=240s
        '
      changed_when: true

    # [é—œéµæ–°å¢] 7.4 ç­‰å¾… Operator çš„ Webhook æœå‹™çœŸæ­£å°±ç·’ (é¿å… Helm å®‰è£å¤±æ•—)
    - name: 7.4 Wait for Slurm Operator Webhook Service
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          echo "[INFO] Waiting for webhook service endpoints..."
          # è¿´åœˆæª¢æŸ¥ Service æ˜¯å¦æœ‰ IP (ä»£è¡¨ Pod Ready ä¸” Cert å®Œæˆ)
          for i in {1..30}; do
            EP=$(kubectl -n {{ ns_slurm_operator }} get ep slurm-operator-webhook -o jsonpath="{.subsets[*].addresses[*].ip}")
            if [ -n "$EP" ]; then
              echo "âœ… Webhook is ready with endpoints: $EP"
              # é¡å¤–å¤šç­‰ 5 ç§’è®“ Service ç©©å®š
              sleep 5
              exit 0
            fi
            echo "â³ Waiting for webhook endpoints... ($i/30)"
            sleep 5
          done
          echo "âŒ Webhook timed out!"
          exit 1
        '
      changed_when: false

    # -----------------------------------------------------------
    # 8) slurm chart (OCI)
    # -----------------------------------------------------------
    - name: 8.1 Create namespace slurm
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          kubectl create ns {{ ns_slurm }} --dry-run=client -o yaml | kubectl apply -f -
        '
      changed_when: false

    # -----------------------------------------------------------
    # 8.2 Install/upgrade slurm chart (OCI) - Pin Controller/Telemetry/Worker
    # -----------------------------------------------------------
    - name: 8.2 Install/upgrade slurm chart (OCI) - Pin Controller to ControlPlane
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -euo pipefail
    
          f="/tmp/slurm-values.yaml"
          : > "$f"
          w(){ printf -- "%s\n" "$1" >> "$f"; }
    
          echo "[INFO] Generating values.yaml..."
    
          # --- Controller (slurmctld) ---
          w "controller:"
          w "  nodeSelector:"
          w "    kubernetes.io/hostname: {{ controlplane_hostname }}"
          w "  tolerations:"
          w "  - key: node-role.kubernetes.io/control-plane"
          w "    operator: Exists"
          w "    effect: NoSchedule"
          w "  - key: node-role.kubernetes.io/master"
          w "    operator: Exists"
          w "    effect: NoSchedule"
    
          w "  persistence:"
          w "    enabled: true"
          w "    storageClassName: \"{{ nfs_storageclass }}\""
          w "    accessModes:"
          w "      - ReadWriteOnce"
          w "    resources:"
          w "      requests:"
          w "        storage: 10Gi"
    
          w "  extraConf: |"
          w "    GresTypes=gpu"
          w "  extraConfMap: {}"
    
          # --- Telemetry (REST API etc.) ---
          w "telemetry:"
          w "  enabled: true"
          w "  nodeSelector:"
          w "    kubernetes.io/hostname: {{ controlplane_hostname }}"
          w "  tolerations:"
          w "  - key: node-role.kubernetes.io/control-plane"
          w "    operator: Exists"
          w "    effect: NoSchedule"
    
          # --- NodeSets (Worker) ---
          w "nodesets:"
          w "  slinky:"
          w "    enabled: true"
          w "    partition:"
          w "      enabled: false"
          w "    podSpec:"
          w "      nodeSelector:"
          w "        kubernetes.io/hostname: {{ gpu_node_hostname }}"
          w "        kubernetes.io/os: linux"
          w "      tolerations:"
          w "      - effect: NoSchedule"
          w "        key: {{ gpu_taint_key }}"
          w "        operator: Equal"
          w "        value: \"{{ gpu_taint_value }}\""
          w "      useResourceLimits: true"
          w "      slurmd:"
          w "        resources:"
          w "          limits:"
          w "            nvidia.com/gpu: {{ slurm_gpu_count }}"
          w "        args:"
          w "        - --conf"
          w "        - Gres=gpu:{{ slurm_gpu_count }}"
    
          # --- Partitions ---
          w "partitions:"
          w "  all:"
          w "    enabled: true"
          w "    nodesets: [ALL]"
          w "    configMap:"
          w "      Default: \"YES\""
          w "      MaxTime: UNLIMITED"
          w "      State: UP"
          w "  slinky:"
          w "    enabled: true"
          w "    nodesets: [ALL]"
          w "    configMap:"
          w "      Default: \"NO\""
          w "      MaxTime: UNLIMITED"
          w "      State: UP"
    
          # --- ConfigFiles ---
          w "configFiles:"
          w "  gres.conf: |"
          w "    Name=gpu File=/dev/nvidia0"
    
          echo "[INFO] values head:"
          head -n 40 "$f"
    
          echo "[INFO] Helm upgrade..."
          helm upgrade --install slurm oci://ghcr.io/slinkyproject/charts/slurm \
            -n {{ ns_slurm }} --create-namespace \
            --version {{ slurm_chart_ver }} \
            -f "$f" \
            --wait --timeout=300s
        '
      changed_when: true    
    
    # -----------------------------------------------------------
    # 8.23 Patch Worker NodeSet slurmd probes -> tcpSocket:6818
    #   - NodeSet æ²’æœ‰ containers[] æ˜¯æ­£å¸¸çš„ï¼Œæ‰€ä»¥æ”¹æ‰¾ spec è£¡çš„ slurmd dict
    #   - patch NodeSet å¾Œåˆª worker pod è®“å®ƒé‡å»º
    # -----------------------------------------------------------
    - name: 8.23 Patch worker NodeSet (slurmd) probes to tcpSocket:6818 and restart pod
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
        set -euo pipefail
    
        NS="{{ ns_slurm | default('slurm') }}"
        WRK_POD="{{ slurm_worker_pod_name | default('slurm-worker-slinky-0') }}"
        PORT=6818
    
        echo "== [8.23] NS=$NS WRK_POD=$WRK_POD PORT=$PORT =="
    
        echo
        echo "== [A] detect NodeSet name from worker pod ownerReferences =="
        NODESET="$(kubectl -n "$NS" get pod "$WRK_POD" -o jsonpath='{.metadata.ownerReferences[0].name}')"
        KIND="$(kubectl -n "$NS" get pod "$WRK_POD" -o jsonpath='{.metadata.ownerReferences[0].kind}')"
        echo "owner kind=$KIND name=$NODESET"
        test "$KIND" = "NodeSet"
    
        echo
        echo "== [B] generate JSON patch for NodeSet: locate 'slurmd' dict under spec (schema-agnostic) =="
        python3 - <<'PY' > /tmp/nodeset_slurmd_probe_patch.json
        import json, subprocess, sys
    
        ns = "{{ ns_slurm | default('slurm') }}"
        nodeset = subprocess.check_output(["bash","-lc", "kubectl -n %s get pod %s -o jsonpath='{.metadata.ownerReferences[0].name}'" % (ns, "{{ slurm_worker_pod_name | default('slurm-worker-slinky-0') }}")]).decode().strip()
        port = 6818
    
        obj = json.loads(subprocess.check_output(["kubectl","-n",ns,"get","nodeset",nodeset,"-o","json"]))
    
        found_path = None
        found_obj = None
    
        def walk(x, path):
            nonlocal_found = False
            global found_path, found_obj
            if found_path is not None:
                return
            if isinstance(x, dict):
                # ç›´æ¥å‘½ä¸­ key == "slurmd"
                if "slurmd" in x and isinstance(x["slurmd"], dict):
                    found_path = path + ["slurmd"]
                    found_obj = x["slurmd"]
                    return
                for k, v in x.items():
                    walk(v, path + [k])
            elif isinstance(x, list):
                for i, v in enumerate(x):
                    walk(v, path + [i])
    
        # åªå¾ spec é–‹å§‹æ‰¾ï¼Œé¿å… metadata/status å¹²æ“¾
        walk(obj.get("spec", {}), ["spec"])
    
        if found_path is None:
            # æŠŠ spec keys å°å‡ºä¾†å¹« debug
            spec_keys = list(obj.get("spec", {}).keys())
            raise SystemExit(f"Cannot find a dict keyed 'slurmd' anywhere under NodeSet.spec. spec keys={spec_keys}")
    
        def esc(p):
            if isinstance(p, int):
                return f"/{p}"
            return "/" + p.replace("~","~0").replace("/","~1")
    
        base = "".join(esc(p) for p in found_path)
    
        # NodeSet è£¡çš„ slurmd dictï¼Œé€šå¸¸æ²’æœ‰ probeï¼ˆæˆ‘å€‘ç”¨ add/replace çš†å¯ï¼‰
        def op_for(field):
            return "replace" if (isinstance(found_obj, dict) and field in found_obj) else "add"
    
        probe_common = {
            "failureThreshold": 120,
            "periodSeconds": 5,
            "timeoutSeconds": 1
        }
    
        patch = [
          {"op": op_for("startupProbe"),  "path": base + "/startupProbe",  "value": {"tcpSocket":{"port":port}, **probe_common}},
          {"op": op_for("readinessProbe"),"path": base + "/readinessProbe","value": {"tcpSocket":{"port":port}, "failureThreshold":12, "periodSeconds":5, "timeoutSeconds":1}},
          {"op": op_for("livenessProbe"), "path": base + "/livenessProbe", "value": {"tcpSocket":{"port":port}, "failureThreshold":6, "periodSeconds":10, "timeoutSeconds":1}},
        ]
    
        print(json.dumps(patch))
        PY
    
        echo "[INFO] patch json:"
        cat /tmp/nodeset_slurmd_probe_patch.json; echo
    
        echo
        echo "== [C] apply patch to NodeSet/$NODESET =="
        kubectl -n "$NS" patch nodeset "$NODESET" --type=json -p "$(cat /tmp/nodeset_slurmd_probe_patch.json)"
    
        echo
        echo "== [D] restart worker pod to pick up new probes =="
        kubectl -n "$NS" delete pod "$WRK_POD" --ignore-not-found=true
    
        echo
        echo "== [E] wait worker Ready =="
        kubectl -n "$NS" wait --for=condition=Ready pod/"$WRK_POD" --timeout=300s
    
        echo
        echo "== [F] verify: worker pod probe (must NOT be httpGet) =="
        kubectl -n "$NS" get pod "$WRK_POD" -o jsonpath='{.spec.containers[?(@.name=="slurmd")].readinessProbe}{"\n"}'
        EOS
      args:
        executable: /bin/bash
      register: patch_nodeset_slurmd_probes
      changed_when: true
      failed_when: patch_nodeset_slurmd_probes.rc != 0

    
    # -----------------------------------------------------------
    # 8.25 Verify worker got correct gres.conf and GPU device + srun smoke test
    # -----------------------------------------------------------
    - name: 8.25 Verify worker gres.conf + /dev/nvidia0 + srun GPU smoke test
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
        set -euo pipefail
        NS="{{ ns_slurm }}"
        WRK="slurm-worker-slinky-0"
        CTL="slurm-controller-0"
    
        echo "[INFO] wait worker Ready..."
        kubectl -n "$NS" wait --for=condition=Ready pod/"$WRK" --timeout=300s
    
        echo "[CHECK] gres.conf:"
        kubectl -n "$NS" exec "$WRK" -c slurmd -- cat /run/slurm/conf/gres.conf
    
        echo "[CHECK] /dev/nvidia0:"
        kubectl -n "$NS" exec "$WRK" -c slurmd -- ls -l /dev/nvidia0
    
        echo
        echo "== sinfo =="
        kubectl -n "$NS" exec "$CTL" -c slurmctld -- sinfo -o "%P %a %D %t %N" || true
    
        echo
        echo "== scontrol show nodes (brief) =="
        kubectl -n "$NS" exec "$CTL" -c slurmctld -- sh -lc 'scontrol show nodes | egrep -i "NodeName=|State=|Gres=|CfgTRES=" | head -n 120' || true
    
        echo
        echo "[CHECK] srun GPU smoke test (authoritative):"
        # å¦‚æœä½ çš„ partition ä¸æ˜¯é è¨­ï¼Œè«‹æŠŠ -p slinky æˆ– -p all æ‰“é–‹å…¶ä¸­ä¸€å€‹
        kubectl -n "$NS" exec "$CTL" -c slurmctld -- sh -lc \
          'timeout 60s srun --immediate=10 -N1 -n1 --gres=gpu:1 bash -lc "hostname; ls -l /dev/nvidia0; echo GPU_OK"'
        EOS
      changed_when: false


    # -----------------------------------------------------------
    # 9) Restart controller + worker (Robust check)
    # -----------------------------------------------------------
    - name: 9.1 Restart slurm pods and Wait for Ready
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          echo "[INFO] 1. Deleting old pods..."
          kubectl -n {{ ns_slurm }} delete pod slurm-controller-0 --ignore-not-found=true --wait=true
          kubectl -n {{ ns_slurm }} delete pod slurm-worker-slinky-0 --ignore-not-found=true --wait=true

          echo "[INFO] 2. Waiting for StatefulSet to recreate pods..."
          sleep 10

          echo "[INFO] 3. Waiting for pods to become Ready..."
          # å¦‚æœé€™è£¡é‚„å¤±æ•—ï¼Œæœƒè‡ªå‹•å°å‡ºç‹€æ…‹
          if ! kubectl -n {{ ns_slurm }} wait --for=condition=Ready pod/slurm-controller-0 --timeout=300s; then
             echo "DEBUG: Controller Failed. PVC Status:"
             kubectl -n {{ ns_slurm }} get pvc
             exit 1
          fi

          if ! kubectl -n {{ ns_slurm }} wait --for=condition=Ready pod/slurm-worker-slinky-0 --timeout=300s; then
             echo "DEBUG: Worker Failed. Describe:"
             kubectl -n {{ ns_slurm }} describe pod slurm-worker-slinky-0
             exit 1
          fi

          echo "ğŸ‰ All Pods Ready!"
          kubectl -n {{ ns_slurm }} get pods -o wide
        '
      changed_when: true

    # -----------------------------------------------------------
    # 10) Verify (inside slurmctld pod)
    # -----------------------------------------------------------
    - name: 10.1 Verify partitions/nodes/GRES and run GPU job (inside slurmctld) - robust + no pod nvidia-smi dependency
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -euo pipefail
    
          NS="{{ ns_slurm }}"
          CTL="slurm-controller-0"
          WRK="slurm-worker-slinky-0"
    
          echo "== sinfo =="
          kubectl -n "$NS" exec "$CTL" -c slurmctld -- sinfo -o "%P %a %l %D %t %N"
    
          echo
          echo "== slurm.conf partitions =="
          kubectl -n "$NS" exec "$CTL" -c slurmctld -- sh -lc "grep -n \"^PartitionName=\" /etc/slurm/slurm.conf 2>/dev/null || true"
    
          echo
          echo "== worker /run/slurm/conf/gres.conf =="
          kubectl -n "$NS" exec "$WRK" -c slurmd -- cat /run/slurm/conf/gres.conf
    
          echo
          echo "== worker /dev/nvidia0 =="
          kubectl -n "$NS" exec "$WRK" -c slurmd -- ls -l /dev/nvidia0
    
          echo
          echo "== node GRES (from slurmctld) =="
          kubectl -n "$NS" exec "$CTL" -c slurmctld -- sh -lc \
            "scontrol show node slinky-0 | egrep -i \"NodeName=|State=|Reason=|Gres=|CfgTRES=\""
    
          echo
          echo "== srun GPU test (default partition=all) =="
          kubectl -n "$NS" exec "$CTL" -c slurmctld -- sh -lc \
            "srun -N1 -n1 --gres=gpu:{{ slurm_gpu_count }} bash -lc \"hostname; ls -l /dev/nvidia0; echo GPU_OK\""
    
          echo
          echo "== srun GPU test (partition=slinky) =="
          kubectl -n "$NS" exec "$CTL" -c slurmctld -- sh -lc \
            "srun -p slinky -N1 -n1 --gres=gpu:{{ slurm_gpu_count }} bash -lc \"hostname; ls -l /dev/nvidia0; echo GPU_OK\""
        '
      changed_when: false


    - name: Done
      debug:
        msg:
          - "ğŸ‰ Standardized Slinky Slurm on K8s deployed"
          - "Mode reset_before_install={{ reset_before_install }}"
          - "NFS SC={{ nfs_storageclass }} (default={{ nfs_make_default_sc }})"
          - "NVIDIA device-plugin ns={{ ns_nvidia }} (nodeSelector={{ gpu_node_hostname }}, toleration {{ gpu_taint_key }}={{ gpu_taint_value }}, affinity cleared)"
          - "Slurm ns={{ ns_slurm }} partitions=all(default)+slinky, GRES autodetect nvidia"
