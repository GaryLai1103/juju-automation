---
# ========================================================
# SKU: Slinky-SlurmOnK8s-Standardized
# Goal:
#   - Baseline (repeatable):
#     1) NFS Server (on machine0) + nfs-subdir-external-provisioner + StorageClass
#     2) NVIDIA k8s-device-plugin (Helm) with tolerations + nodeSelector
#     3) cert-manager + slurm-operator-crds + slurm-operator + slurm chart (Slinky)
#   - Avoid pitfalls:
#     - NEVER kubectl patch Controller extraConf (Helm conflict)
#     - Partitions do NOT depend on Feature
#     - Device-plugin MUST have toleration + nodeSelector (GPU node taint)
# ========================================================

- name: Application Layer - Deploy Slinky Slurm on K8s (Standardized)
  hosts: maasjuju
  gather_facts: yes

  vars:
    # ---- Juju / machines ----
    test_model: "slinky-cluster"
    machine0: "0"                 # control-plane node (slurmhn)
    machine1: "1"                 # gpu worker node (gpu-node01)

    # ---- K8s nodes ----
    controlplane_hostname: "slurmhn"
    gpu_node_hostname: "gpu-node01"

    # ---- Namespaces ----
    ns_certmgr: "cert-manager"
    ns_nvidia: "nvidia-device-plugin"
    ns_slurm_operator: "slinky"
    ns_slurm: "slurm"
    ns_nfs: "nfs-provisioner"

    # ---- NFS (server on machine0 by default) ----
    nfs_server_setup: true
    nfs_server: ""                # leave empty => use machine0_ip
    nfs_export_path: "/srv/nfs/k8s"
    nfs_storageclass: "nfs-rwx"
    nfs_make_default_sc: false     # å¦‚æžœä½ è¦æŠŠ nfs-rwx è¨­æˆ defaultï¼Œæ”¹ true

    # ---- NVIDIA device plugin ----
    gpu_taint_key: "node-role.anxpert/gpu"
    gpu_taint_value: "true"
    # ç”¨ hostname selector æœ€ç©©ï¼Œé¿å…è·‘åˆ°åˆ¥å°
    nvidia_node_selector:
      kubernetes.io/hostname: "gpu-node01"

    # ---- Versions ----
    cert_manager_chart_version: "v1.19.2"

    # Slinky charts (OCI)
    slurm_operator_chart_ver: "1.0.1"
    slurm_chart_ver: "1.0.1"

    # ---- Slurm nodeset resources ----
    slurm_gpu_count: 1

  tasks:
    # -----------------------------------------------------------
    # 1) Discover machine IPs from Juju
    # -----------------------------------------------------------
    - name: 1.1 Get Juju model status (json)
      shell: "juju status --model {{ test_model }} --format json"
      register: juju_status_raw
      changed_when: false

    - name: 1.2 Parse machine0/machine1 IPv4
      set_fact:
        machine0_ip: "{{ (juju_status_raw.stdout | from_json).machines[machine0]['ip-addresses'] | select('match','^[0-9.]+$') | first }}"
        machine1_ip: "{{ (juju_status_raw.stdout | from_json).machines[machine1]['ip-addresses'] | select('match','^[0-9.]+$') | first }}"

    - name: 1.3 Show discovered IPs
      debug:
        msg:
          - "Machine0 (control-plane) IP: {{ machine0_ip }}"
          - "Machine1 (gpu worker)   IP: {{ machine1_ip }}"

    - name: 1.4 Decide NFS server IP
      set_fact:
        nfs_server_ip: "{{ (nfs_server | trim) if (nfs_server | trim != '') else machine0_ip }}"

    # -----------------------------------------------------------
    # 2) Ensure Helm exists on machine0
    # -----------------------------------------------------------
    - name: 2.1 Install Helm on machine0 if missing
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          if ! command -v helm >/dev/null 2>&1; then
            echo "[INFO] Installing Helm..."
            curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          else
            echo "[INFO] Helm already installed"
          fi
        '
      changed_when: false

    # -----------------------------------------------------------
    # 3) (Optional) Setup NFS server on machine0
    # -----------------------------------------------------------
    - name: 3.1 Setup NFS server export on machine0 (optional)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          sudo apt-get update -y
          sudo apt-get install -y nfs-kernel-server
          sudo mkdir -p {{ nfs_export_path }}
          sudo chmod 777 {{ nfs_export_path }}
          # allow your cluster subnet(s); adjust if needed
          EXPORT_LINE="{{ nfs_export_path }} *(rw,sync,no_subtree_check,no_root_squash)"
          if ! sudo grep -qF "{{ nfs_export_path }}" /etc/exports; then
            echo "$EXPORT_LINE" | sudo tee -a /etc/exports
          fi
          sudo exportfs -ra
          sudo systemctl enable --now nfs-server
          echo "[INFO] NFS exports:"
          sudo exportfs -v | sed -n "1,120p"
        '
      when: nfs_server_setup | bool
      changed_when: true

    # -----------------------------------------------------------
    # 4) Install NFS provisioner (nfs-subdir-external-provisioner)
    # -----------------------------------------------------------
    - name: 4.1 Create namespace for NFS provisioner
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          kubectl create ns {{ ns_nfs }} --dry-run=client -o yaml | kubectl apply -f -
        '
      changed_when: false

    - name: 4.2 Install/upgrade nfs-subdir-external-provisioner via Helm
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ >/dev/null 2>&1 || true
          helm repo update >/dev/null 2>&1

          cat > /tmp/nfs-values.yaml <<EOF
nfs:
  server: "{{ nfs_server_ip }}"
  path: "{{ nfs_export_path }}"
storageClass:
  name: "{{ nfs_storageclass }}"
  defaultClass: {{ (nfs_make_default_sc | bool) | lower }}
  reclaimPolicy: Delete
  archiveOnDelete: true
EOF

          helm upgrade --install nfs-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
            -n {{ ns_nfs }} --create-namespace \
            -f /tmp/nfs-values.yaml
        '
      changed_when: true

    # -----------------------------------------------------------
    # 5) Install NVIDIA device plugin (Helm) with toleration + nodeSelector
    # -----------------------------------------------------------
    - name: 5.1 Create namespace for NVIDIA device plugin
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          kubectl create ns {{ ns_nvidia }} --dry-run=client -o yaml | kubectl apply -f -
        '
      changed_when: false

    - name: 5.2 Install/upgrade NVIDIA device plugin (daemonset) with tolerations + nodeSelector
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          helm repo add nvidia https://nvidia.github.io/k8s-device-plugin >/dev/null 2>&1 || true
          helm repo update >/dev/null 2>&1

          cat > /tmp/nvidia-device-plugin-values.yaml <<EOF
tolerations:
  - key: "{{ gpu_taint_key }}"
    operator: "Equal"
    value: "{{ gpu_taint_value }}"
    effect: "NoSchedule"
nodeSelector:
  kubernetes.io/hostname: "{{ gpu_node_hostname }}"
EOF

          helm upgrade --install nvidia-device-plugin nvidia/nvidia-device-plugin \
            -n {{ ns_nvidia }} --create-namespace \
            -f /tmp/nvidia-device-plugin-values.yaml

          echo "[INFO] wait device-plugin ready..."
          kubectl -n {{ ns_nvidia }} rollout status ds/nvidia-device-plugin --timeout=180s
        '
      changed_when: true

    - name: 5.3 Verify GPU allocatable on gpu node
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          kubectl describe node {{ gpu_node_hostname }} | egrep -i "Taints|nvidia.com/gpu|Allocatable|Capacity" -n || true
        '
      changed_when: false

    # (Optional) quick smoke test pod (idempotent: delete then apply)
    - name: 5.4 GPU smoke test pod (delete -> run -> logs -> delete)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          kubectl delete pod gpu-smoke --ignore-not-found=true >/dev/null 2>&1 || true

          cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: gpu-smoke
spec:
  restartPolicy: Never
  nodeSelector:
    kubernetes.io/hostname: "{{ gpu_node_hostname }}"
  tolerations:
  - key: "{{ gpu_taint_key }}"
    operator: "Equal"
    value: "{{ gpu_taint_value }}"
    effect: "NoSchedule"
  containers:
  - name: c
    image: nvidia/cuda:12.4.1-base-ubuntu22.04
    command: ["bash","-lc","nvidia-smi -L || true; ls -l /dev/nvidia0 || true; sleep 2"]
    resources:
      limits:
        nvidia.com/gpu: 1
EOF

          kubectl wait --for=condition=Ready pod/gpu-smoke --timeout=120s || true
          kubectl logs gpu-smoke || true
          kubectl delete pod gpu-smoke --ignore-not-found=true >/dev/null 2>&1 || true
        '
      changed_when: true

    # -----------------------------------------------------------
    # 6) Install cert-manager (required by operator webhook)
    # -----------------------------------------------------------
    - name: 6.1 Create namespace cert-manager
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          kubectl create ns {{ ns_certmgr }} --dry-run=client -o yaml | kubectl apply -f -
        '
      changed_when: false

    - name: 6.2 Install/upgrade cert-manager via Helm
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          helm repo add jetstack https://charts.jetstack.io >/dev/null 2>&1 || true
          helm repo update >/dev/null 2>&1

          helm upgrade --install cert-manager jetstack/cert-manager \
            -n {{ ns_certmgr }} --create-namespace \
            --version {{ cert_manager_chart_version }} \
            --set crds.enabled=true

          kubectl -n {{ ns_certmgr }} rollout status deploy/cert-manager --timeout=180s
          kubectl -n {{ ns_certmgr }} rollout status deploy/cert-manager-webhook --timeout=180s
          kubectl -n {{ ns_certmgr }} rollout status deploy/cert-manager-cainjector --timeout=180s
        '
      changed_when: true

    # -----------------------------------------------------------
    # 7) Install slurm-operator-crds + slurm-operator (OCI charts)
    # -----------------------------------------------------------
    - name: 7.1 Create namespace for slurm operator
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          kubectl create ns {{ ns_slurm_operator }} --dry-run=client -o yaml | kubectl apply -f -
        '
      changed_when: false

    - name: 7.2 Install/upgrade slurm-operator-crds (OCI)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          helm upgrade --install slurm-operator-crds oci://ghcr.io/slinkyproject/charts/slurm-operator-crds \
            -n default --create-namespace \
            --version {{ slurm_operator_chart_ver }}
        '
      changed_when: true

    - name: 7.3 Install/upgrade slurm-operator (OCI)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          helm upgrade --install slurm-operator oci://ghcr.io/slinkyproject/charts/slurm-operator \
            -n {{ ns_slurm_operator }} --create-namespace \
            --version {{ slurm_operator_chart_ver }}

          kubectl -n {{ ns_slurm_operator }} rollout status deploy/slurm-operator --timeout=180s
        '
      changed_when: true

    # -----------------------------------------------------------
    # 8) Install slurm chart (OCI) with stable values (no Feature dependency)
    # -----------------------------------------------------------
    - name: 8.1 Create namespace slurm
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          kubectl create ns {{ ns_slurm }} --dry-run=client -o yaml | kubectl apply -f -
        '
      changed_when: false

    - name: 8.2 Install/upgrade slurm chart with stable partitions + GRES autodetect
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e

          cat > /tmp/slurm-values.yaml <<EOF
controller:
  # é¿å… controller åˆå¡ž extraConf é€ æˆé‡è¤‡/è¡çª
  extraConf: ""
  extraConfMap: {}

nodesets:
  slinky:
    enabled: true

    # nodeset ä¸è¦è‡ªå‹•ç”Ÿæˆ partitionï¼ˆé¿å…ä¾è³´ Featureï¼‰
    partition:
      enabled: false

    # å›ºå®šæŽ’åˆ° GPU nodeï¼ˆæœ€ç©©ï¼‰
    podSpec:
      nodeSelector:
        kubernetes.io/hostname: "{{ gpu_node_hostname }}"
        kubernetes.io/os: linux
      tolerations:
      - effect: NoSchedule
        key: "{{ gpu_taint_key }}"
        operator: Equal
        value: "{{ gpu_taint_value }}"

    useResourceLimits: true

    slurmd:
      resources:
        limits:
          nvidia.com/gpu: {{ slurm_gpu_count }}
      # ä¸é  Featureï¼Œåƒ…å®£å‘Š Gresï¼ˆslurmd æœƒç”¨ configless å›žå ±ï¼‰
      args:
      - --conf
      - Gres=gpu:{{ slurm_gpu_count }}

partitions:
  # default partition ä¸€å®šè¦å­˜åœ¨ï¼ˆé¿å… "No partition specified"ï¼‰
  all:
    enabled: true
    nodesets: [ALL]
    configMap:
      Default: "YES"
      MaxTime: UNLIMITED
      State: UP

  # slinky partitionï¼šä¸é  featureï¼Œç›´æŽ¥åƒ ALLï¼ˆä½ å¯¦æ¸¬æœ€ç©©ï¼‰
  slinky:
    enabled: true
    nodesets: [ALL]
    configMap:
      Default: "NO"
      MaxTime: UNLIMITED
      State: UP

configFiles:
  gres.conf: |
    AutoDetect=nvidia
EOF

          helm upgrade --install slurm oci://ghcr.io/slinkyproject/charts/slurm \
            -n {{ ns_slurm }} --create-namespace \
            --version {{ slurm_chart_ver }} \
            -f /tmp/slurm-values.yaml
        '
      changed_when: true

    # -----------------------------------------------------------
    # 9) Restart controller + worker to ensure clean re-read
    # -----------------------------------------------------------
    - name: 9.1 Restart slurm controller + worker pods
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          kubectl -n {{ ns_slurm }} delete pod slurm-controller-0 --ignore-not-found=true || true
          kubectl -n {{ ns_slurm }} delete pod slurm-worker-slinky-0 --ignore-not-found=true || true
          kubectl -n {{ ns_slurm }} wait --for=condition=Ready pod -l app.kubernetes.io/name=slurmctld --timeout=180s
          kubectl -n {{ ns_slurm }} wait --for=condition=Ready pod -l app.kubernetes.io/name=slurmd --timeout=180s
        '
      changed_when: true

    # -----------------------------------------------------------
    # 10) Verify (sinfo + srun inside slurmctld pod)
    # -----------------------------------------------------------
    - name: 10.1 Verify partitions/nodes/GRES and run GPU job (inside slurmctld pod)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -e
          echo "== pods =="
          kubectl -n {{ ns_slurm }} get pods -o wide

          echo
          echo "== sinfo =="
          kubectl -n {{ ns_slurm }} exec -it slurm-controller-0 -c slurmctld -- bash -lc \
            '"'"'sinfo -o "%P %a %l %D %t %N"'"'"'

          echo
          echo "== node GRES =="
          kubectl -n {{ ns_slurm }} exec -it slurm-controller-0 -c slurmctld -- bash -lc \
            '"'"'scontrol show node slinky-0 | egrep -i "NodeName=|State=|Reason=|Gres=|CfgTRES="'"'"'

          echo
          echo "== srun GPU test (partition=all) =="
          kubectl -n {{ ns_slurm }} exec -it slurm-controller-0 -c slurmctld -- bash -lc \
            '"'"'srun -p all -N1 -n1 --gres=gpu:{{ slurm_gpu_count }} bash -lc "ls -l /dev/nvidia0 && echo GPU_OK"'"'"'

          echo
          echo "== srun GPU test (partition=slinky) =="
          kubectl -n {{ ns_slurm }} exec -it slurm-controller-0 -c slurmctld -- bash -lc \
            '"'"'srun -p slinky -N1 -n1 --gres=gpu:{{ slurm_gpu_count }} bash -lc "ls -l /dev/nvidia0 && echo GPU_OK"'"'"'
        '
      changed_when: false

    - name: Done
      debug:
        msg:
          - "ðŸŽ‰ Standardized Slinky Slurm on K8s deployed"
          - "NFS SC: {{ nfs_storageclass }} (default={{ nfs_make_default_sc }})"
          - "NVIDIA device-plugin: installed in ns={{ ns_nvidia }} with nodeSelector={{ gpu_node_hostname }} + toleration {{ gpu_taint_key }}={{ gpu_taint_value }}"
          - "Slurm: ns={{ ns_slurm }}, partitions: all(default) + slinky, GRES autodetect nvidia"
