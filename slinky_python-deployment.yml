---
# ========================================================
# Scenario: Slinky Offline AI Environment - Deployment Only
# Goal: Upload files & Prep Env, BUT DO NOT RUN JOBS.
# ========================================================
- name: ä½ˆç½² Slinky PyTorch æ¸¬è©¦æª”æ¡ˆ (ä¸è‡ªå‹•åŸ·è¡Œ)
  hosts: maasjuju
  gather_facts: no
  vars:
    # ---- æœ¬åœ°æª”æ¡ˆä½ç½® (Jumpbox) ----
    local_wheel_path: "/home/gary/torch.whl"
    
    # ---- Slinky è¨­å®š ----
    test_model: "slinky-cluster"
    k8s_machine: "0"
    slurm_ns: "slurm"
    controller_pod: "slurm-controller-0"
    shared_dir: "/home/slurm"

  tasks:
    # ----------------------------------------------------
    # 1. Pre-check Files
    # ----------------------------------------------------
    - name: 1. Check if offline files exist
      stat:
        path: "{{ item }}"
      loop:
        - "/home/gary/pip-25.3-py3-none-any.whl"                # [å·²ä¿®æ­£] æ¨™æº– Pip æª”å
        - "/home/gary/torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl" # [å·²ä¿®æ­£] æ¨™æº– Torch æª”å
      register: file_check
      failed_when: not item.stat.exists

    - name: 1.2 ç¢ºèª Slurm Controller æ´»è‘—
      shell: |
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- \
          "kubectl get pod -n {{ slurm_ns }} {{ controller_pod }} -o jsonpath='{.status.phase}'"
      register: pod_status
      failed_when: "'Running' not in pod_status.stdout"

    # ----------------------------------------------------
    # 2. æ¬é‹å¤§æª”æ¡ˆ (Wheel)
    # ----------------------------------------------------
    - name: 2.1 ä¸Šå‚³ torch.whl åˆ° Controller å…±äº«ç›®éŒ„
      shell: |
        # 1. Jumpbox -> Machine 0
        juju scp {{ local_wheel_path }} {{ test_model }}/{{ k8s_machine }}:/tmp/torch.whl
        
        # 2. Machine 0 -> Pod (NFS Shared Dir)
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- \
          "kubectl cp /tmp/torch.whl {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/torch.whl"
        
        # 3. æ¸…é™¤æš«å­˜
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- "rm -f /tmp/torch.whl"
      async: 1200
      poll: 10

    # ----------------------------------------------------
    # 3. é›¢ç·šå®‰è£ Pip èˆ‡ PyTorch (æœ€çµ‚ä¿®æ­£ç‰ˆ)
    # ----------------------------------------------------
    - name: 3.1 é›¢ç·šå®‰è£ Pip èˆ‡ PyTorch (ä½¿ç”¨æ¨™æº–æª”å)
      shell: |
        # å®šç¾©æ¨™æº–æª”å (é€™å…©å€‹æª”æ¡ˆéƒ½å·²ç¶“åœ¨ Jumpbox æ”¹å¥½åäº†)
        PIP_WHEEL="pip-25.3-py3-none-any.whl"
        TORCH_WHEEL="torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl"

        # 1. å‚³é€ PIP
        cat /home/gary/$PIP_WHEEL | juju ssh -m {{ test_model }} {{ k8s_machine }} "cat > /tmp/$PIP_WHEEL"
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- "kubectl cp /tmp/$PIP_WHEEL {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/$PIP_WHEEL"
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- "rm -f /tmp/$PIP_WHEEL"

        # 2. å‚³é€ TORCH
        cat /home/gary/$TORCH_WHEEL | juju ssh -m {{ test_model }} {{ k8s_machine }} "cat > /tmp/$TORCH_WHEEL"
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- "kubectl cp /tmp/$TORCH_WHEEL {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/$TORCH_WHEEL"
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- "rm -f /tmp/$TORCH_WHEEL"

        # 3. é€²å…¥ Pod åŸ·è¡Œå®‰è£
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- bash -s <<'EOS'
          kubectl exec -n {{ slurm_ns }} {{ controller_pod }} -- bash -c '
            set -e
            cd {{ shared_dir }}
            export PATH=$HOME/.local/bin:$PATH
            
            PIP_FILE="pip-25.3-py3-none-any.whl"
            TORCH_FILE="torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl"
            
            # --- å®‰è£ PIP (å¦‚æœå·²å®‰è£æœƒè‡ªå‹•è·³é) ---
            if ! command -v pip3 >/dev/null 2>&1; then
               echo "ğŸ” Bootstrapping pip..."
               mkdir -p pip_tmp
               python3 -m zipfile -e "$PIP_FILE" pip_tmp
               export PYTHONPATH=$(pwd)/pip_tmp
               python3 -m pip install "$PIP_FILE" --no-index --user --break-system-packages
               unset PYTHONPATH
               rm -rf pip_tmp
            else
               echo "âœ… pip3 is already installed."
            fi

            # --- å®‰è£ PyTorch ---
            echo "ğŸ“¦ Installing PyTorch from $TORCH_FILE..."
            python3 -m pip install "$TORCH_FILE" \
                --no-index \
                --find-links {{ shared_dir }} \
                --user \
                --break-system-packages
            
            echo "ğŸ” Verifying installation..."
            python3 -c "import torch; print(f\"ğŸ‰ Success! PyTorch {torch.__version__} is ready.\")"
          '
        EOS
      async: 900
      poll: 10
      register: install_res

    # ----------------------------------------------------
    # 4. ä½ˆç½²æ¸¬è©¦è…³æœ¬ (30ç§’å€’æ•¸ + å¯ä¸­æ–·)
    # ----------------------------------------------------
    - name: 4.1 æ³¨å…¥ train.py (30ç§’è‡ªå‹•å€’æ•¸ï¼Œå¯ Ctrl+C)
      shell: |
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- bash -s <<'EOS'
          cat << 'PY' > /tmp/train.py
        import torch
        import time
        import os
        import socket
        import sys

        # 1. è¨­å®š
        TARGET_DURATION = 30  # è¨­å®šè·‘ 30 ç§’
        node_name = socket.gethostname()
        job_id = os.environ.get('SLURM_JOB_ID', 'Interactive')

        print(f"\nğŸš€ [Job {job_id}] GPU Stress Test (Duration: {TARGET_DURATION}s)")
        print(f"ğŸ“ Node: {node_name}")
        print("-" * 65)

        # 2. æª¢æŸ¥ GPU
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            print(f"âœ… GPU Detected: {gpu_name}")
            dev = torch.device("cuda")
        else:
            print("âŒ No GPU found! Running on CPU.")
            dev = torch.device("cpu")

        # 3. æº–å‚™çŸ©é™£
        N = 10000
        print(f"ğŸ”¥ Allocating Tensors ({N}x{N})...")
        a = torch.randn(N, N, device=dev)
        b = torch.randn(N, N, device=dev)

        print(f"â³ Starting Countdown... Press [Ctrl+C] to stop early.")
        print("-" * 65)

        # 4. åŸ·è¡Œè¿´åœˆ
        start_time = time.time()
        count = 0

        try:
            while True:
                # è¨ˆç®—ç¶“éæ™‚é–“
                current_time = time.time()
                elapsed = current_time - start_time
                remaining = TARGET_DURATION - elapsed

                # æ™‚é–“åˆ°å°±è‡ªå‹•è·³å‡º
                if remaining <= 0:
                    break

                # é‹ç®—è² è¼‰
                c = torch.mm(a, b)
                torch.cuda.synchronize() # ç­‰å¾… GPU ç®—å®Œ
                
                count += 1
                
                # å‹•æ…‹é¡¯ç¤ºé€²åº¦æ¢
                # \r å›åˆ°è¡Œé¦–ï¼Œè¦†è“‹èˆŠæ–‡å­—
                sys.stdout.write(f"\râš¡ Batch: {count} | â±ï¸ Used: {elapsed:4.1f}s | â³ Left: {remaining:4.1f}s ")
                sys.stdout.flush()

        except KeyboardInterrupt:
            print("\n\nğŸ›‘ Test Stopped by User (Ctrl+C).")
        
        # 5. çµç®—
        final_time = time.time() - start_time
        print(f"\n\n" + "="* 65)
        print(f"âœ… Test Finished in {final_time:.2f} seconds.")
        print(f"ğŸ“Š Total Batches Processed: {count}")
        print("="* 65 + "\n")
        PY
          
          kubectl cp /tmp/train.py {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/train.py
        EOS

    - name: 4.2 æ³¨å…¥ run_job.sh (Slurm æ’ç¨‹è…³æœ¬)
      shell: |
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- bash -s <<'EOS'
          cat << 'SBATCH' > /tmp/run_job.sh
        #!/bin/bash
        #SBATCH --job-name=gpu-test
        #SBATCH --output=gpu-%j.out
        #SBATCH --ntasks=1
        #SBATCH --gres=gpu:1
        #SBATCH --time=00:05:00
        #SBATCH --partition=slinky

        echo "â¡ï¸ Loading AI Environment..."
        source {{ shared_dir }}/ai-env/bin/activate

        echo "â¡ï¸ Starting Training..."
        python {{ shared_dir }}/train.py
        SBATCH
        
          kubectl cp /tmp/run_job.sh {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/run_job.sh
        EOS

    # ----------------------------------------------------
    # 5. æ‰‹å‹•æ“ä½œæŒ‡å¼•
    # ----------------------------------------------------
    - name: ğŸš€ ä½ˆç½²å®Œæˆï¼è«‹æ‰‹å‹•é–‹å§‹æ¸¬è©¦
      debug:
        msg: 
          - "========================================================"
          - "âœ… æª”æ¡ˆèˆ‡ç’°å¢ƒå·²å°±ç·’ï¼Œè«‹ä¾ç…§ä¸‹åˆ—æ­¥é©Ÿæ‰‹å‹•æ¸¬è©¦ï¼š"
          - "========================================================"
          - "1. [ç™»å…¥ Controller Pod]"
          - "   juju ssh -m {{ test_model }} 0 -- kubectl exec -it -n slurm slurm-controller-0 -- bash"
          - ""
          - "2. [é€²å…¥å…±äº«ç›®éŒ„]"
          - "   cd /home/slurm"
          - ""
          - "3. [æ–¹å¼ A: äº’å‹•å¼æ¸¬è©¦ (æ¨è–¦! å¯ä»¥ç›´æ¥çœ‹è¢å¹•è¼¸å‡º)]"
          - "   source ai-env/bin/activate"
          - "   srun --gres=gpu:1 --pty python train.py"
          - ""
          - "4. [æ–¹å¼ B: æ’ç¨‹æ¸¬è©¦ (æ¨™æº–åšæ³•)]"
          - "   sbatch run_job.sh"
          - "   squeue (æŸ¥çœ‹ç‹€æ…‹)"
          - "   cat gpu-*.out (æŸ¥çœ‹çµæœ)"
          - "========================================================"
