---
# ========================================================
# Scenario: Slinky Offline AI Environment - Install & Burn-in
# Goal: Install PyTorch (WHL) & Deploy Temp Monitor Burn-in
# ========================================================
- name: ä½ˆç½² Slinky PyTorch èˆ‡ GPU æº«åº¦ç‡’æ©Ÿæ¸¬è©¦
  hosts: maasjuju
  gather_facts: no
  vars:
    # ---- Slinky è¨­å®š ----
    test_model: "slinky-cluster"
    k8s_machine: "0"
    slurm_ns: "slurm"
    controller_pod: "slurm-controller-0"
    shared_dir: "/home/slurm"

  tasks:
    # ----------------------------------------------------
    # 1. é›¢ç·šå®‰è£ Pip èˆ‡ Dependencies (å¼·åŠ›æ¸…é™¤ V3 ç‰ˆ)
    # ----------------------------------------------------
    - name: 1.1 é›¢ç·šå®‰è£æ‰€æœ‰ Wheel å¥—ä»¶ (Bulk Install from WHL)
      shell: |
        # =======================================================
        # [æ­¥é©Ÿ 0] ä½¿ç”¨ sh -c å¼·åˆ¶åœ¨ Pod å…§åŸ·è¡Œåˆªé™¤
        # =======================================================
        echo "ğŸ§¹ Nuke ALL old wheels in Pod (Force)..."
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- \
           "kubectl exec -n {{ slurm_ns }} {{ controller_pod }} -- sh -c 'rm -f {{ shared_dir }}/*.whl'"

        # -------------------------------------------------------
        # [æ­¥é©Ÿ 1] å–å¾— Jumpbox ä¸Š WHL è³‡æ–™å¤¾çš„æª”æ¡ˆ
        # -------------------------------------------------------
        WHEEL_FILES=$(ls /home/gary/WHL/*.whl)
        
        if [ -z "$WHEEL_FILES" ]; then
            echo "âŒ Error: No .whl files found in /home/gary/WHL/"
            exit 1
        fi
        
        # -------------------------------------------------------
        # [æ­¥é©Ÿ 2] è¿´åœˆï¼šå°‡æ¯å€‹æª”æ¡ˆå‚³é€åˆ° Pod
        # -------------------------------------------------------
        for FILE in $WHEEL_FILES; do
            FILENAME=$(basename "$FILE")
            echo "ğŸš€ Transferring $FILENAME..."
            cat "$FILE" | juju ssh -m {{ test_model }} {{ k8s_machine }} "cat > /tmp/$FILENAME"
            juju ssh -m {{ test_model }} {{ k8s_machine }} -- \
               "kubectl cp /tmp/$FILENAME {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/$FILENAME"
            juju ssh -m {{ test_model }} {{ k8s_machine }} -- "rm -f /tmp/$FILENAME"
        done

        # -------------------------------------------------------
        # [æ­¥é©Ÿ 3] é€²å…¥ Pod åŸ·è¡Œå®‰è£
        # -------------------------------------------------------
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- bash -s <<'EOS'
          kubectl exec -n {{ slurm_ns }} {{ controller_pod }} -- bash -c '
            set -e
            cd {{ shared_dir }}
            export PATH=$HOME/.local/bin:$PATH
            
            echo "ğŸ“‚ Checking files..."
            ls -1 *.whl | wc -l
            
            echo "ğŸ“¦ Installing ALL wheels..."
            python3 -m pip install *.whl \
                --no-index \
                --find-links {{ shared_dir }} \
                --user \
                --break-system-packages
            
            echo "ğŸ” Verifying PyTorch on Controller..."
            python3 -c "import torch; print(f\"ğŸ‰ Success! PyTorch {torch.__version__} is ready.\")"
          '
        EOS
      async: 1800
      poll: 10
      register: install_res

    # ----------------------------------------------------
    # 2. ä½ˆç½² GPU ç‡’æ©Ÿèˆ‡æº«åº¦ç›£æ§è…³æœ¬
    # ----------------------------------------------------
    - name: 2.1 æ³¨å…¥ç‡’æ©Ÿç¨‹å¼ (burn_test.py - å«æº«åº¦é¡¯ç¤º)
      shell: |
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- bash -s <<'EOS'
          # 1. å»ºç«‹ Python æ¸¬è©¦æª” (å« nvidia-smi å‘¼å«)
          cat << 'PY' > /tmp/burn_test.py
        import torch
        import sys
        import time
        import os

        # --- è¨­å®š ---
        DURATION = 300       # è·‘ 5 åˆ†é˜
        MATRIX_SIZE = 8192   # çŸ©é™£å¤§å°

        print(f"ğŸ”¥ Starting GPU Burn-in Test (With Temp Monitor)")
        print(f"â±ï¸  Duration: {DURATION}s")

        if not torch.cuda.is_available():
            print("âŒ Error: No GPU found!")
            sys.exit(1)

        device = torch.device("cuda")
        gpu_name = torch.cuda.get_device_name(0)
        print(f"âœ… Using GPU: {gpu_name}")

        # æº–å‚™è³‡æ–™
        x = torch.randn(MATRIX_SIZE, MATRIX_SIZE, device=device)
        y = torch.randn(MATRIX_SIZE, MATRIX_SIZE, device=device)

        start_time = time.time()
        end_time = start_time + DURATION
        batch = 0

        print("ğŸš€ Burn-in started!")
        print("-" * 65)
        print(f"{'Batch':<8} | {'Time':<10} | {'Temp':<8} | {'Status'}")
        print("-" * 65)

        try:
            while time.time() < end_time:
                # 1. é‹ç®—
                z = torch.mm(x, y)
                torch.cuda.synchronize()
                
                batch += 1
                elapsed = time.time() - start_time
                remaining = DURATION - elapsed
                
                # 2. æ¯ 10 å€‹ Batch æŠ“ä¸€æ¬¡æº«åº¦
                if batch % 10 == 0:
                    try:
                        # å‘¼å«ç³»çµ±æŒ‡ä»¤æŠ“æº«åº¦
                        temp_str = os.popen("nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader").read().strip()
                        temp = f"{temp_str}Â°C"
                    except:
                        temp = "N/A"

                    # 3. å°å‡º Log (\r è®“å®ƒå³æ™‚æ›´æ–°)
                    sys.stdout.write(f"\râš¡ {batch:<6} | {elapsed:>5.1f}s used | ğŸ”¥ {temp:<6} | â³ Left: {remaining:>5.1f}s  ")
                    sys.stdout.flush()

        except KeyboardInterrupt:
            print("\nğŸ›‘ Stopped by user.")

        print(f"\n\nâœ… Finished! Total Batches: {batch}")
        PY
          
          # 2. å‚³å…¥ Pod
          kubectl cp /tmp/burn_test.py {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/burn_test.py
        EOS

    - name: 2.2 æ³¨å…¥ Slurm æ’ç¨‹è…³æœ¬ (run_burn.sh)
      shell: |
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- bash -s <<'EOS'
          # 1. å»ºç«‹ SBATCH è…³æœ¬
          cat << 'SBATCH' > /tmp/run_burn.sh
        #!/bin/bash
        #SBATCH --job-name=gpu-burn
        #SBATCH --output=burn-%j.out
        #SBATCH --partition=slinky
        #SBATCH --gres=gpu:1
        #SBATCH --ntasks=1
        #SBATCH --time=00:10:00   # è¨­å®š 10 åˆ†é˜ï¼Œç¢ºä¿å¤ è·‘ Python çš„ 5 åˆ†é˜

        export PATH=$HOME/.local/bin:$PATH

        echo "ğŸš€ Starting Burn-in on Worker Node..."
        python3 {{ shared_dir }}/burn_test.py
        SBATCH
        
          # 2. å‚³å…¥ Pod
          kubectl cp /tmp/run_burn.sh {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/run_burn.sh
          
          # 3. è‡ªå‹•é€å‡ºå·¥ä½œ (å…ˆæ¸…ç©ºèˆŠçš„ï¼Œå†é€æ–°çš„)
          echo "ğŸ“¤ Submitting Slurm Job..."
          kubectl exec -n {{ slurm_ns }} {{ controller_pod }} -- bash -c "cd {{ shared_dir }} && scancel -u slurm && sbatch run_burn.sh"
        EOS

    # ----------------------------------------------------
    # 3. æç¤ºè¨Šæ¯
    # ----------------------------------------------------
    - name: ğŸš€ ä½ˆç½²èˆ‡æ¸¬è©¦å•Ÿå‹•å®Œæˆ
      debug:
        msg: 
          - "========================================================"
          - "âœ… ç‡’æ©Ÿç¨‹å¼å·²å•Ÿå‹•ï¼(å°‡é‹è¡Œ 5 åˆ†é˜)"
          - "========================================================"
          - "è«‹ä¾ç…§ä¸‹åˆ—æŒ‡ä»¤æŸ¥çœ‹å³æ™‚æº«åº¦ï¼š"
          - "1. ç™»å…¥: juju ssh -m {{ test_model }} 0 -- kubectl exec -it -n slurm slurm-controller-0 -- bash"
          - "2. é€²å…¥ç›®éŒ„: cd /home/slurm"
          - "3. ç›£çœ‹ Log: tail -f burn-*.out"
          - "========================================================"
