---
# ========================================================
# Scenario: Slinky Offline AI Environment - Deployment Only
# Goal: Upload files & Prep Env (WHL Folder Version)
# ========================================================
- name: ä½ˆç½² Slinky PyTorch æ¸¬è©¦æª”æ¡ˆ (WHL è³‡æ–™å¤¾ç‰ˆ)
  hosts: maasjuju
  gather_facts: no
  vars:
    # ---- Slinky è¨­å®š ----
    test_model: "slinky-cluster"
    k8s_machine: "0"
    slurm_ns: "slurm"
    controller_pod: "slurm-controller-0"
    shared_dir: "/home/slurm"

  tasks:
    # ----------------------------------------------------
    # 1. é›¢ç·šå®‰è£ Pip èˆ‡ Dependencies (å¼·åŠ›æ¸…é™¤ V3 ç‰ˆ)
    # ----------------------------------------------------
    - name: 1.1 é›¢ç·šå®‰è£æ‰€æœ‰ Wheel å¥—ä»¶ (Bulk Install from WHL)
      shell: |
        # =======================================================
        # [ä¿®æ­£æ­¥é©Ÿ] 0. ä½¿ç”¨ sh -c å¼·åˆ¶åœ¨ Pod å…§åŸ·è¡Œåˆªé™¤
        # é€™æ¬¡çµ•å°æœƒåˆªä¹¾æ·¨ï¼
        # =======================================================
        echo "ğŸ§¹ Nuke ALL old wheels in Pod (Force)..."
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- \
           "kubectl exec -n {{ slurm_ns }} {{ controller_pod }} -- sh -c 'rm -f {{ shared_dir }}/*.whl'"

        # -------------------------------------------------------
        # 1. å–å¾— Jumpbox ä¸Š WHL è³‡æ–™å¤¾çš„æª”æ¡ˆ
        # -------------------------------------------------------
        WHEEL_FILES=$(ls /home/gary/WHL/*.whl)
        
        if [ -z "$WHEEL_FILES" ]; then
            echo "âŒ Error: No .whl files found in /home/gary/WHL/"
            exit 1
        fi
        
        # -------------------------------------------------------
        # 2. è¿´åœˆï¼šå°‡æ¯å€‹æª”æ¡ˆå‚³é€åˆ° Pod
        # -------------------------------------------------------
        for FILE in $WHEEL_FILES; do
            FILENAME=$(basename "$FILE")
            echo "ğŸš€ Transferring $FILENAME..."
            
            # Jumpbox -> Machine 0
            cat "$FILE" | juju ssh -m {{ test_model }} {{ k8s_machine }} "cat > /tmp/$FILENAME"
            
            # Machine 0 -> Pod
            juju ssh -m {{ test_model }} {{ k8s_machine }} -- \
               "kubectl cp /tmp/$FILENAME {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/$FILENAME"
            
            # Clean up Machine 0
            juju ssh -m {{ test_model }} {{ k8s_machine }} -- "rm -f /tmp/$FILENAME"
        done

        # -------------------------------------------------------
        # 3. é€²å…¥ Pod åŸ·è¡Œå®‰è£
        # -------------------------------------------------------
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- bash -s <<'EOS'
          kubectl exec -n {{ slurm_ns }} {{ controller_pod }} -- bash -c '
            set -e
            cd {{ shared_dir }}
            export PATH=$HOME/.local/bin:$PATH
            
            # å†æ¬¡æª¢æŸ¥ (é€™æ¬¡æ‡‰è©²åªæœƒçœ‹åˆ°æ–°å‚³é€²å»çš„é‚£ 20 å¹¾å€‹æª”æ¡ˆ)
            echo "ğŸ“‚ Files in directory (Should contain NO duplicates):"
            ls -1 *.whl | wc -l
            
            # --- å®‰è£æ‰€æœ‰ .whl æª” ---
            echo "ğŸ“¦ Installing ALL wheels..."
            python3 -m pip install *.whl \
                --no-index \
                --find-links {{ shared_dir }} \
                --user \
                --break-system-packages
            
            echo "ğŸ” Verifying PyTorch..."
            # é©—è­‰æ˜¯å¦èƒ½æŠ“åˆ° CUDA
            python3 -c "import torch; print(f\"ğŸ‰ Success! PyTorch {torch.__version__} is ready. CUDA available: {torch.cuda.is_available()}\")"
            
            if python3 -c "import torch; exit(0 if torch.cuda.is_available() else 1)"; then
               echo "âœ… GPU is detected!"
            else
               echo "âš ï¸ GPU not detected (Running on CPU mode)"
            fi
          '
        EOS
      async: 1800
      poll: 10
      register: install_res

    # ----------------------------------------------------
    # 2. ä½ˆç½²æ¸¬è©¦è…³æœ¬
    # ----------------------------------------------------
    - name: 2.1 æ³¨å…¥ train.py
      shell: |
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- bash -s <<'EOS'
          cat << 'PY' > /tmp/train.py
        import torch
        import time
        import os
        import socket
        import sys

        TARGET_DURATION = 30
        node_name = socket.gethostname()
        job_id = os.environ.get('SLURM_JOB_ID', 'Interactive')

        print(f"\nğŸš€ [Job {job_id}] GPU Test (Node: {node_name})")
        
        if torch.cuda.is_available():
            print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
            dev = torch.device("cuda")
        else:
            print("âŒ No GPU found! Using CPU.")
            dev = torch.device("cpu")

        N = 4096 # çŸ©é™£å¤§å°
        a = torch.randn(N, N, device=dev)
        b = torch.randn(N, N, device=dev)

        print(f"ğŸ”¥ Start Matrix Multiplication loop ({TARGET_DURATION}s)...")
        start_time = time.time()
        count = 0
        
        try:
            while (time.time() - start_time) < TARGET_DURATION:
                c = torch.mm(a, b)
                torch.cuda.synchronize() if torch.cuda.is_available() else None
                count += 1
                sys.stdout.write(f"\râš¡ Batch: {count}")
                sys.stdout.flush()
        except KeyboardInterrupt:
            print("\nStopped.")

        print(f"\nâœ… Finished. Total Batches: {count}")
        PY
          
          kubectl cp /tmp/train.py {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/train.py
        EOS

    - name: 2.2 æ³¨å…¥ run_job.sh
      shell: |
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- bash -s <<'EOS'
          cat << 'SBATCH' > /tmp/run_job.sh
        #!/bin/bash
        #SBATCH --job-name=gpu-test
        #SBATCH --output=gpu-%j.out
        #SBATCH --ntasks=1
        #SBATCH --gres=gpu:1
        #SBATCH --time=00:05:00
        #SBATCH --partition=slinky

        # [ä¿®æ”¹é» 2] ç§»é™¤ source ai-envï¼Œå› ç‚ºæˆ‘å€‘æ˜¯ç”¨ --user å®‰è£åˆ°é è¨­è·¯å¾‘
        echo "â¡ï¸ Starting Training..."
        export PATH=$HOME/.local/bin:$PATH
        
        python3 {{ shared_dir }}/train.py
        SBATCH
        
          kubectl cp /tmp/run_job.sh {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/run_job.sh
        EOS

    # ----------------------------------------------------
    # 3. æç¤ºè¨Šæ¯
    # ----------------------------------------------------
    - name: ğŸš€ ä½ˆç½²å®Œæˆ
      debug:
        msg: 
          - "âœ… å®‰è£å®Œæˆï¼è«‹ç™»å…¥ Controller æ¸¬è©¦ï¼š"
          - "juju ssh -m {{ test_model }} 0 -- kubectl exec -it -n slurm slurm-controller-0 -- bash"
          - "cd /home/slurm"
          - "python3 train.py"
