---
# ========================================================
# Scenario: Slinky Offline AI Environment - Install & Burn-in
# Goal: Install PyTorch (WHL) & Deploy Temp Monitor Burn-in
# ========================================================
- name: ä½ˆç½² Slinky PyTorch èˆ‡ GPU æº«åº¦ç‡’æ©Ÿæ¸¬è©¦
  hosts: maasjuju
  gather_facts: no
  vars:
    # ---- Slinky è¨­å®š ----
    test_model: "slinky-cluster"
    k8s_machine: "0"
    slurm_ns: "slurm"
    controller_pod: "slurm-controller-0"
    shared_dir: "/home/slurm"

  tasks:
    # ----------------------------------------------------
    # 1. é›¢ç·šå®‰è£ Pip èˆ‡ Dependencies (å¼·åŠ›æ¸…é™¤ V3 ç‰ˆ)
    # ----------------------------------------------------
    - name: 1.1 é›¢ç·šå®‰è£æ‰€æœ‰ Wheel å¥—ä»¶ (Bulk Install from WHL)
      shell: |
        # [æ­¥é©Ÿ 0] å¼·åˆ¶æ¸…ç†
        echo "ğŸ§¹ Nuke ALL old wheels in Pod (Force)..."
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- \
            "kubectl exec -n {{ slurm_ns }} {{ controller_pod }} -- sh -c 'rm -f {{ shared_dir }}/*.whl'"

        # [æ­¥é©Ÿ 1] å–å¾— Jumpbox ä¸Š WHL è³‡æ–™å¤¾çš„æª”æ¡ˆ
        WHEEL_FILES=$(ls /home/gary/WHL/*.whl)
        if [ -z "$WHEEL_FILES" ]; then
            echo "âŒ Error: No .whl files found in /home/gary/WHL/"
            exit 1
        fi

        # [æ­¥é©Ÿ 2] å‚³é€åˆ° Pod
        for FILE in $WHEEL_FILES; do
            FILENAME=$(basename "$FILE")
            echo "ğŸš€ Transferring $FILENAME..."
            cat "$FILE" | juju ssh -m {{ test_model }} {{ k8s_machine }} "cat > /tmp/$FILENAME"
            juju ssh -m {{ test_model }} {{ k8s_machine }} -- \
                "kubectl cp /tmp/$FILENAME {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/$FILENAME"
            juju ssh -m {{ test_model }} {{ k8s_machine }} -- "rm -f /tmp/$FILENAME"
        done

        # [æ­¥é©Ÿ 3] é€²å…¥ Pod åŸ·è¡Œå®‰è£ (ä¿®æ­£ï¼šè‡ªèˆ‰ Pip æ¨¡å¼)
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- bash -s <<'EOS'
          kubectl exec -n {{ slurm_ns }} {{ controller_pod }} -- bash -c "
            set -e
            cd {{ shared_dir }}
            
            # 1. æ‰¾åˆ° pip wheel
            BOOTSTRAP_PIP=\$(ls pip-*.whl | head -n 1)
            echo \"ğŸ› ï¸  Found pip wheel: \$BOOTSTRAP_PIP\"
            
            # 2. æ³¨å…¥ PYTHONPATH é€²è¡Œå®‰è£
            PYTHONPATH=\$BOOTSTRAP_PIP python3 -m pip install *.whl \
                --no-index \
                --find-links . \
                --user \
                --break-system-packages
            
            # 3. é©—è­‰
            export PATH=\$HOME/.local/bin:\$PATH
            python3 -m pip --version
            python3 -c 'import torch; print(f\"ğŸ‰ Success! PyTorch {torch.__version__} is ready.\")'
          "
        EOS
      async: 1800
      poll: 10
      register: install_res

    # ----------------------------------------------------
    # 2. ä½ˆç½² GPU ç‡’æ©Ÿèˆ‡æº«åº¦ç›£æ§è…³æœ¬
    # ----------------------------------------------------
    - name: 2.1 æ³¨å…¥ç‡’æ©Ÿç¨‹å¼ (burn_test.py)
      shell: |
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- bash -s <<'EOS'
          cat << 'PY' > /tmp/burn_test.py
        import torch
        import sys
        import time
        import os

        DURATION = 45
        MATRIX_SIZE = 8192

        print(f"ğŸ”¥ Starting GPU Burn-in Test")
        if not torch.cuda.is_available():
            print("âŒ Error: No GPU found!")
            sys.exit(1)

        device = torch.device("cuda")
        x = torch.randn(MATRIX_SIZE, MATRIX_SIZE, device=device)
        y = torch.randn(MATRIX_SIZE, MATRIX_SIZE, device=device)
        start_time = time.time()
        end_time = start_time + DURATION
        batch = 0

        try:
            while time.time() < end_time:
                z = torch.mm(x, y)
                torch.cuda.synchronize()
                batch += 1
                if batch % 10 == 0:
                    temp = os.popen("nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader").read().strip()
                    sys.stdout.write(f"\râš¡ Batch: {batch} | Temp: {temp}Â°C | Left: {int(end_time - time.time())}s ")
                    sys.stdout.flush()
        except KeyboardInterrupt:
            pass
        print(f"\nâœ… Finished! Total Batches: {batch}")
        PY
          kubectl cp /tmp/burn_test.py {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/burn_test.py
        EOS

    - name: 2.2 æ³¨å…¥ä¸¦é€å‡º Slurm ä»»å‹™
      shell: |
        juju ssh -m {{ test_model }} {{ k8s_machine }} -- bash -s <<'EOS'
          cat << 'SBATCH' > /tmp/run_burn.sh
        #!/bin/bash
        #SBATCH --job-name=gpu-burn
        #SBATCH --output=burn-%j.out
        #SBATCH --partition=slinky
        #SBATCH --gres=gpu:1
        #SBATCH --time=00:10:00
        export PATH=$HOME/.local/bin:$PATH
        python3 {{ shared_dir }}/burn_test.py
        SBATCH
          kubectl cp /tmp/run_burn.sh {{ slurm_ns }}/{{ controller_pod }}:{{ shared_dir }}/run_burn.sh
          kubectl exec -n {{ slurm_ns }} {{ controller_pod }} -- bash -c "cd {{ shared_dir }} && scancel -u slurm && sbatch run_burn.sh"
        EOS

    - name: ğŸš€ ä½ˆç½²å®Œæˆ
      debug:
        msg: "âœ… æ¸¬è©¦å·²å•Ÿå‹•ï¼è«‹åŸ·è¡Œ tail -f {{ shared_dir }}/burn-*.out ç›£çœ‹"
