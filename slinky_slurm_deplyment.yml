---
# ==========================================
# SKU: Slinky-K8s-v1.6-Remote-Exec
# ÁâπÈªûÔºö
#   1. ÂÆåÂÖ®‰∏çÂãï maasjuju (‰∏çË£ù Python Â∫´Ôºå‰∏çÊîπ Kubeconfig)
#   2. ÈÄèÈÅé juju ssh Âú® Master ÁØÄÈªûÂÖßÈÉ®Âü∑Ë°å kubectl
#   3. Á¢∫‰øù Local Path Ë£ùÂú®Ê≠£Á¢∫ÁöÑÊñ∞Âè¢ÈõÜ‰∏ä
# ==========================================
- name: Infrastructure Test - Deploy Modern Charmed Kubernetes
  hosts: maasjuju
  gather_facts: yes
  vars:
    test_model: "slinky-cluster"
    k8s_channel: "1.31/stable"
    bundle_path: "{{ ansible_user_dir }}/charmed-k8s-modern.yaml"

  tasks:
    # 1. Áí∞Â¢ÉÊ∏ÖÁêÜ
    - name: 1.0 Âº∑Âà∂Ê∏ÖÁêÜËàäÊ®°Âûã
      command: "juju destroy-model {{ test_model }} --no-prompt --force --destroy-storage"
      register: destroy_result
      failed_when: 
        - destroy_result.rc != 0 
        - '"not found" not in destroy_result.stderr'
      ignore_errors: yes

    - name: 1.1 Á≠âÂæÖÊ®°ÂûãÈáãÊîæ
      pause:
        seconds: 15

    - name: 1.2 Âª∫Á´ãÂÖ®Êñ∞ Juju Model
      shell: juju add-model {{ test_model }}

    # 2. Ë£Ω‰ΩúËóçÂúñ
    - name: 2.0 Ê∏ÖÈô§ËàäÁöÑ Bundle Ê™îÊ°à
      become: yes
      file:
        path: "{{ bundle_path }}"
        state: absent
        
    - name: 2.1 Áî¢ÁîüÊú¨Âú∞ Bundle Ê™îÊ°à
      copy:
        dest: "{{ bundle_path }}"
        mode: '0644'
        content: |
          description: Slinky Modern K8s Cluster
          base: ubuntu@22.04
          
          machines:
            "0":
              constraints: tags=virtual
            "1":
              constraints: tags=slurm-node

          applications:
            easyrsa:
              charm: easyrsa
              channel: latest/stable
              num_units: 1
              to: ["0"]
            
            etcd:
              charm: etcd
              channel: latest/stable
              num_units: 1
              to: ["0"]

            kubernetes-control-plane:
              charm: kubernetes-control-plane
              channel: {{ k8s_channel }}
              num_units: 1
              to: ["0"]
              options:
                allow-privileged: "true"

            kubernetes-worker:
              charm: kubernetes-worker
              channel: {{ k8s_channel }}
              num_units: 1
              to: ["1"]
              expose: true

            calico:
              charm: calico
              channel: latest/stable
              
            containerd:
              charm: containerd
              channel: latest/stable

          relations:
            - ["kubernetes-control-plane:kube-control", "kubernetes-worker:kube-control"]
            - ["kubernetes-control-plane:certificates", "easyrsa:client"]
            - ["etcd:certificates", "easyrsa:client"]
            - ["kubernetes-control-plane:etcd", "etcd:db"]
            - ["kubernetes-worker:certificates", "easyrsa:client"]
            - ["calico:etcd", "etcd:db"]
            - ["calico:cni", "kubernetes-control-plane:cni"]
            - ["calico:cni", "kubernetes-worker:cni"]
            - ["containerd:containerd", "kubernetes-worker:container-runtime"]
            - ["containerd:containerd", "kubernetes-control-plane:container-runtime"]

    # 3. Âü∑Ë°åÈÉ®ÁΩ≤
    - name: 3.1 ÈÉ®ÁΩ≤ Bundle
      shell: |
        juju deploy {{ bundle_path }} --model {{ test_model }} --trust
      register: deploy_out
      until: deploy_out.rc == 0
      retries: 3
      delay: 10

    # -----------------------------------------------------------
    # 4. Áõ£Êéß (ÁÑ°ÈôêÁ≠âÂæÖÁâà)
    # -----------------------------------------------------------
    - name: 4.1 Á≠âÂæÖ K8s ÁãÄÊÖãËÆäÁÇ∫ Active
      shell: |
        echo "‚è≥ ÈÄ≤ÂÖ•ÁÑ°ÈôêÁõ£Ê∏¨Ê®°Âºè... (Ë´ãÊâãÂãïÁõ£Êéß juju status)"
        
        while true; do
          STATUS=$(juju status --model {{ test_model }} --format json | jq -r '.applications["kubernetes-worker"]["application-status"].current')
          CP_STATUS=$(juju status --model {{ test_model }} --format json | jq -r '.applications["kubernetes-control-plane"]["application-status"].current')

          if [ "$STATUS" == "active" ] && [ "$CP_STATUS" == "active" ]; then
            echo "‚úÖ Kubernetes Cluster is Ready!"
            exit 0
          fi
          sleep 10
        done
      args:
        executable: /bin/bash
      register: k8s_wait_result
      changed_when: false 

    # -----------------------------------------------------------
    # 5. Local Path Provisioner (ÈÅ†Á´ØÂü∑Ë°å - ‰∏çÂãï maasjuju)
    # -----------------------------------------------------------
    # [‰øÆÊ≠£ÈáçÈªû]
    # 1. ÁßªÈô§‰∫ÜÊâÄÊúâ pip/apt ÂÆâË£ùÊ≠•È©ü (Task 4.2 Â∑≤Âà™Èô§)
    # 2. ÊîπÁî® juju ssh Âà∞ Machine 0 ÂÖßÈÉ®Âü∑Ë°å kubectl
    # 3. ÈÄôÊ®£ maasjuju ‰∏çÈúÄË¶Å‰ªª‰Ωï K8s Ë®≠ÂÆöÔºå‰πü‰∏çÊúÉË£ùÈåØÂà∞ËàäÂè¢ÈõÜ
    
    - name: 5.1 ÂÆâË£ù Local Path Provisioner (Remote Exec)
      shell: |
        juju ssh --model {{ test_model }} 0 -- \
        "kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.30/deploy/local-path-storage.yaml"
      register: install_sc
      # ÁÇ∫‰∫ÜËÆì Ansible È°ØÁ§∫ changed ÁãÄÊÖãÔºåÊàëÂÄëÁ∞°ÂñÆÂà§Êñ∑Ëº∏Âá∫
      changed_when: "'created' in install_sc.stdout or 'configured' in install_sc.stdout"

    - name: 5.2 Ë®≠ÂÆöÁÇ∫È†êË®≠ StorageClass (Remote Exec)
      shell: |
        juju ssh --model {{ test_model }} 0 -- \
        "kubectl patch storageclass local-path -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'"
      register: patch_sc
      changed_when: "'patched' in patch_sc.stdout"

    - name: ‚úÖ ÈÉ®ÁΩ≤ÂÆåÊàê
      debug:
        msg: "Charmed Kubernetes Â∑≤ÊàêÂäüÈÉ®ÁΩ≤„ÄÇStorageClass Â∑≤ÈÄèÈÅé Machine 0 ÂÖßÈÉ®Êåá‰ª§ÂÆåÊàêÈÖçÁΩÆÔºåÊú™Êõ¥Âãï maasjuju Áí∞Â¢É„ÄÇ"

# -----------------------------------------------------------
# GATE) Wait cluster fully ready (Juju active + K8s nodes/pods ready)
# - ‰∏çÂãï maasjuju ÁöÑ kubeconfig
# - Juju status Áî® from_json Ëß£ÊûêÔºå‰∏çÁî® jq
# - kubectl ÂÖ®ÈÉ®ÈÄèÈÅé juju ssh Âà∞ machine0 ÂÖßÈÉ®Âü∑Ë°å
# -----------------------------------------------------------
- name: GATE - Wait cluster fully ready (Juju active + K8s ready)
  hosts: maasjuju
  gather_facts: no
  vars:
    test_model: "slinky-cluster"
    machine0: "0"
    controlplane_hostname: "slurmhn"
    gpu_node_hostname: "gpu-node01"

  tasks:
    - name: GATE 1 - Wait Juju apps active (control-plane/worker/calico/containerd) [no jq]
      shell: "juju status --model {{ test_model }} --format json"
      register: juju_status_gate
      changed_when: false
      retries: 240
      delay: 5
      until: >
        (
          ((juju_status_gate.stdout | from_json).applications["kubernetes-control-plane"]["application-status"].current | default("unknown")) == "active"
        ) and (
          ((juju_status_gate.stdout | from_json).applications["kubernetes-worker"]["application-status"].current | default("unknown")) == "active"
        ) and (
          ((juju_status_gate.stdout | from_json).applications["calico"]["application-status"].current | default("unknown")) == "active"
        ) and (
          ((juju_status_gate.stdout | from_json).applications["containerd"]["application-status"].current | default("unknown")) == "active"
        )

    - name: GATE 1.1 - Show final Juju app status (for log)
      vars:
        j: "{{ juju_status_gate.stdout | from_json }}"
      debug:
        msg:
          - "cp={{ j.applications['kubernetes-control-plane']['application-status'].current | default('unknown') }}"
          - "wk={{ j.applications['kubernetes-worker']['application-status'].current | default('unknown') }}"
          - "ca={{ j.applications['calico']['application-status'].current | default('unknown') }}"
          - "cd={{ j.applications['containerd']['application-status'].current | default('unknown') }}"

    - name: GATE 1.95 - Wait apiserver /readyz (remote exec on machine0)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -lc '
          set -euo pipefail
          echo "[WAIT] apiserver /readyz ..."
          for i in {1..120}; do
            if kubectl get --raw="/readyz" >/dev/null 2>&1; then
              echo "‚úÖ apiserver ready"
              exit 0
            fi
            echo "‚è≥ apiserver not ready yet ($i/120)"; sleep 5
          done
          echo "‚ùå apiserver not ready (timeout)"
          kubectl get nodes -o wide || true
          kubectl -n kube-system get pods -o wide || true
          exit 1
        '
      changed_when: false

    - name: GATE 2 - Wait K8s nodes Ready + kube-system core pods Ready (remote exec on machine0)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
        set -euo pipefail

        echo "== nodes =="
        kubectl get nodes -o wide

        echo
        echo "[WAIT] nodes Ready..."
        kubectl wait node/{{ controlplane_hostname }} --for=condition=Ready --timeout=900s
        kubectl wait node/{{ gpu_node_hostname }}     --for=condition=Ready --timeout=900s

        echo
        echo "== kube-system pods (non-Running) =="
        kubectl -n kube-system get pods | egrep -v "Running|Completed" || true

        echo
        echo "[WAIT] calico-node Ready..."
        kubectl -n kube-system wait pod -l k8s-app=calico-node --for=condition=Ready --timeout=900s

        echo
        echo "[WAIT] coredns/kube-dns Ready..."
        kubectl -n kube-system wait pod -l k8s-app=kube-dns --for=condition=Ready --timeout=600s 2>/dev/null || \
        kubectl -n kube-system wait pod -l k8s-app=coredns  --for=condition=Ready --timeout=600s 2>/dev/null || true

        echo
        echo "‚úÖ K8s layer ready"
        kubectl get nodes -o wide
        kubectl -n kube-system get pods -o wide
        EOS
      changed_when: false
      
---
# ========================================================
# SKU: Slinky-SlurmOnK8s-Golden (IPv6-Disabled, Robust)
#
# Fixes applied:
#   1. Nuclear cleanup of conflicting NVIDIA APT repos.
#   2. System-wide IPv6 Disable (Kernel level).
#   3. Force-kill K8s processes to refresh /etc/hosts (fix ::1 resolution).
#   4. Aggressive Kube-Cache cleaning to fix Helm connection errors.
#   5. Robust Bash syntax (HereDoc) to avoid quoting errors.
# ========================================================

- name: Application Layer - Deploy Slinky Slurm on K8s (Golden Standard)
  hosts: maasjuju
  gather_facts: yes

  vars:
    # ---- Mode ----
    reset_before_install: true

    # ---- Juju / machines ----
    test_model: "slinky-cluster"
    machine0: "0"                 # control-plane node (slurmhn)
    machine1: "1"                 # gpu worker node (gpu-node01)

    # ---- K8s nodes ----
    controlplane_hostname: "slurmhn"
    gpu_node_hostname: "gpu-node01"

    # ---- Namespaces ----
    ns_nfs: "nfs-provisioner"
    ns_nvidia: "nvidia-device-plugin"
    ns_certmgr: "cert-manager"
    ns_slurm_operator: "slinky"
    ns_slurm: "slurm"

    # ---- NFS (server on machine0) ----
    nfs_server_setup: true
    nfs_server: ""                # leave empty => use machine0_ip
    nfs_export_path: "/srv/nfs/k8s"
    nfs_storageclass: "nfs-rwx"
    nfs_make_default_sc: false

    # ---- NVIDIA device plugin ----
    gpu_taint_key: "node-role.anxpert/gpu"
    gpu_taint_value: "true"
    slurm_gpu_count: 1

    # ---- Versions ----
    cert_manager_chart_version: "v1.19.2"
    slurm_operator_chart_ver: "1.0.1"
    slurm_chart_ver: "1.0.1"

  tasks:
    # -----------------------------------------------------------
    # 1) Discover machine IPs from Juju
    # -----------------------------------------------------------
    - name: 1.1 Get Juju model status (json)
      shell: "juju status --model {{ test_model }} --format json"
      register: juju_status_raw
      changed_when: false

    - name: 1.2 Parse machine0/machine1 IPv4
      set_fact:
        machine0_ip: "{{ (juju_status_raw.stdout | from_json).machines[machine0]['ip-addresses'] | select('match','^[0-9.]+$') | first }}"
        machine1_ip: "{{ (juju_status_raw.stdout | from_json).machines[machine1]['ip-addresses'] | select('match','^[0-9.]+$') | first }}"

    - name: 1.3 Show discovered IPs
      debug:
        msg:
          - "Machine0 (control-plane) IP: {{ machine0_ip }}"
          - "Machine1 (gpu worker)    IP: {{ machine1_ip }}"

    - name: 1.4 Decide NFS server IP
      set_fact:
        nfs_server_ip: "{{ (nfs_server | trim) if (nfs_server | trim != '') else machine0_ip }}"

    # -----------------------------------------------------------
    # 2) Ensure Helm exists on machine0
    # -----------------------------------------------------------
    - name: 2.1 Install Helm on machine0 if missing
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
          set -e
          if ! command -v helm >/dev/null 2>&1; then
            echo "[INFO] Installing Helm..."
            curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          else
            echo "[INFO] Helm already installed"
          fi
        EOS
      changed_when: false

    # -----------------------------------------------------------
    # 0) Reset (two-mode)
    # -----------------------------------------------------------
    - name: 0.1 Reset (helm uninstall releases)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
          set +e
          echo "[RESET] uninstall releases..."
          helm -n {{ ns_slurm }} uninstall slurm 2>/dev/null || true
          helm -n {{ ns_slurm_operator }} uninstall slurm-operator 2>/dev/null || true
          helm -n default uninstall slurm-operator-crds 2>/dev/null || true
          helm -n {{ ns_certmgr }} uninstall cert-manager 2>/dev/null || true
          helm -n {{ ns_nvidia }} uninstall nvidia-device-plugin 2>/dev/null || true
          helm -n {{ ns_nfs }} uninstall nfs-provisioner 2>/dev/null || true
        EOS
      when: reset_before_install | bool
      changed_when: true

    - name: 0.2 Reset (delete namespaces)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
          set +e
          echo "[RESET] delete namespaces..."
          kubectl delete ns {{ ns_slurm }} --ignore-not-found=true --wait=true || true
          kubectl delete ns {{ ns_slurm_operator }} --ignore-not-found=true --wait=true || true
          kubectl delete ns {{ ns_certmgr }} --ignore-not-found=true --wait=true || true
          kubectl delete ns {{ ns_nvidia }} --ignore-not-found=true --wait=true || true
          kubectl delete ns {{ ns_nfs }} --ignore-not-found=true --wait=true || true
        EOS
      when: reset_before_install | bool
      changed_when: true

    - name: 0.3 Reset (webhooks cleanup)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
          set +e
          echo "[RESET] Cleaning up all Webhooks..."
          kubectl delete mutatingwebhookconfigurations -l app.kubernetes.io/name=slurm-operator --ignore-not-found=true
          kubectl delete validatingwebhookconfigurations -l app.kubernetes.io/name=slurm-operator --ignore-not-found=true
        EOS
      when: reset_before_install | bool
      changed_when: true

    # -----------------------------------------------------------
    # 3) Install NFS Utils (With Repo Cleanup First)
    # -----------------------------------------------------------
    - name: 3.0 Setup NFS server export on machine0
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
        set -euo pipefail
        export DEBIAN_FRONTEND=noninteractive NEEDRESTART_MODE=a
        sudo -n true

        echo "[INFO] wait dpkg lock..."
        while sudo fuser /var/lib/dpkg/lock-frontend >/dev/null 2>&1; do echo "[WAIT]"; sleep 5; done
        
        # Cleanup potential bad repos first
        sudo rm -f /etc/apt/sources.list.d/nvidia-*.list 2>/dev/null || true
        
        timeout 600 sudo apt-get -o Acquire::ForceIPv4=true update -y
        timeout 600 sudo apt-get -o Acquire::ForceIPv4=true install -y nfs-kernel-server

        sudo mkdir -p {{ nfs_export_path }}
        sudo chmod 777 {{ nfs_export_path }}
        sudo mkdir -p /etc/exports.d
        sudo touch /etc/exports
        
        EXPORT_LINE="{{ nfs_export_path }} *(rw,sync,no_subtree_check,no_root_squash)"
        echo "$EXPORT_LINE" | sudo tee /etc/exports.d/k8s.exports >/dev/null
        sudo exportfs -ra
        sudo systemctl enable --now nfs-server
        EOS
      when: nfs_server_setup | bool
      changed_when: true

    - name: 3.1 Install nfs-common on k8s nodes (CLEANUP BROKEN REPOS FIRST)
      shell: |
        juju ssh --model {{ test_model }} {{ item }} -- bash -s <<'EOS'
        set -euo pipefail
        export DEBIAN_FRONTEND=noninteractive NEEDRESTART_MODE=a
        sudo -n true

        # --- [CRITICAL FIX] ---
        echo "[FIX] Cleaning up conflicting NVIDIA repo lists to unblock apt..."
        sudo rm -f /etc/apt/sources.list.d/nvidia-container-toolkit.list
        sudo rm -f /etc/apt/sources.list.d/nvidia-docker.list
        sudo rm -f /etc/apt/sources.list.d/libnvidia-container.list
        # ----------------------

        echo "[INFO] wait dpkg lock..."
        while sudo fuser /var/lib/dpkg/lock-frontend >/dev/null 2>&1; do sleep 5; done

        timeout 600 sudo apt-get -o Acquire::ForceIPv4=true update -y
        timeout 600 sudo apt-get -o Acquire::ForceIPv4=true install -y nfs-common
        command -v mount.nfs || ls -l /sbin/mount.nfs* || true
        EOS
      loop:
        - "{{ machine0 }}"
        - "{{ machine1 }}"
      changed_when: true

    # -----------------------------------------------------------
    # 4) Environment Fixes & NFS Provisioner
    # -----------------------------------------------------------
    - name: 4.1 Create namespace for NFS provisioner
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
          kubectl create ns {{ ns_nfs }} --dry-run=client -o yaml | kubectl apply -f -
        EOS
      changed_when: false

    - name: 4.2 Install NFS Provisioner (Force Process Restart + Hosts Fix)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
        set -euo pipefail

        echo "[FIX] Ensuring /etc/hosts has NO ::1 ..."
        if grep -q "^::1" /etc/hosts; then
            sudo sed -i 's/^::1/#::1/g' /etc/hosts
        fi

        # 1. [ÁµïÊãõ] Áõ¥Êé•ÊÆ∫ÊéâÈÄ≤Á®ãÔºåÂº∑Ëø´ÈáçÂïü (‰∏çÁÆ°ÊòØ Snap ÈÇÑÊòØ Apt Áâà)
        echo "[FIX] Force killing K8s processes to refresh hosts..."
        sudo pkill -f kube-apiserver || true
        sudo pkill -f kubelet || true
        
        # 2. Á≠âÂæÖÂæ©Ê¥ª
        echo "[WAIT] Waiting for API Server to respawn (max 120s)..."
        KUBE_DIR="$HOME/.kube"
        USER_CONF="$KUBE_DIR/config"
        
        # Á¢∫‰øù Config ÊòØ IPv4
        if [ -f "$USER_CONF" ]; then
             sudo chown $(id -u):$(id -g) "$USER_CONF"
             sed -i 's|https://\[::1\]:6443|https://127.0.0.1:6443|g' "$USER_CONF"
             sed -i 's|https://localhost:6443|https://127.0.0.1:6443|g' "$USER_CONF"
        fi
        
        export KUBECONFIG="$USER_CONF"
        for i in {1..40}; do
            if kubectl get nodes >/dev/null 2>&1; then
                echo " -> API Server is UP!"
                break
            fi
            sleep 3
        done

        # 3. [ÈóúÈçµ] Âà™Èô§ÊâÄÊúâ Webhook
        echo "[FIX] Cleaning up potential zombie Webhooks..."
        kubectl delete validatingwebhookconfigurations --all --ignore-not-found=true
        kubectl delete mutatingwebhookconfigurations --all --ignore-not-found=true

        # 4. Ê∏ÖÈô§ Client Cache
        rm -rf "$KUBE_DIR/cache" "$KUBE_DIR/http-cache"

        # --- Helm Execution ---
        helm --kubeconfig "$USER_CONF" repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ >/dev/null 2>&1 || true
        helm --kubeconfig "$USER_CONF" repo update >/dev/null 2>&1

        f=/tmp/nfs-values.yaml
        cat > "$f" <<EOF
        nfs:
          server: {{ nfs_server_ip }}
          path: {{ nfs_export_path }}
        storageClass:
          name: {{ nfs_storageclass }}
          defaultClass: {{ nfs_make_default_sc | bool | lower }}
          reclaimPolicy: Delete
          archiveOnDelete: true
        EOF

        echo "[INFO] Helm install NFS provisioner..."
        helm --kubeconfig "$USER_CONF" upgrade --install nfs-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
          -n {{ ns_nfs }} --create-namespace \
          -f "$f"
        EOS
      changed_when: true

    - name: 4.3 Install NVIDIA Container Toolkit on GPU Node (Nuclear Cleanup Fixed)
      shell: |
        juju ssh --model {{ test_model }} {{ machine1 }} -- bash -s <<'EOS'
        set -euo pipefail
        export DEBIAN_FRONTEND=noninteractive NEEDRESTART_MODE=a
        sudo -n true

        echo "[INFO] wait dpkg/apt lock..."
        while sudo fuser /var/lib/dpkg/lock-frontend >/dev/null 2>&1; do sleep 5; done

        # --- [NUCLEAR FIX START] ---
        echo "[FIX] Hunting down ALL references to nvidia-container repositories..."
        sudo rm -f /etc/apt/sources.list.d/nvidia-*.list
        sudo rm -f /etc/apt/sources.list.d/libnvidia-*.list
        sudo rm -f /etc/apt/sources.list.d/cuda*.list
        (grep -l "nvidia.github.io/libnvidia-container" /etc/apt/sources.list.d/* 2>/dev/null || true) | xargs -r sudo rm -f
        if grep -q "nvidia.github.io/libnvidia-container" /etc/apt/sources.list; then
            sudo sed -i '/nvidia.github.io\/libnvidia-container/d' /etc/apt/sources.list
        fi
        # --- [NUCLEAR FIX END] ---

        echo "[INFO] add NVIDIA Container Toolkit repo (IPv4 + retry)..."
        curl -4 --retry 6 --retry-connrefused -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor --yes -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
        curl -4 --retry 6 --retry-connrefused -fsSL https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
          sed "s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g" | \
          sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list >/dev/null

        echo "[INFO] apt update/install..."
        sudo apt-get clean
        timeout 600 sudo apt-get -o Acquire::ForceIPv4=true update -y
        timeout 600 sudo apt-get -o Acquire::ForceIPv4=true install -y nvidia-container-toolkit

        echo "[INFO] configure containerd runtime..."
        sudo nvidia-ctk runtime configure --runtime=containerd
        sudo systemctl restart containerd
        sudo systemctl restart kubelet || true
        EOS
      changed_when: true

    - name: 4.4 Disable IPv6 Completely on ALL nodes
      shell: |
        juju ssh --model {{ test_model }} {{ item }} -- bash -s <<'EOS'
        set -euo pipefail
        echo "[INFO] Disabling IPv6 system-wide on {{ item }}..."
        cat <<EOF | sudo tee /etc/sysctl.d/99-disable-ipv6.conf >/dev/null
        net.ipv6.conf.all.disable_ipv6 = 1
        net.ipv6.conf.default.disable_ipv6 = 1
        net.ipv6.conf.lo.disable_ipv6 = 1
        EOF
        sudo sysctl --system >/dev/null
        
        # Ê™¢Êü•
        if [ "$(cat /proc/sys/net/ipv6/conf/all/disable_ipv6)" == "1" ]; then
            echo "‚úÖ IPv6 is DISABLED."
        fi
        
        # Restart to flush connections
        sudo systemctl restart containerd
        sudo systemctl restart kubelet || true
        EOS
      loop:
        - "{{ machine0 }}"
        - "{{ machine1 }}"
      changed_when: true

    # -----------------------------------------------------------
    # 5) NVIDIA device plugin
    # -----------------------------------------------------------
    - name: "5.2 Install/upgrade NVIDIA device plugin (Robust + Webhook cleanup)"
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
        set -euo pipefail
        
        # Cleanup again to be safe
        kubectl delete validatingwebhookconfigurations -l app.kubernetes.io/name=nvidia-device-plugin --ignore-not-found=true
        kubectl delete mutatingwebhookconfigurations -l app.kubernetes.io/name=nvidia-device-plugin --ignore-not-found=true

        helm repo add nvidia https://nvidia.github.io/k8s-device-plugin >/dev/null 2>&1 || true
        helm repo update >/dev/null 2>&1

        f=/tmp/nvidia-values.yaml
        : > "$f"
        w(){ printf "%s\n" "$1" >> "$f"; }

        w "affinity:"
        w "  nodeAffinity:"
        w "    requiredDuringSchedulingIgnoredDuringExecution:"
        w "      nodeSelectorTerms:"
        w "      - matchExpressions:"
        w "        - key: kubernetes.io/hostname"
        w "          operator: In"
        w "          values:"
        w "          - {{ gpu_node_hostname }}"
        w "nodeSelector:"
        w "  kubernetes.io/hostname: {{ gpu_node_hostname }}"
        w "  kubernetes.io/os: linux"
        w "tolerations:"
        w "- key: {{ gpu_taint_key }}"
        w "  operator: Equal"
        w "  value: \"{{ gpu_taint_value }}\""
        w "  effect: NoSchedule"

        helm upgrade --install nvidia-device-plugin nvidia/nvidia-device-plugin \
          -n {{ ns_nvidia }} --create-namespace \
          --reset-values \
          -f "$f" \
          --wait --timeout=600s
        EOS
      changed_when: true

    # -----------------------------------------------------------
    # 6) cert-manager
    # -----------------------------------------------------------
    - name: 6.2 Install/upgrade cert-manager via Helm
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
        set -euo pipefail
        helm repo add jetstack https://charts.jetstack.io >/dev/null 2>&1 || true
        helm repo update >/dev/null 2>&1

        helm upgrade --install cert-manager jetstack/cert-manager \
          -n {{ ns_certmgr }} --create-namespace \
          --version {{ cert_manager_chart_version }} \
          --set crds.enabled=true \
          --wait --timeout=600s
        EOS
      changed_when: true

    # -----------------------------------------------------------
    # 7) slurm-operator
    # -----------------------------------------------------------
    - name: 7.2 Install/upgrade slurm-operator-crds (OCI)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
          set -euo pipefail
          helm upgrade --install slurm-operator-crds oci://ghcr.io/slinkyproject/charts/slurm-operator-crds \
            -n default --create-namespace \
            --version {{ slurm_operator_chart_ver }}
        EOS
      changed_when: true

    - name: 7.3 Install/upgrade slurm-operator (OCI) - Extended Timeout
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
        set -euo pipefail
        NS="{{ ns_slurm_operator }}"
        
        helm upgrade --install slurm-operator oci://ghcr.io/slinkyproject/charts/slurm-operator \
            -n "$NS" --create-namespace \
            --version {{ slurm_operator_chart_ver }}

        echo "[INFO] Waiting for rollout (timeout=600s)..."
        if ! kubectl -n "$NS" rollout status deploy/slurm-operator --timeout=600s; then
            echo "‚ùå Rollout Failed!"
            kubectl -n "$NS" get events --sort-by='.lastTimestamp' | tail -n 20
            exit 1
        fi
        EOS
      changed_when: true

    - name: 7.4 Wait for Slurm Operator Webhook endpoints
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
        set -euo pipefail
        NS="{{ ns_slurm_operator }}"
        kubectl -n "$NS" rollout status deploy/slurm-operator-webhook --timeout=300s || true
        SVC=$(kubectl -n "$NS" get svc -o name | grep -i webhook | head -n1 | cut -d/ -f2)
        
        for i in {1..120}; do
          EP=$(kubectl -n "$NS" get ep "$SVC" -o jsonpath="{.subsets[*].addresses[*].ip}" 2>/dev/null || true)
          if [ -n "$EP" ]; then
            echo "‚úÖ Webhook endpoints ready: $EP"
            exit 0
          fi
          sleep 5
        done
        exit 1
        EOS
      changed_when: false

    # -----------------------------------------------------------
    # 8) slurm chart (OCI)
    # -----------------------------------------------------------
    - name: 8.2 Install/upgrade slurm chart (OCI)
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
        set -euo pipefail
        NS="{{ ns_slurm }}"
        CHART_VER="{{ slurm_chart_ver }}"

        cat > /tmp/slurm-values.yaml <<EOF
        controller:
          persistence:
            enabled: true
            storageClassName: "{{ nfs_storageclass }}"
            accessModes: [ReadWriteOnce]
            resources:
              requests:
                storage: 10Gi
          extraConf: |
            GresTypes=gpu
          podSpec:
            nodeSelector:
              kubernetes.io/hostname: {{ controlplane_hostname }}
              kubernetes.io/os: linux
            tolerations:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
                effect: NoSchedule
              - key: node-role.kubernetes.io/master
                operator: Exists
                effect: NoSchedule
        restapi:
          replicas: 1
          podSpec:
            nodeSelector:
              kubernetes.io/hostname: {{ controlplane_hostname }}
              kubernetes.io/os: linux
            tolerations:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
                effect: NoSchedule
        nodesets:
          slinky:
            enabled: true
            partition:
              enabled: false
            useResourceLimits: true
            podSpec:
              nodeSelector:
                kubernetes.io/hostname: {{ gpu_node_hostname }}
                kubernetes.io/os: linux
              tolerations:
                - key: {{ gpu_taint_key }}
                  operator: Equal
                  value: "{{ gpu_taint_value }}"
                  effect: NoSchedule
            slurmd:
              resources:
                limits:
                  nvidia.com/gpu: {{ slurm_gpu_count }}
              args:
                - --conf
                - Gres=gpu:{{ slurm_gpu_count }}
        partitions:
          all:
            enabled: true
            nodesets: [ALL]
            configMap:
              Default: "YES"
              MaxTime: UNLIMITED
              State: UP
          slinky:
            enabled: true
            nodesets: [ALL]
            configMap:
              Default: "NO"
              MaxTime: UNLIMITED
              State: UP
        configFiles:
          gres.conf: |
            AutoDetect=nvidia
        EOF

        # Ensure yq
        if ! [ -x /tmp/yq ]; then
          curl -fsSL -o /tmp/yq https://github.com/mikefarah/yq/releases/download/v4.44.3/yq_linux_amd64
          chmod +x /tmp/yq
        fi

        # Post-renderer
        cat > /tmp/slurm-post-renderer.sh <<'SH'
        #!/usr/bin/env bash
        set -euo pipefail
        YQ="/tmp/yq"
        in="$(mktemp)"
        cat > "$in"
        # 1. Move initContainers to containers
        "$YQ" eval -i '
          (select(.kind=="StatefulSet" and .metadata.name=="slurm-controller").spec.template.spec.containers) += 
          (select(.kind=="StatefulSet" and .metadata.name=="slurm-controller").spec.template.spec.initContainers) |
          (select(.kind=="StatefulSet" and .metadata.name=="slurm-controller").spec.template.spec.initContainers) = []
        ' "$in"
        # 2. Force tcpSocket probes
        "$YQ" eval -i '
          (select(.kind=="StatefulSet" and .metadata.name=="slurm-controller").spec.template.spec.containers[] | select(.name=="slurmctld")
          ).startupProbe = {"tcpSocket":{"port":6817},"failureThreshold":240,"periodSeconds":5,"timeoutSeconds":1} |
          (select(.kind=="StatefulSet" and .metadata.name=="slurm-controller").spec.template.spec.containers[] | select(.name=="slurmctld")
          ).readinessProbe = {"tcpSocket":{"port":6817},"failureThreshold":24,"periodSeconds":5,"timeoutSeconds":1} |
          (select(.kind=="StatefulSet" and .metadata.name=="slurm-controller").spec.template.spec.containers[] | select(.name=="slurmctld")
          ).livenessProbe = {"tcpSocket":{"port":6817},"failureThreshold":12,"periodSeconds":10,"timeoutSeconds":1} |
          (select(.kind=="NodeSet" and .metadata.name=="slurm-worker-slinky").spec.slurmd).startupProbe =
            {"tcpSocket":{"port":6818},"failureThreshold":240,"periodSeconds":5,"timeoutSeconds":1} |
          (select(.kind=="NodeSet" and .metadata.name=="slurm-worker-slinky").spec.slurmd).readinessProbe =
            {"tcpSocket":{"port":6818},"failureThreshold":24,"periodSeconds":5,"timeoutSeconds":1} |
          (select(.kind=="NodeSet" and .metadata.name=="slurm-worker-slinky").spec.slurmd).livenessProbe =
            {"tcpSocket":{"port":6818},"failureThreshold":12,"periodSeconds":10,"timeoutSeconds":1}
        ' "$in"
        cat "$in"
        rm "$in"
        SH
        chmod +x /tmp/slurm-post-renderer.sh

        helm upgrade --install slurm oci://ghcr.io/slinkyproject/charts/slurm \
            -n "$NS" --create-namespace \
            --version "$CHART_VER" \
            -f /tmp/slurm-values.yaml \
            --post-renderer /tmp/slurm-post-renderer.sh \
            --wait --timeout=600s
        EOS
      changed_when: true

    # -----------------------------------------------------------
    # 9) Restart controller + worker (Robust check)
    # -----------------------------------------------------------
    - name: 9.1 Restart slurm pods and Wait for Ready
      shell: |
        juju ssh --model {{ test_model }} {{ machine0 }} -- bash -s <<'EOS'
          set -e
          echo "[INFO] Restarting pods..."
          kubectl -n {{ ns_slurm }} delete pod slurm-controller-0 --ignore-not-found=true --wait=true
          kubectl -n {{ ns_slurm }} delete pod slurm-worker-slinky-0 --ignore-not-found=true --wait=true
          sleep 10
          kubectl -n {{ ns_slurm }} wait --for=condition=Ready pod/slurm-controller-0 --timeout=300s
          kubectl -n {{ ns_slurm }} wait --for=condition=Ready pod/slurm-worker-slinky-0 --timeout=300s
          echo "üéâ All Pods Ready!"
        EOS
      changed_when: true

    - name: Done
      debug:
        msg:
          - "üéâ Standardized Slinky Slurm on K8s deployed (GOLDEN EDITION)"
          - "System IPv6 Disabled. Kube-cache cleared. Hosts fixed."
          - "RTX 4090 Ready for jobs!"
