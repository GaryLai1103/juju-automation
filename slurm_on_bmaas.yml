# ==========================================
# SKU: HPC-Hybrid-v7.0 (Native OS + Ansible App)
# é‚è¼¯ï¼šJuju ä½ˆç½² OSï¼ŒAnsible æ‰‹å‹•å®‰è£ K3s/Slurm/NVIDIA
# ==========================================
- name: 1. åŸºç¤å»ºè¨­ (Juju ä¾›æ‡‰ OS)
  hosts: maasjuju
  gather_facts: no
  vars:
    slurm_model: "hpc-lab"
    target_os: "ubuntu@24.04"
    # ç¡¬é«”æ¨™ç±¤å®šç¾©
    master_tags: "tags=virtual"
    worker_tags: "tags=slurm-node"

  tasks:
    - name: 1.1 åˆå§‹åŒ–ç’°å¢ƒ
      shell: |
        juju add-model {{ slurm_model }} || true
        # è«‹æ±‚ Machine 0 (Master)
        juju add-machine -m {{ slurm_model }} --constraints {{ master_tags }} --base {{ target_os }}
        # è«‹æ±‚ Machine 1 (Worker)
        juju add-machine -m {{ slurm_model }} --constraints {{ worker_tags }} --base {{ target_os }}

    - name: 1.2 ç­‰å¾…å…©å°æ©Ÿå™¨ SSH å•Ÿå‹•
      shell: |
        echo "Waiting for Machine 0 & 1..."
        while ! juju ssh -m {{ slurm_model }} 0 -- 'echo up' >/dev/null 2>&1; do sleep 5; done
        while ! juju ssh -m {{ slurm_model }} 1 -- 'echo up' >/dev/null 2>&1; do sleep 5; done
      changed_when: false

    - name: 1.2.1 æ ¹æ“š MAAS Tag å‹•æ…‹åµæ¸¬ Machine ID
      shell: |
        STATUS=$(juju machines -m {{ slurm_model }} --format json)
        # å°‹æ‰¾å¸¶æœ‰ virtual tag çš„æ©Ÿå™¨ ID (Master)
        M0=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ master_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        # å°‹æ‰¾å¸¶æœ‰ slurm-node tag çš„æ©Ÿå™¨ ID (Worker)
        M1=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ worker_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        echo "$M0|$M1"
      register: detected_ids
      changed_when: false

    - name: 1.2.2 å„²å­˜ ID è®Šæ•¸
      set_fact:
        master_id: "{{ detected_ids.stdout.split('|')[0] }}"
        worker_id: "{{ detected_ids.stdout.split('|')[1] }}"

    - name: 1.3 Machine 0ï¼šå®‰è£ K3s (åŒ…å« TLS SAN ä¿®æ­£)
      shell: |
        juju ssh -m {{ slurm_model }} {{ master_id }} '
          # 1. è‡ªå‹•æŠ“å–é€™å°æ©Ÿå™¨çš„å¯¦é«” IP
          NODE_IP=$(hostname -I | awk "{print \$1}")
          
          # 2. å®‰è£ K3s ä¸¦æ³¨å…¥ TLS SAN
          curl -sfL https://get.k3s.io | sudo sh -s - server \
            --write-kubeconfig-mode 644 \
            --tls-san $NODE_IP \
            --disable traefik \
            --disable servicelb
            
          sudo kubectl get nodes
        '
      changed_when: false

    - name: 1.4 Machine 1ï¼šå®‰è£ NVIDIA Driver (æ‰‹å‹•æ¨¡å¼)
      shell: |
        juju ssh -m {{ slurm_model }} {{ worker_id }} '
          sudo apt-get update && sudo apt-get install -y ubuntu-drivers-common
          if ! nvidia-smi; then
            sudo ubuntu-drivers autoinstall
            sudo reboot
          fi
        '
      ignore_errors: yes

    - name: 1.5 ç­‰å¾… Worker é‡å•Ÿå®Œæˆ
      shell: |
        echo "Waiting for Worker back..."
        sleep 30
        while ! juju ssh -m {{ slurm_model }} {{ worker_id }} -- 'nvidia-smi' >/dev/null 2>&1; do sleep 10; done
      changed_when: false

# ==========================================
# ç¬¬äºŒéšæ®µï¼šæ‰‹å‹•ä½ˆç½² Slurm (å» Charm åŒ–)
# ==========================================
- name: 2. Slurm åŸç”Ÿå®‰è£èˆ‡é…ç½®
  hosts: maasjuju
  vars:
    slurm_model: "hpc-lab"
    # é»ƒé‡‘è¦æ ¼æ•¸å€¼
    node_cpus: 18
    node_mem: 128646
    # ç¡¬é«”æ¨™ç±¤å®šç¾©
    master_tags: "tags=virtual"
    worker_tags: "tags=slurm-node"
  tasks:
    - name: 2.0.1 æ ¹æ“š MAAS Tag å‹•æ…‹åµæ¸¬ Machine ID
      shell: |
        STATUS=$(juju machines -m {{ slurm_model }} --format json)
        # å°‹æ‰¾å¸¶æœ‰ virtual tag çš„æ©Ÿå™¨ ID (Master)
        M0=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ master_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        # å°‹æ‰¾å¸¶æœ‰ slurm-node tag çš„æ©Ÿå™¨ ID (Worker)
        M1=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ worker_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        echo "$M0|$M1"
      register: detected_ids
      changed_when: false

    - name: 2.0.2 å„²å­˜ ID è®Šæ•¸
      set_fact:
        master_id: "{{ detected_ids.stdout.split('|')[0] }}"
        worker_id: "{{ detected_ids.stdout.split('|')[1] }}"
  
    - name: 2.1 æŠ“å– Master IP (ç”¨æ–¼ hosts è§£æ)
      shell: "juju ssh -m {{ slurm_model }} {{ master_id }} 'hostname -I' | awk '{print $1}'"
      register: master_ip

    - name: 2.2 æ§åˆ¶ç«¯å®‰è£ Slurmctld
      shell: |
        juju ssh -m {{ slurm_model }} {{ master_id }} '
          sudo apt-get update && sudo apt-get install -y slurmctld munge
          sudo systemctl enable munge --now
        '

    - name: 2.3 é‹ç®—ç«¯å®‰è£ Slurmd
      shell: |
        MASTER_IP="{{ master_ip.stdout }}"
        juju ssh -m {{ slurm_model }} {{ worker_id }} "
          sudo apt-get update && sudo apt-get install -y slurmd munge
          echo '$MASTER_IP slurm-master' | sudo tee -a /etc/hosts
          sudo systemctl enable munge --now
        "

    - name: 2.4 åŒæ­¥ Munge Key (Master to Worker)
      shell: |
        # å¾ Master æŠ“ Key åˆ°è·³æ¿æ©Ÿï¼Œå†å‚³çµ¦ Worker
        juju ssh -m {{ slurm_model }} {{ master_id }} 'sudo cat /etc/munge/munge.key' > /tmp/munge.key
        juju scp /tmp/munge.key {{ slurm_model }}:{{ worker_id }}:/tmp/munge.key
        juju ssh -m {{ slurm_model }} {{ worker_id }} 'sudo mv /tmp/munge.key /etc/munge/munge.key && sudo chown munge:munge /etc/munge/munge.key && sudo systemctl restart munge'
        rm /tmp/munge.key
          
    - name: 2.5 åœ¨ Worker å®‰è£ GPU ç›£æ§ (DCGM Exporter)
      shell: |
        juju ssh -m {{ slurm_model }} {{ worker_id }} "
          sudo snap install dcgm || true
          sudo snap start --enable dcgm.nv-hostengine
          sleep 5
          sudo snap start --enable dcgm.dcgm-exporter
          sleep 5
          sudo snap restart dcgm.nv-hostengine
          sleep 10
          sudo snap restart dcgm.dcgm-exporter
        "
      changed_when: true
      
    - name: 2.6 è¨­å®š DCGM Exporter (çµ‚æ¥µç‰ˆï¼šå¯«å…¥æ‰€æœ‰é—œéµæŒ‡æ¨™ & é‡å•Ÿ)
      shell: |
        juju ssh -m {{ slurm_model }} {{ worker_id }} "
          # 1. [æ¸…ç†] ç§»é™¤èˆŠæª”
          sudo rm -f /var/snap/dcgm/common/custom-metrics.csv

          # 2. [å¯«å…¥] ä½¿ç”¨ printf å¯«å…¥å®Œæ•´æŒ‡æ¨™æ¸…å–® (åŒ…å« PCIe, XID, Power, Clocks, Fan)
          # æ³¨æ„ï¼šæè¿°ä¸­çš„ % ç¬¦è™Ÿå·²è½‰ç¾©ç‚º %% ä»¥é˜²æ­¢ printf éŒ¯èª¤
          sudo bash -c 'printf \"DCGM_FI_DEV_DEC_UTIL,      gauge, Decoder utilization (in %%%%)\nDCGM_FI_DEV_ENC_UTIL,      gauge, Encoder utilization (in %%%%)\nDCGM_FI_DEV_GPU_TEMP,      gauge, GPU temperature (in C)\nDCGM_FI_DEV_GPU_UTIL,      gauge, GPU utilization (in %%%%)\nDCGM_FI_DEV_MEM_CLOCK,     gauge, Memory clock frequency (in MHz)\nDCGM_FI_DEV_MEM_COPY_UTIL, gauge, Memory utilization (in %%%%)\nDCGM_FI_DEV_POWER_USAGE,   gauge, Power usage (in W)\nDCGM_FI_DEV_SM_CLOCK,      gauge, SM clock frequency (in MHz)\nDCGM_FI_DEV_TOTAL_ENERGY_CONSUMPTION, counter, Total energy consumption since boot (in mJ)\nDCGM_FI_DEV_FB_USED,       gauge, Framebuffer memory used (in MiB)\nDCGM_FI_DEV_FB_FREE,       gauge, Framebuffer memory free (in MiB)\nDCGM_FI_DEV_FAN_SPEED,     gauge, Fan speed (in %%%%)\nDCGM_FI_DEV_MEMORY_TEMP,   gauge, Memory temperature (in C)\nDCGM_FI_DEV_PCIE_TX_THROUGHPUT, gauge, PCIe TX throughput (in KB/s)\nDCGM_FI_DEV_PCIE_RX_THROUGHPUT, gauge, PCIe RX throughput (in KB/s)\nDCGM_FI_DEV_PCIE_REPLAY_COUNTER, counter, PCIe replay counter\nDCGM_FI_DEV_XID_ERRORS,    gauge, Value of the last XID error encountered\nDCGM_FI_DEV_ECC_DBE_VOL_TOTAL, counter, Total double-bit ECC errors\nDCGM_FI_DEV_ECC_SBE_VOL_TOTAL, counter, Total single-bit ECC errors\" > /var/snap/dcgm/common/custom-metrics.csv'

          # 3. [è¨­å®š] å¼·åˆ¶æŒ‡å®šæ”¶é›†å™¨è·¯å¾‘
          sudo mkdir -p /etc/systemd/system/snap.dcgm.dcgm-exporter.service.d/
          echo '[Service]
          Environment=\"DCGM_EXPORTER_COLLECTORS=/var/snap/dcgm/common/custom-metrics.csv\"' | sudo tee /etc/systemd/system/snap.dcgm.dcgm-exporter.service.d/override.conf

          # 4. [é‡å•Ÿ] ç¢ºä¿å¥—ç”¨
          sudo systemctl daemon-reload
          sudo snap stop dcgm.dcgm-exporter
          sudo snap start dcgm.dcgm-exporter
        "
      changed_when: true
          
# ==========================================
# ç¬¬ä¸‰éšæ®µï¼šè¨­å®šæª”æ·±åº¦å¯«å…¥èˆ‡å•Ÿå‹•
# ==========================================
- name: 3. æœ€çµ‚è¨­å®šèˆ‡æœå‹™å•Ÿå‹•
  hosts: maasjuju
  vars:
    slurm_model: "hpc-lab"
    node_cpus: 18
    node_mem: 128646
    # ç¡¬é«”æ¨™ç±¤å®šç¾©
    master_tags: "tags=virtual"
    worker_tags: "tags=slurm-node"
  tasks:
    - name: 3.0.1 æ ¹æ“š MAAS Tag å‹•æ…‹åµæ¸¬ Machine ID
      shell: |
        STATUS=$(juju machines -m {{ slurm_model }} --format json)
        # å°‹æ‰¾å¸¶æœ‰ virtual tag çš„æ©Ÿå™¨ ID (Master)
        M0=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ master_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        # å°‹æ‰¾å¸¶æœ‰ slurm-node tag çš„æ©Ÿå™¨ ID (Worker)
        M1=$(echo $STATUS | jq -r '.machines | to_entries[] | select(.value.constraints | contains("tags={{ worker_tags | replace("tags=", "") }}")) | .key' | head -n 1)
        echo "$M0|$M1"
      register: detected_ids
      changed_when: false

    - name: 3.0.2 å„²å­˜ ID è®Šæ•¸
      set_fact:
        master_id: "{{ detected_ids.stdout.split('|')[0] }}"
        worker_id: "{{ detected_ids.stdout.split('|')[1] }}"
    
    - name: 3.1 å¯«å…¥ Master slurm.conf (èªæ³•ä¿®æ­£ç‰ˆ)
      shell: |
        juju ssh -m {{ slurm_model }} {{ master_id }} "
        # 1. å»ºç«‹å¿…è¦ç›®éŒ„
        sudo mkdir -p /var/spool/slurmctld /var/run/slurm /var/log/slurm
        sudo chown -R slurm:slurm /var/spool/slurmctld /var/run/slurm /var/log/slurm
        
        # 2. å¯«å…¥è¨­å®šæª” (ç§»é™¤ä¸ç›¸å®¹çš„ MungeSocketPath)
        sudo bash -c \"cat > /etc/slurm/slurm.conf <<'EOF'
        ClusterName=hpc-lab
        SlurmctldHost=\$(hostname)
        
        # [ä¿®æ­£] ç§»é™¤ MungeSocketPathï¼Œè®“ AuthType æ’ä»¶è‡ªè¡Œè™•ç†
        AuthType=auth/munge
        SlurmUser=slurm
        
        # PID èˆ‡è·¯å¾‘
        SlurmctldPidFile=/var/run/slurm/slurmctld.pid
        SlurmdPidFile=/var/run/slurm/slurmd.pid
        StateSaveLocation=/var/spool/slurmctld
        SlurmdSpoolDir=/var/spool/slurmd
        
        # æ—¥èªŒ
        SlurmctldLogFile=/var/log/slurm/slurmctld.log
        SlurmdLogFile=/var/log/slurm/slurmd.log
        
        # æ’ä»¶è¨­å®š
        GresTypes=gpu
        ProctrackType=proctrack/cgroup
        TaskPlugin=task/affinity,task/cgroup
        SelectType=select/cons_tres
        SelectTypeParameters=CR_Core_Memory
        
        # ç¯€é»å®šç¾©
        NodeName=gpu-node01 CPUs=18 Sockets=1 CoresPerSocket=18 ThreadsPerCore=1 RealMemory=128646 Gres=gpu:rtx4090:1 State=UNKNOWN
        PartitionName=debug Nodes=gpu-node01 Default=YES MaxTime=INFINITE State=UP
        EOF\"
        
        # 3. æ¬Šé™èˆ‡å•Ÿå‹•
        sudo chown slurm:slurm /etc/slurm/slurm.conf
        sudo systemctl restart munge
        sudo systemctl restart slurmctld
        "

    - name: 3.2 åŒæ­¥è¨­å®šæª”èˆ‡ Munge Key (æš´åŠ›åŒæ­¥ç‰ˆ)
      shell: |
        # 1. æŠ“å– Master çš„è¨­å®šæª”èˆ‡ Key (Base64 ç¢ºä¿å®‰å…¨å‚³è¼¸)
        CONF_B64=$(juju ssh -m {{ slurm_model }} {{ master_id }} "sudo base64 -w0 /etc/slurm/slurm.conf")
        KEY_B64=$(juju ssh -m {{ slurm_model }} {{ master_id }} "sudo base64 -w0 /etc/munge/munge.key")
        
        # 2. æ¨é€åˆ° Worker ä¸¦å¼·åˆ¶é…ç½®
        juju ssh -m {{ slurm_model }} {{ worker_id }} "
          # ç›®éŒ„èˆ‡æ¬Šé™
          sudo mkdir -p /etc/slurm /var/spool/slurmd /var/run/slurm /var/log/slurm
          sudo chown -R slurm:slurm /etc/slurm /var/spool/slurmd /var/run/slurm /var/log/slurm
          
          # å¯«å…¥ Slurm è¨­å®š
          echo '$CONF_B64' | base64 -d | sudo tee /etc/slurm/slurm.conf > /dev/null
          
          # å¯«å…¥ä¸¦ä¿è­· Munge Key (é—œéµä¿®æ­£)
          echo '$KEY_B64' | base64 -d | sudo tee /etc/munge/munge.key > /dev/null
          sudo chown munge:munge /etc/munge/munge.key
          sudo chmod 400 /etc/munge/munge.key
          
          # å¯«å…¥ GPU è¨­å®š
          echo 'Name=gpu Type=rtx4090 File=/dev/nvidia0' | sudo tee /etc/slurm/gres.conf
          
          # ä¾åºå•Ÿå‹•æœå‹™
          sudo systemctl restart munge
          sudo systemctl restart slurmd
        "
      args:
        executable: /bin/bash

    - name: 3.3 æœ€çµ‚å–šé†’ç¯€é»
      shell: |
        # å…ˆè¨­ç‚º Down å† Resumeï¼Œå¼·åˆ¶é‡ç½®ç‹€æ…‹
        juju ssh -m {{ slurm_model }} {{ master_id }} "sudo scontrol update nodename=gpu-node01 state=down reason='ansible_sync'"
        sleep 2
        juju ssh -m {{ slurm_model }} {{ master_id }} "sudo scontrol update nodename=gpu-node01 state=resume"
      changed_when: false

    - name: 3.4 é…ç½® K3s ç›£æ§å°æ¥ (GPU Metrics to K8s)
      shell: |
        # 1. åœ¨æœ¬åœ°åŸ·è¡Œ juju æŒ‡ä»¤æŠ“å– Worker IP
        W_IP=$(juju ssh -m {{ slurm_model }} {{ worker_id }} 'hostname -I' | awk '{print $1}')
        
        # 2. åœ¨ Master (Machine 0) é€éä¸€è¡ŒæŒ‡ä»¤å®Œæˆå‘½åç©ºé–“èˆ‡æ–‡ä»¶å¯«å…¥
        # æˆ‘å€‘å°‡ YAML å…§å®¹å¯«å…¥ /tmp/gpu-metrics.yaml é¿é–‹ EOF ç¸®æ’å•é¡Œ
        juju ssh -m {{ slurm_model }} {{ master_id }} "
          sudo kubectl create ns kubesphere-monitoring-system --dry-run=client -o yaml | sudo kubectl apply -f -
          
          echo \"apiVersion: v1
        kind: Endpoints
        metadata:
          name: gpu-metrics-worker
          namespace: kubesphere-monitoring-system
        subsets:
          - addresses:
              - ip: $W_IP
            ports:
              - name: metrics
                port: 9400
        ---
        apiVersion: v1
        kind: Service
        metadata:
          name: gpu-metrics-worker
          namespace: kubesphere-monitoring-system
          labels:
            app: dcgm-exporter
        spec:
          ports:
            - name: metrics
              port: 9400
              targetPort: 9400\" > /tmp/gpu-metrics.yaml

          sudo kubectl apply -f /tmp/gpu-metrics.yaml
        "
      vars:
        slurm_model: "hpc-lab"
# ==========================================
# ç¬¬å››éšæ®µï¼šå¤šå¢é›†ç™»éŒ„èˆ‡ç›£æ§è‡ªå‹•æ¿€æ´»
# ==========================================
- name: 4. Register Cluster to KubeSphere and Enable Monitoring
  hosts: maasjuju
  vars:
    # è«‹æ ¹æ“š AWX Survey å‚³å…¥æˆ–æ‰‹å‹•å®šç¾©
    k8s_cluster: "hpc-native-cluster" 
    ks_host_ip: "192.168.100.4"
    ks_host_user: "ubuntu"
    slurm_model: "hpc-lab"

  tasks:
    # ----------------------------------------------------------------
    # 1. åœ¨ Machine 0 å–å¾— K3s Config
    # ----------------------------------------------------------------
    # æ­¥é©Ÿ A: å…ˆæŠ“ IPï¼Œå­˜å…¥ Ansible è®Šæ•¸
    - name: 4.0 ç²å– Machine 0 å¯¦é«” IP
      shell: "juju ssh -m {{ slurm_model }} {{ master_id }} 'hostname -I' | awk '{print $1}'"
      register: master_ip_result

    # æ­¥é©Ÿ B: é€é Ansible è®Šæ•¸ç›´æ¥å¯«æ­» IP é€²å»
    - name: 4.1 ç”¢ç”Ÿ K3s Kubeconfig Base64
      shell: |
        juju ssh -m {{ slurm_model }} {{ master_id }} "sudo cat /etc/rancher/k3s/k3s.yaml | sed 's/127.0.0.1/{{ master_ip_result.stdout | trim }}/g' | base64 -w 0"
      register: k3s_kubeconfig_b64

    - name: 4.2 å°‡ KubeSphere Host åŠ å…¥è‡¨æ™‚ Inventory
      add_host:
        name: "ks_host"
        ansible_host: "{{ ks_host_ip }}"
        ansible_user: "{{ ks_host_user }}"
        cluster_name: "{{ k8s_cluster }}"
        cluster_config: "{{ k3s_kubeconfig_b64.stdout }}"

# ==========================================
# ç¬¬äº”éšæ®µï¼šåœ¨ KubeSphere Host åŸ·è¡Œè¨»å†Šèˆ‡ç›£æ§é‹ªè·¯
# ==========================================
- name: 5. åŸ·è¡Œ KubeSphere è³‡æºæ³¨å…¥èˆ‡ç›£æ§ä¿®å¾©
  hosts: ks_host
  gather_facts: no
  vars:
    # è‡¨æ™‚ Kubeconfig è·¯å¾‘
    member_cluster_kubeconfig: "/tmp/kubeconfig-{{ cluster_name }}"

  tasks:
    # --------------------------------------------------------
    # 5.1 è¨»å†Šå¢é›† (è®“ KubeSphere ç´ç®¡ K3s)
    # --------------------------------------------------------
    - name: 5.1 æ³¨å…¥ Cluster CRD (ç›´æ¥é€£æ¥æ¨¡å¼)
      shell: |
        cat <<EOF | kubectl apply -f -
        apiVersion: cluster.kubesphere.io/v1alpha1
        kind: Cluster
        metadata:
          name: {{ cluster_name }}
          labels:
            kubesphere.io/managed: "true"
            cluster-role.kubesphere.io/member: ""
            cluster-role.kubesphere.io/public: ""
        spec:
          provider: k3s
          joinFederation: true
          connection:
            type: direct
            # é€™è£¡ä½¿ç”¨ä¸Šä¸€éšæ®µå‚³éä¾†çš„ base64 è®Šæ•¸
            kubeconfig: {{ cluster_config }}
        EOF
      register: register_result

    # --------------------------------------------------------
    # 5.2 æº–å‚™ç›£æ§ç’°å¢ƒ (å–ä»£èˆŠçš„è‡ªå‹• Agent å®‰è£)
    # èªªæ˜ï¼šé€™è£¡ä¸ç›´æ¥é–‹ Agentï¼Œè€Œæ˜¯å…ˆæŠŠè·¯é‹ªå¥½ (CRD + ServiceMonitor)
    # é€™æ¨£æ‚¨åœ¨ UI é»é¸å•Ÿç”¨å¾Œï¼ŒGPU æ•¸æ“šæœƒé¦¬ä¸Šé€š
    # --------------------------------------------------------
    
    - name: 5.2.1 ç”¢ç”Ÿè‡¨æ™‚ Kubeconfig æª”æ¡ˆ (ç”¨æ–¼å¾ŒçºŒä¿®å¾©)
      shell: |
        # å°‡è®Šæ•¸ä¸­çš„ Kubeconfig è§£ç¢¼ä¸¦å­˜æˆæª”æ¡ˆï¼Œä¾› kubectl ä½¿ç”¨
        echo "{{ cluster_config }}" | base64 -d > {{ member_cluster_kubeconfig }}
      
    - name: 5.2.2 ç­‰å¾…å¢é›†è¨»å†Šç”Ÿæ•ˆ (ç°¡å–®æª¢æŸ¥ Namespace)
      shell: |
        # å˜—è©¦é€£ç·š Member Clusterï¼Œç¢ºä¿è¨»å†ŠæˆåŠŸä¸”ç¶²è·¯äº’é€š
        kubectl --kubeconfig {{ member_cluster_kubeconfig }} get nodes
      register: cluster_check
      until: cluster_check.rc == 0
      retries: 10
      delay: 5

    - name: 5.2.3 é å…ˆè£œè£ ServiceMonitor CRD
      shell: |
        # é€™æ˜¯ç‚ºäº†é˜²æ­¢ Agent å®‰è£æ™‚å ±éŒ¯ "no matches for kind ServiceMonitor"
        kubectl --kubeconfig {{ member_cluster_kubeconfig }} apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/v0.68.0/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
      ignore_errors: yes

    - name: 5.2.4 é å…ˆæ³¨å…¥ GPU æ¡é›†è¦å‰‡ (ServiceMonitor)
      shell: |
        # é€™æ˜¯é€£æ¥ Machine 1 dcgm-exporter çš„é—œéµæ©‹æ¨‘
        cat <<EOF | kubectl --kubeconfig {{ member_cluster_kubeconfig }} apply -f -
        apiVersion: monitoring.coreos.com/v1
        kind: ServiceMonitor
        metadata:
          name: gpu-bridge-monitor
          namespace: kubesphere-monitoring-system
          labels:
            app.kubernetes.io/vendor: kubesphere
        spec:
          selector:
            matchLabels:
              app: dcgm-exporter
          endpoints:
          - port: metrics
            interval: 15s
            path: /metrics
          namespaceSelector:
            matchNames:
            - kubesphere-monitoring-system
        EOF

    - name: 5.3 æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
      file:
        path: "{{ member_cluster_kubeconfig }}"
        state: absent

    - name: ğŸš€ éƒ¨ç½²å®Œæˆæç¤º
      debug:
        msg: 
          - "ğŸ‰ åŸºç¤å»ºè¨­èˆ‡å¢é›†è¨»å†Šå·²å®Œæˆï¼"
          - "âš ï¸ æœ€å¾Œä¸€æ­¥ï¼šè«‹å‰å¾€ KubeSphere ç¶²é ä»‹é¢ -> æ“´å±•ä¸­å¿ƒ (Marketplace)"
          - "   -> WhizardTelemetry -> Agent Configs"
          - "   -> å‹¾é¸ '{{ cluster_name }}' ä¸¦ç¢ºèªé–‹å•Ÿ GPU é¸é …ã€‚"
          - "   (å› ç‚ºç›£æ§è¦å‰‡å·²åœ¨ 5.2.4 é å…ˆæ³¨å…¥ï¼Œæ‚¨å•Ÿç”¨å¾Œæ•¸æ“šæœƒç«‹åˆ»å‡ºç¾)"
